{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cd029a",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "21e2eaa6",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd984775",
   "metadata": {},
   "source": [
    "# Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d014e",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9f579b1",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "artists_path = 'data\\\\artists.csv'\n",
    "tracks_path = 'data\\\\tracks.csv'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6c6d9fd",
   "metadata": {},
   "source": [
    "This code automatically detects the correct separator for two dataset files (tracks and artists) by checking which character — comma, semicolon, or tab — appears most in the first line. It then loads each file into a pandas DataFrame using the detected separator, prints their shapes, and displays the first few rows.\n",
    "\n",
    " The tracks dataset has 11,166 rows and 45 columns, while the artists dataset has 104 rows and 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d8cef75",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Funzione helper per capire il separatore corretto\n",
    "def detect_separator(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample = f.readline()\n",
    "    # Conta quanti separatori compaiono\n",
    "    seps = {',': sample.count(','), ';': sample.count(';'), '\\t': sample.count('\\t')}\n",
    "    best_sep = max(seps, key=seps.get)\n",
    "    print(f\"Detected separator for {filepath}: '{best_sep}'\")\n",
    "    return best_sep\n",
    "\n",
    "# Rileva automaticamente il separatore\n",
    "sep_tracks = detect_separator(tracks_path)\n",
    "sep_artists = detect_separator(artists_path)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "# Carica i dataset in base al separatore rilevato\n",
    "tracks = pd.read_csv(tracks_path, sep=sep_tracks, encoding='utf-8', engine='python')\n",
    "artists = pd.read_csv(artists_path, sep=sep_artists, encoding='utf-8', engine='python')\n",
    "\n",
    "# Mostra alcune info per verifica\n",
    "print(f\"Tracks shape: {tracks.shape}\")\n",
    "print(f\"Artists shape: {artists.shape}\")\n",
    "print('------------------------------------')\n",
    "\n",
    "print('TRACKS')\n",
    "display(tracks.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('ARTISTS')\n",
    "display(artists.head(3))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc073dbf",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "print(\"Artists Features\")\n",
    "print(artists.columns.tolist())\n",
    "\n",
    "print(\"Tracks Features\")\n",
    "print(tracks.columns.tolist())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65149c0e",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47124c",
   "metadata": {},
   "source": [
    "### Artists\n",
    "\n",
    "The following code checks the artists dataset for duplicates in two ways: first, it looks for identical full rows to detect any completely repeated entries; then, it checks for duplicates specifically based on the artist ID and artist name columns.\n",
    "<B> After performing both checks, it confirms that there are no duplicate artists in the dataset </B>."
   ]
  },
  {
   "cell_type": "code",
   "id": "0140ce52",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for duplicated artists rows\n",
    "duplicates_artists = artists[artists.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated Artists rows: {duplicates_artists.shape[0]}\")\n",
    "display(duplicates_artists.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb66536e",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for duplicated artists based on artist id\n",
    "duplicates_artists_id = artists[artists.duplicated(subset='id_author')]\n",
    "print(f\"Number of duplicated artist based on ID: {duplicates_artists_id.shape[0]}\")\n",
    "display(duplicates_artists_id.head(5))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b37a9dc",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for duplicated artists based on artist name\t\n",
    "duplicates_artists_name = artists[artists.duplicated(subset='name')]\n",
    "print(f\"Number of duplicated artist based on Name: {duplicates_artists_name.shape[0]}\")\n",
    "display(duplicates_artists_name.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b90c100",
   "metadata": {},
   "source": [
    "### Tracks\n",
    "Duplicates rows check has been also performed here.\n",
    "No duplicated rows were detected, indicating that all track entries are unique."
   ]
  },
  {
   "cell_type": "code",
   "id": "5ddc8e39",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for duplicated tracks rows\n",
    "duplicates_tracks = tracks[tracks.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated rows: {duplicates_tracks.shape[0]}\")\n",
    "display(duplicates_tracks.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ab1fa4b",
   "metadata": {},
   "source": [
    "#### Duplicated Tracks based on ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8f773",
   "metadata": {},
   "source": [
    "This code checks the tracks dataset for duplicates based specifically on the track ID column. It identifies all rows where the same ID appears more than once, counts them, and displays them.\n",
    " It first identifies all rows where the same ID appears more than once, counts how many duplicated tracks exist, and displays them. Then, it counts how many times each track ID occurs in the dataset. \n",
    "\n",
    "<B> The result shows that there are 73 duplicated rows based on track IDs. \n",
    "Precisely we have 71  distinct IDs that have duplicates. </B>\n",
    "\n",
    "<B>one track ID is repeated four times, while the others are each repeated twice </B>"
   ]
  },
  {
   "cell_type": "code",
   "id": "8119395e",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for duplicated tracks based on track id\n",
    "duplicates_tracks_id = tracks[tracks.duplicated(subset='id')]\n",
    "print(f\"Number of duplicated Tracks rows based on ID: {duplicates_tracks_id.shape[0]}\")\n",
    "display(duplicates_tracks_id)\n",
    "\n",
    "\n",
    "# Count how many times each id_track appears\n",
    "id_counts = tracks['id'].value_counts()\n",
    "duplicate_id_counts = id_counts[id_counts > 1]\n",
    "\n",
    "print('Number of distinct IDs that have duplicates')\n",
    "print(duplicate_id_counts.size)\n",
    "print(\"Number of tracks for each id:\")\n",
    "print(duplicate_id_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66c11057",
   "metadata": {},
   "source": [
    "The following code lists every full_title associated with each duplicated track ID. The results show 71 duplicated IDs in total. Most of these IDs are linked to two different songs, except for one ID that is associated with four songs (two pairs sharing the same title)."
   ]
  },
  {
   "cell_type": "code",
   "id": "df6b045d",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Find all duplicated track IDs\n",
    "duplicate_ids = tracks[tracks.duplicated(subset='id', keep=False)]\n",
    "\n",
    "# Group by 'id' and list all titles\n",
    "titles_per_id = duplicate_ids.groupby('id')['full_title'].apply(list)\n",
    "\n",
    "# Display each ID with all titles and the count of unique titles\n",
    "for track_id, titles in titles_per_id.items():\n",
    "    unique_count = len(set(titles))  # number of unique titles\n",
    "    print(f\"Track ID: {track_id} Number(of total songs: {len(titles)})(Unique titles: {unique_count})\")\n",
    "    for title in titles:\n",
    "        print(f\"  - {title}\")\n",
    "    print('----------------------------------------------------------')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77df80be",
   "metadata": {},
   "source": [
    "#### Fixing Duplicated Tracks Id\n",
    "After reviewing the songs associated with the duplicated IDs, we found that each duplicated ID corresponds to different songs, except for one case that will be treated later. Therefore, the most reasonable solution is to modify the duplicated IDs by appending the row number to each one. This approach ensures that all songs are preserved while maintaining unique identifiers for every track."
   ]
  },
  {
   "cell_type": "code",
   "id": "045dd7ca",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Identify duplicated IDs\n",
    "duplicate_mask = tracks.duplicated(subset='id', keep=False)\n",
    "\n",
    "# Assign new unique IDs only to duplicated rows\n",
    "tracks.loc[duplicate_mask, 'id'] = (\n",
    "    tracks.loc[duplicate_mask]\n",
    "    .apply(lambda x: f\"{x['id']}_{x.name}\", axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Example of updated duplicates:\")\n",
    "display(tracks[duplicate_mask][['id', 'full_title']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef274c1b",
   "metadata": {},
   "source": [
    "#### Duplicated Tracks based on Title\n",
    "The following code identifies tracks that share the same full_title, meaning duplicate song titles. We found four duplicated tracks, corresponding to two pairs of songs with identical titles."
   ]
  },
  {
   "cell_type": "code",
   "id": "4d7f90c9",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Find duplicated full_title\n",
    "duplicate_titles = tracks[tracks.duplicated(subset='full_title', keep=False)]\n",
    "\n",
    "# Sort by full_title to see them together\n",
    "duplicate_titles = duplicate_titles.sort_values('full_title')\n",
    "\n",
    "print(f\"Tracks with duplicate track based on full_title: {duplicate_titles.shape[0]}\")\n",
    "display(duplicate_titles)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c40a6da",
   "metadata": {},
   "source": [
    "#### Fixing Duplicated Tracks full_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13927df1",
   "metadata": {},
   "source": [
    "The duplicated titles  — \"BUGIE by Madame (Ft. Carl Brave & Rkomi)\" and \"sentimi by Madame\" — actually refer to the same songs released in two different formats: one from the album and one from the single version. \n",
    "We decided to keep the duplicated tracks in the dataset but add a clear indication in the full_title to show whether each song comes from a single or an album. This way, all versions are preserved while making it easy to distinguish between different releases of the same song"
   ]
  },
  {
   "cell_type": "code",
   "id": "da8b9963",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# Find duplicated full_titles\n",
    "duplicate_mask = tracks.duplicated(subset='full_title', keep=False)\n",
    "\n",
    "# Update only the duplicated titles by appending album_type\n",
    "tracks.loc[duplicate_mask, 'full_title'] = (\n",
    "    tracks.loc[duplicate_mask, 'full_title'] + \n",
    "    \" (\" + tracks.loc[duplicate_mask, 'album_type'].fillna('unknown').str.capitalize() + \")\"\n",
    ")\n",
    "\n",
    "# Verify the changes\n",
    "duplicate_titles = tracks[tracks.duplicated(subset='full_title', keep=False)].sort_values('full_title')\n",
    "display(duplicate_titles[['full_title', 'album_type', 'id']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0345a5e0",
   "metadata": {},
   "source": [
    "## Merging the Datasets\n",
    "\n",
    "\n",
    "Merging the tracks and artists datasets into a single DataFrame called df. It matches rows where the <B> id_artist column in tracks</B> corresponds to the <B>id_author column in artists</B>, using a left join so that all tracks are kept even if some artists are missing. After merging, it prints the number of rows and columns in the unified dataset and shows the first three rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c9692d9",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "df = tracks.merge(artists, left_on='id_artist', right_on='id_author', how='left')\n",
    "\n",
    "print(f\"Unified dataset: {df.shape[0]} rows , {df.shape[1]} columns\")\n",
    "display(df.head(3))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d4eb0a9",
   "metadata": {},
   "source": [
    "Checking if there is a track without an artist"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1749624",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Check for tracks without a matching artist\n",
    "missing_artists = df[df['id_author'].isna()]\n",
    "\n",
    "print(f\"Number of tracks without an artist: {missing_artists.shape[0]}\")\n",
    "display(missing_artists.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ea824a2",
   "metadata": {},
   "source": [
    "Checking if there is an artist without a track"
   ]
  },
  {
   "cell_type": "code",
   "id": "d218d12f",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "missing_artists = artists[~artists['id_author'].isin(tracks['id_artist'])]\n",
    "print(\"Number of artists without any tracks:\", len(missing_artists))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62b01a6a",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "This code analyzes missing values in the DataFrame by counting how many entries are NaN for each column and calculating the corresponding percentage. It creates a summary table showing only columns with missing data, sorted by the highest percentage."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c9a79fd",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Calcolo missing values e percentuali\n",
    "missing_count = df.isna().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "missing_df = (\n",
    "    pd.DataFrame({'missing_count': missing_count, 'missing_percent': missing_percent})\n",
    "    .sort_values('missing_percent', ascending=False)\n",
    "    .query('missing_percent > 0')\n",
    ")\n",
    "\n",
    "# Mostra tabella riepilogativa (gradiente rosso-magenta)\n",
    "display(\n",
    "    missing_df\n",
    "    .style.background_gradient(subset=['missing_percent'], cmap='RdPu')  \n",
    "    .format({'missing_percent': '{:.2f}%'})\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5078ab2f",
   "metadata": {},
   "source": [
    "The following heatmap visualizes missing values in the dataset, with each row representing a record and each column a feature. Colored cells indicate missing entries, providing a clear overview of where data is incomplete."
   ]
  },
  {
   "cell_type": "code",
   "id": "758890c2",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
    "plt.title(\"Missing Values Matrix (Overview)\", fontsize=20, pad=12, color=\"#000000\")\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0a76413",
   "metadata": {},
   "source": [
    "The following bar plot shows the percentage of missing values per feature, with the top 20 features that have the most missing data"
   ]
  },
  {
   "cell_type": "code",
   "id": "6967f829",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=missing_df.head(20),\n",
    "    x='missing_percent',\n",
    "    y=missing_df.head(20).index,\n",
    "    hue=missing_df.head(20).index,  \n",
    "    palette='RdPu_r'  \n",
    ")\n",
    "plt.title(\"Percentage of Missing Values by Feature\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Missing values (%)\", fontsize=12)\n",
    "plt.ylabel(\"Feature name\", fontsize=12)\n",
    "\n",
    "# Etichette percentuali\n",
    "for index, value in enumerate(missing_df.head(20)['missing_percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#b30059')\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2dff26e8",
   "metadata": {},
   "source": [
    "#### Missing Values Propagation After Merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc0d9c46",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "artists_missing = artists.isna().mean().sort_values(ascending=False) * 100\n",
    "print(artists_missing)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da5ed1e9",
   "metadata": {},
   "source": [
    "The visualization highlights that missing values in attributes such as active_start, region, and birth_place have increased after merging due to the replication of incomplete artist metadata across multiple tracks.\n",
    "This confirms that the merge process did not introduce new nulls, but propagated pre-existing ones."
   ]
  },
  {
   "cell_type": "code",
   "id": "30b808b4",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Colonne provenienti dal dataset artists \n",
    "artist_cols =list(artists.columns)\n",
    "\n",
    "# Conta i NaN prima e dopo il merge\n",
    "missing_before = artists[artist_cols].isna().sum()\n",
    "missing_after = df[artist_cols].isna().sum()\n",
    "\n",
    "# Differenza assoluta e percentuale\n",
    "missing_diff = missing_after - missing_before\n",
    "increase_percent = (missing_diff / missing_before.replace(0, pd.NA)) * 100\n",
    "\n",
    "# Tabella riepilogativa\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"missing_before\": missing_before,\n",
    "        \"missing_after\": missing_after,\n",
    "        \"difference\": missing_diff,\n",
    "        \"increase_%\": increase_percent\n",
    "    })\n",
    "    .sort_values(\"difference\", ascending=False)\n",
    ")\n",
    "\n",
    "plot_df = missing_summary[missing_summary['difference'] > 0].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x='difference',\n",
    "    y=plot_df.index,\n",
    "    hue=plot_df.index,\n",
    "    palette='RdPu_r'\n",
    ")\n",
    "plt.title(\"Increase in Missing Values After Merge\", fontsize=15, pad=12, color=\"#000000\")\n",
    "plt.xlabel(\"Increase in number of missing values\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "\n",
    "# Etichette numeriche a fianco delle barre\n",
    "for index, value in enumerate(plot_df['difference']):\n",
    "    plt.text(value + 50, index, f\"{int(value):,}\", va='center', fontsize=9, color=\"#000000\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "adeccf32",
   "metadata": {},
   "source": [
    "After analyzing the percentage of missing values in each column, We need to better understand the overall data quality before applying any filling strategies. Cleaning and validating the data first ensures that missing values are handled correctly and that no incorrect or misleading information is introduced during imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d0065",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "id": "adc69283",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "867294bd",
   "metadata": {},
   "source": [
    "### Initial Data Cleaning\n",
    "\n",
    "Based on the initial exploration of the dataset, we:\n",
    "\n",
    "- **Removed empty column (`active_end`)** since it contained no useful information.  \n",
    "- **Converted `popularity` and `year`** to numeric types to ensure consistency and enable statistical analysis.  \n",
    "- **Transformed date-related columns** (`album_release_date`, `birth_date`, `active_start`, ) into proper datetime format for easier time-based operations.\n",
    "\n",
    "Before directly converting year and popularity from objects to numeric and album_release_date, birth_date, and active_start from objects to datetime, we need to inspect the data to check if all values can be converted correctly and handle those that cannot be converted\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5abce5f",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# 1. Remove empty column\n",
    "df.drop(columns=['active_end'], inplace=True)  # drop the 'active_end' column because it's empty\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7da45447",
   "metadata": {},
   "source": [
    "#### Objects to Numeric\n",
    "Inspecting the values in popularity and year columns to see the values that cannot be converted to numbers directly"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf9f43ba",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "numeric_cols = ['popularity','year']\n",
    "\n",
    "\n",
    "# --- Check numeric columns ---\n",
    "for col in numeric_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_numeric(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to numeric:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to numeric.\")\n",
    "    print('----------------------------------------------------------------')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "faf2dceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Looking at the values of the  `popularity` column, we noticed that some entries contained **non-numeric characters**, percent signs (`%`), abbreviations like `K` (thousands) or `M` (millions), and words such as `\"views\"` appended to the numbers.  \n",
    "\n",
    "Instead of converting the column directly to numeric using pd.to_numeric(errors='coerce'), which would have turned all invalid entries into NaN, we applied a cleaning function to preserve and correctly interpret useful numeric information before conversion. The function:\n",
    "\n",
    "- Removed non-numeric characters and words like `\"views\"` and `%`.\n",
    "- Converted abbreviations (`K → 1,000`, `M → 1,000,000`) to numeric values.\n",
    "- Extracted the first numeric part if extra text was present.\n",
    "- Converted the cleaned values to floats, marking any remaining invalid entries as `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4d6c297",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "def clean_popularity(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value_str = str(value).strip().lower()  # normalize\n",
    "    \n",
    "    # Remove common words like 'views'\n",
    "    value_str = value_str.replace('views','').replace('%','').strip()\n",
    "    value_str = value_str.lower()  \n",
    "    # Handle K and M\n",
    "    multiplier = 1\n",
    "    if value_str.endswith('k'):\n",
    "        multiplier = 1_000\n",
    "        value_str = value_str[:-1]\n",
    "    elif value_str.endswith('m'):\n",
    "        multiplier = 1_000_000\n",
    "        value_str = value_str[:-1]\n",
    "    \n",
    "    # Take only first token if words remain\n",
    "    value_str = value_str.split()[0]\n",
    "    \n",
    "    # Try converting to float\n",
    "    try:\n",
    "        return (float(value_str) * multiplier)\n",
    "    except:\n",
    "        return None  # invalid entries become None/NaN\n",
    "    \n",
    "df['popularity'].apply(clean_popularity)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "284ed82a",
   "metadata": {},
   "source": [
    "Inspecting the values in the year column, we observed that while most entries were numerical, some contained unexpected or non-numeric characters. To handle this, we converted the column directly to a numeric type using pd.to_numeric() with the errors='coerce' parameter, which automatically transforms any invalid or non-numeric values into NaN."
   ]
  },
  {
   "cell_type": "code",
   "id": "6f483bf5",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce') "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "903c4bd1",
   "metadata": {},
   "source": [
    "#### Objects to DateTime\n",
    "\n",
    "Inspecting the values in 'album_release_date', 'birth_date', 'active_start' columns to see the values that cannot be converted to DateTime directly"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9e0b5a2",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "date_cols = ['album_release_date', 'birth_date', 'active_start']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0592d6c0",
   "metadata": {},
   "source": [
    "Looking at the values in the album_release_date column that could not be converted to datetime, we noticed that many of them were just years (e.g., \"2004\"). If we used pd.to_datetime(errors='coerce') directly, these entries would have been turned into NaT. However, we wanted to keep this information by assigning a default month and day — the first day of the year.\n",
    "\n",
    "- Instead of converting the column directly, we applied a cleaning function that:\n",
    "\n",
    "- Detected values that were only a year (e.g., \"2004\") and changed them to a full date (\"2004-01-01\").\n",
    "\n",
    "- Kept valid full dates (e.g., \"2021-04-09\") unchanged.\n",
    "\n",
    "- Left missing values as they are.\n",
    "\n",
    "- Finally, converted everything into proper datetime format for consistency."
   ]
  },
  {
   "cell_type": "code",
   "id": "08dd9a98",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "def fix_year_only_dates(val):\n",
    "    \"\"\"\n",
    "    If the value looks like a 4-digit year, convert it to 'YYYY-01-01'.\n",
    "    Otherwise, return the original value.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val_str = str(val).strip()\n",
    "    if re.fullmatch(r'\\d{4}', val_str):\n",
    "        return f\"{val_str}-01-01\"\n",
    "    return val_str\n",
    "\n",
    "# Apply to album_release_date\n",
    "df['album_release_date'] = df['album_release_date'].apply(fix_year_only_dates)\n",
    "\n",
    "# Convert to datetime\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43fc82dc",
   "metadata": {},
   "source": [
    "Based on the values that could not be converted to datetime, we found that the birth_date column contained several invalid entries, such as URLs (e.g., \"http://www.wikidata.org/.well-known/genid/...\") instead of actual dates. Since these values do not represent meaningful or recoverable information, there is nothing worth preserving. Therefore, we are going to apply the pd.to_datetime(errors='coerce') function directly, allowing all invalid entries to be converted to NaT.\n",
    "\n",
    "For the active_start column, all non-missing entries are  already in a valid date format, so they are going to be  successfully converted to datetime without any issues."
   ]
  },
  {
   "cell_type": "code",
   "id": "49c6ec78",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "\n",
    "date_cols = ['birth_date', 'active_start', ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22aedd1f",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "The following table and histogram show the numerical data distribution in the dataset:\n",
    "\n",
    "- **Most features** (`n_sentences`, `n_tokens`, `tokens_per_sent`, `char_per_tok`, `lexical_density`, `avg_token_per_clause`, `centroid`, `rolloff`, `rms`, `zcr`, `flatness`, `flux`, `spectral_complexity`, `pitch`, `loudness`) show **bell-shaped or near-normal distributions**.\n",
    "\n",
    "- **Highly skewed features** (`stats_pageviews`, `bpm`, `tokens_per_sent`, `duration_ms`, `popularity`) have a **long right tail**, indicating a few extreme values or outliers (common in popularity or count-based features).\n",
    "\n",
    "- **Temporal features** (`year`, `month`, `day`) display **non-uniform distributions**; e.g., `year` is concentrated around recent decades, showing most songs are modern.\n",
    "\n",
    "- **Geographical features** (`latitude`, `longitude`) have **peaks corresponding to specific locations**, likely representing where artists or tracks are clustered.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c24f786",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# Select numeric columns\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# --- Summary statistics table ---\n",
    "display(df[num_cols].describe().T.style.background_gradient(cmap='RdPu'))\n",
    "\n",
    "# --- Histograms for each numeric column ---\n",
    "n_cols = 4\n",
    "n_rows = -(-len(num_cols) // n_cols)  # ceil division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col].dropna(), bins=30, kde=True, color=\"#d36ba8\", ax=axes[i])\n",
    "    axes[i].set_title(col, fontsize=18, color=\"#b30059\")   # larger title font\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "    axes[i].tick_params(axis='both', labelsize=12)          # larger tick labels\n",
    "\n",
    "# Remove unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Distribution of Numerical Features\", fontsize=24, color=\"#000000\", y=1.02)  # larger main title\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ec1f3f9",
   "metadata": {},
   "source": [
    "\n",
    "The data distribution and the statistics presented above reveal some anomalies and irregularities in the dataset. These issues will be examined and addressed in the following section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f213cbe",
   "metadata": {},
   "source": [
    "###  Features Inspection Anomalies Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be716d76",
   "metadata": {},
   "source": [
    "#### Track Year and Album release date\n",
    "Looking at the distribution of values in the track year in the previous section, we notice some entries before 1950 and after 2025, which don’t make much sense. Similarly, there are album release dates after 2025 that seem unrealistic. Therefore, we will investigate these cases further to understand the cause and decide how to correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588b410",
   "metadata": {},
   "source": [
    "The following code groups songs and albums into 20-year intervals based on their release years and visualizes the percentage distribution in each range. It first cleans and converts the year fields, then calculates how many songs or albums fall into each 20-year period.\n",
    "\n",
    "Result:\n",
    "From the plots, we can see that more than half of the songs and albums were released between 2000 and 2020, indicating that most of the data comes from the recent two decades"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf8eaa11",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# --- Convert 'year' to numeric ---\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "\n",
    "# --- Drop missing years and make a copy ---\n",
    "df_years = df.dropna(subset=['year']).copy()\n",
    "\n",
    "# --- Create 20-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_years['year'].min())\n",
    "end = int(df_years['year'].max())\n",
    "bins = list(range(start, end, 20)) + [end]  # ensure last bin ends exactly at max\n",
    "labels = [f\"{b}-{min(b+19, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_years['year_group'] = pd.cut(df_years['year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_years['year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='year_group', y='percent', hue='year_group', palette='viridis', legend=False)\n",
    "\n",
    "# --- Add percentage labels ---\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Songs by 20-Year Intervals\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Convert 'album_release_date' to datetime and extract year ---\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "df['album_year'] = df['album_release_date'].dt.year\n",
    "\n",
    "# --- Drop missing album years and make a copy ---\n",
    "df_album_years = df.dropna(subset=['album_year']).copy()\n",
    "\n",
    "# --- Create 20-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_album_years['album_year'].min())\n",
    "end = int(df_album_years['album_year'].max())\n",
    "bins = list(range(start, end, 20)) + [end]\n",
    "labels = [f\"{b}-{min(b+19, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_album_years['album_year_group'] = pd.cut(df_album_years['album_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_album_years['album_year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'album_year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='album_year_group', y='percent', hue='album_year_group', palette='mako', legend=False)\n",
    "\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Songs by Album Release Year (20-Year Intervals)\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Album Release Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d95c31f",
   "metadata": {},
   "source": [
    "Descriptive Statistics\n",
    "The summary statistics show that the song release years range from 1900 to 2100, with an average around 2013, indicating some unrealistic future values.\n",
    "For album release years, the range is 1962 to 2025, with an average around 2017, which is more reasonable and reflects that most albums were released in the last decade."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddbfd109",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# For the 'year' column\n",
    "print(df['year'].describe())  \n",
    "\n",
    "# For 'album_release_date' (datetime type)\n",
    "df['album_release_year'] = df['album_release_date'].dt.year\n",
    "print(df['album_release_year'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "273d0b48",
   "metadata": {},
   "source": [
    "Number of Songs before 1950 and after 2025 "
   ]
  },
  {
   "cell_type": "code",
   "id": "820ce44b",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "tracks['year'] = pd.to_numeric(tracks['year'], errors='coerce')\n",
    "\n",
    "# Count songs released before 1950 and after 2025\n",
    "songs_before_1950 = tracks[tracks['year'] < 1950].shape[0]\n",
    "songs_after_2025 = tracks[tracks['year'] > 2025].shape[0]\n",
    "\n",
    "print(f\"Number of songs before 1950: {songs_before_1950}\")\n",
    "# Filter songs with year > 2025\n",
    "future_songs = tracks[tracks['year'] <1950 ][['full_title', 'album_release_date', 'year']]\n",
    "# Display the results\n",
    "display(future_songs)\n",
    "\n",
    "print(f\"Number of songs after 2025: {songs_after_2025}\")\n",
    "# Filter songs with year > 2025\n",
    "future_songs = tracks[tracks['year'] > 2025][['full_title', 'album_release_date', 'year']]\n",
    "# Display the results\n",
    "display(future_songs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bd9957f",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "tracks['album_release_date'] = pd.to_datetime(tracks['album_release_date'], errors='coerce')\n",
    "\n",
    "cutoff_after = pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "album_release_date_after_2025 = tracks[tracks['album_release_date'] > cutoff_after].shape[0]\n",
    "\n",
    "print(f\"Number of album_release_date after 2025: {album_release_date_after_2025}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "772d995d",
   "metadata": {},
   "source": [
    "#### Artist's BirthDate"
   ]
  },
  {
   "cell_type": "code",
   "id": "2821fb99",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "df['birth_year'] = df['birth_date'].dt.year\n",
    "\n",
    "# --- Drop missing birth years and make a copy ---\n",
    "df_birth_years = df.dropna(subset=['birth_year']).copy()\n",
    "\n",
    "# --- Create 20-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_birth_years['birth_year'].min())\n",
    "end = int(df_birth_years['birth_year'].max())\n",
    "bins = list(range(start, end, 10)) + [end]\n",
    "labels = [f\"{b}-{min(b+9, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_birth_years['birth_year_group'] = pd.cut(df_birth_years['birth_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_birth_years['birth_year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'birth_year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='birth_year_group', y='percent', hue='birth_year_group', palette='coolwarm', legend=False)\n",
    "\n",
    "# --- Add percentage labels ---\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Artists by Birth Year (10-Year Intervals)\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Birth Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f10bb2d3",
   "metadata": {},
   "source": [
    "#### Active start"
   ]
  },
  {
   "cell_type": "code",
   "id": "0be35580",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "df['active_start_year'] = df['active_start'].dt.year\n",
    "\n",
    "# --- Drop missing active_start years and make a copy ---\n",
    "df_active_years = df.dropna(subset=['active_start_year']).copy()\n",
    "\n",
    "# --- Create 10-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_active_years['active_start_year'].min())\n",
    "end = int(df_active_years['active_start_year'].max())\n",
    "bins = list(range(start, end, 10)) + [end]\n",
    "labels = [f\"{b}-{min(b+9, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_active_years['active_year_group'] = pd.cut(df_active_years['active_start_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_active_years['active_year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'active_year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='active_year_group', y='percent', hue='active_year_group', palette='coolwarm', legend=False)\n",
    "\n",
    "# --- Add percentage labels ---\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Artists by Active Start Year (10-Year Intervals)\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Active Start Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6deb1836",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "id": "6abc1d62",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# --- Count popularity values ---\n",
    "pop_counts = (df['popularity'].astype(str)).value_counts().sort_index()  # sort index for ascending y-axis\n",
    "\n",
    "# --- Horizontal bar plot ---\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=pop_counts.values, y=pop_counts.index,hue=pop_counts.index, palette='viridis')\n",
    "plt.xlabel(\"Number of Songs\", fontsize=12)\n",
    "plt.ylabel(\"Popularity\", fontsize=12)\n",
    "plt.title(\"Distribution of Song Popularity\", fontsize=16, pad=15)\n",
    "\n",
    "# --- Add count labels ---\n",
    "for i, val in enumerate(pop_counts.values):\n",
    "    plt.text(val + 0.5, i, f\"{val}\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "02053b73",
   "metadata": {},
   "source": [
    "####  Artists Location Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95977a",
   "metadata": {},
   "source": [
    "##### Checking if all the coordinates of the artists are inside italy's coordinates"
   ]
  },
  {
   "cell_type": "code",
   "id": "1322bc42",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "geo_outliers = df[(df['latitude'] < 35.5) | (df['latitude'] > 47.1) |\n",
    "                  (df['longitude'] < 6.6) | (df['longitude'] > 18.5)]\n",
    "print(f\"Number of Geographic coordinates outside Italy range: {len(geo_outliers)} records\")\n",
    "display(geo_outliers[['name_artist', 'latitude', 'longitude', 'birth_place']].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7efe781c",
   "metadata": {},
   "source": [
    "##### Artists' Country Values\n",
    "\n",
    "All the countries have the value of Italia"
   ]
  },
  {
   "cell_type": "code",
   "id": "7deac616",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Count the occurrences of each country\n",
    "country_counts = df['country'].value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "country_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Distribution of Artists by Country', fontsize=14, pad=12)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Artists', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8cc8ca2",
   "metadata": {},
   "source": [
    "##### Checking if there is an artist his/her country not Italy but his/her coordinates are in Italy"
   ]
  },
  {
   "cell_type": "code",
   "id": "adea6290",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Filter rows where country is not Italy and coordinates are present\n",
    "non_italy_with_coords = df[\n",
    "    (df['country'].notna()) & \n",
    "    (df['country'] != \"Italia\") & \n",
    "    (df['latitude'].notna()) & \n",
    "    (df['longitude'].notna())\n",
    "]\n",
    "\n",
    "# Count the number of such records\n",
    "num_records = len(non_italy_with_coords)\n",
    "print(f\"Number of non-Italy records with coordinates: {num_records}\")\n",
    "\n",
    "# Show the records\n",
    "print(non_italy_with_coords[['country', 'latitude', 'longitude']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3298f16",
   "metadata": {},
   "source": [
    "##### Artists Nationality Distribution\n",
    "\n",
    "Almost all artists are Italian (99.5%), with a small minority from Argentina (0.5%)."
   ]
  },
  {
   "cell_type": "code",
   "id": "4798aa72",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "print(df['nationality'].value_counts())\n",
    "# Count and calculate percentages\n",
    "nat_counts = df['nationality'].value_counts()\n",
    "nat_percent = (nat_counts / nat_counts.sum()) * 100\n",
    "nat_df = nat_percent.reset_index()\n",
    "nat_df.columns = ['nationality', 'percent']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=nat_df.head(20),  # show top 20 nationalities\n",
    "    x='percent',\n",
    "    y='nationality',\n",
    "    hue='nationality',\n",
    "    palette='crest',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Artists by Nationality\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Nationality\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(nat_df.head(20)['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, nat_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe28b3e8",
   "metadata": {},
   "source": [
    "##### Checking if there are artists with Non-Italian Nationality and Italian Coordinates (doubt)\n",
    "\n",
    "There are 40 artists with a nationality other than Italian (all Argentinian) but also have italian geographic coordinates. All these 40 artists share the same coordinates (After searching for this coordinates refers to the province of Parma)."
   ]
  },
  {
   "cell_type": "code",
   "id": "5fd9d0f7",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Filter rows where country is not Italy and coordinates are present\n",
    "non_italy_with_coords = df[\n",
    "    (df['nationality'].notna()) & \n",
    "    (df['nationality'] != \"Italia\") & \n",
    "    (df['latitude'].notna()) & \n",
    "    (df['longitude'].notna())\n",
    "]\n",
    "\n",
    "# Count the number of such records\n",
    "num_records = len(non_italy_with_coords)\n",
    "print(f\"Number of non-Italy Nationality records with coordinates: {num_records}\")\n",
    "\n",
    "# Show the records\n",
    "print(non_italy_with_coords[['nationality','latitude', 'longitude']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec08490a",
   "metadata": {},
   "source": [
    "##### Nationality and Country Coherence Check  (doubt)\n",
    "\n",
    "This check ensures that each artist’s nationality matches the country. For example, artists from Italy should have nationality Italia, and those from Argentine should have Argentina.\n",
    "The results show no mismatches, meaning all records have consistent country–nationality relationships."
   ]
  },
  {
   "cell_type": "code",
   "id": "02aa1175",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# Example mapping of country → expected nationality\n",
    "country_to_nationality = {\n",
    "    \"Italy\": \"Italia\",\n",
    "    \"Argentine\": \"Argentina\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to check nationality vs country\n",
    "def check_nationality_country(row):\n",
    "    if pd.notna(row['country']) and pd.notna(row['nationality']):\n",
    "        expected_nationality = country_to_nationality.get(row['country'])\n",
    "        if expected_nationality and expected_nationality != row['nationality']:\n",
    "            return True  # incoherent\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['nationality_country_mismatch'] = df.apply(check_nationality_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['nationality_country_mismatch'].sum()\n",
    "print(f\"Number of nationality-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['nationality_country_mismatch']]\n",
    "print(mismatched_records[['country', 'nationality']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a38becc0",
   "metadata": {},
   "source": [
    "##### Distribution of Artist's Birth Places\n",
    "\n",
    "The majority of artists were born in major Italian cities, with Milano (1,843) and Roma (1,048) being the most frequent birthplaces, indicating a strong concentration of artists from these cultural and economic centers.\n",
    "\n",
    "Smaller Italian towns such as Senigallia (443), Torino (397), and Avellino (329) also show notable representation, suggesting a widespread national distribution beyond just the biggest cities.\n",
    "\n",
    "Only a few artists were born outside Italy — such as Buenos Aires (40) and Almería (26) — representing less than 1% of the total, which confirms that the dataset is predominantly composed of Italian-born artists."
   ]
  },
  {
   "cell_type": "code",
   "id": "75617f9f",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Count occurrences and calculate percentages\n",
    "birth_place_counts = df['birth_place'].value_counts()\n",
    "print(birth_place_counts)\n",
    "birth_place_percent = (birth_place_counts / len(df)) * 100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.bar(birth_place_counts.index, birth_place_counts.values, color='skyblue')\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Distribution of Birth Places', fontsize=14)\n",
    "plt.xlabel('Birth Place')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add both count and percentage labels above bars\n",
    "for i, (count, percent) in enumerate(zip(birth_place_counts.values, birth_place_percent.values)):\n",
    "    plt.text(i, count + 10, f\"{count:,} \\n({percent:.1f}%)\", \n",
    "             ha='center', va='bottom', fontsize=6, color='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ac490f1",
   "metadata": {},
   "source": [
    "##### Checking Birth Place–Country Consistency (doubt)\n",
    "\n",
    "This section verifies whether each artist’s birth place matches their country. It defines a list of known Italian cities and maps a few foreign cities to their respective countries. T The result shows the number of mismatches and lists the inconsistent records. The results show 26 mismatches, all involving artists born in Almería (Spain) but recorded with the country Italia"
   ]
  },
  {
   "cell_type": "code",
   "id": "50935057",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# List of Italian cities from the data\n",
    "italian_cities = [\n",
    "    \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "    \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "    \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "    \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "    \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "    \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "    \"Lodi\"\n",
    "]\n",
    "\n",
    "# Map known foreign cities to their countries\n",
    "foreign_cities_to_country = {\n",
    "    \"Singapore\": \"Singapore\",\n",
    "    \"Buenos Aires\": \"Argentina\",\n",
    "    \"Almería\": \"Spagna\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to check birth_place vs country\n",
    "def check_birth_place_country(row):\n",
    "    if pd.notna(row['birth_place']) and pd.notna(row['country']):\n",
    "        if row['birth_place'] in italian_cities and row['country'] != \"Italia\":\n",
    "            return True  # mismatch\n",
    "        elif row['birth_place'] in foreign_cities_to_country:\n",
    "            if row['country'] != foreign_cities_to_country[row['birth_place']]:\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_country_mismatch'] = df.apply(check_birth_place_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_country_mismatch'].sum()\n",
    "print(f\"Number of birth_place-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['birth_place_country_mismatch']]\n",
    "display(mismatched_records[['birth_place', 'country',]])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ab7a1bc",
   "metadata": {},
   "source": [
    "##### Birth Place vs Nationality Consistency Check\n",
    "\n",
    "This step verifies that each artist’s birth place aligns with their nationality. A list of Italian cities and a mapping of known foreign cities (like Almería, Buenos Aires, and Singapore) were used for comparison.\n",
    "\n",
    "The results show 107 mismatches, mainly involving artists born in Almería or Singapore but labeled with the nationality Italia, indicating possible errors or inconsistencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "eefa29ea",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# List of Italian cities\n",
    "italian_cities = [\n",
    "    \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "    \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "    \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "    \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "    \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "    \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "    \"Lodi\"\n",
    "]\n",
    "\n",
    "\n",
    "# Map special foreign cities to nationality\n",
    "foreign_cities_to_nationality = {\n",
    "    \"Singapore\": \"Singapore\",\n",
    "    \"Buenos Aires\": \"Argentina\",\n",
    "    \"Almería\": \"Spagna\",\n",
    "}\n",
    "\n",
    "# Function to check birth_place vs nationality\n",
    "def check_birth_place_nationality(row):\n",
    "    if pd.notna(row['birth_place']) and pd.notna(row['nationality']):\n",
    "        if row['birth_place'] in italian_cities and row['nationality'] != \"Italia\":\n",
    "            return True  # mismatch\n",
    "        elif row['birth_place'] in foreign_cities_to_nationality:\n",
    "            if row['nationality'] != foreign_cities_to_nationality[row['birth_place']]:\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_nationality_mismatch'] = df.apply(check_birth_place_nationality, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_nationality_mismatch'].sum()\n",
    "print(f\"Number of birth_place-nationality mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['birth_place_nationality_mismatch']]\n",
    "print(mismatched_records[['name','birth_place', 'nationality','country']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "506d46e9",
   "metadata": {},
   "source": [
    "##### Distribution of Songs by Province and Region\n",
    "\n",
    "This code calculates and visualizes the percentage distribution of songs by province and region. It counts occurrences, converts them to percentages, and displays bar charts with labeled values to show which areas have the highest song representation"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcbbc8ad",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# Count occurrences and convert to percentages\n",
    "province_counts = df['province'].value_counts()\n",
    "province_percent = (province_counts / province_counts.sum()) * 100\n",
    "print('Provinces')\n",
    "print(province_counts)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "province_df = province_percent.reset_index()\n",
    "province_df.columns = ['province', 'percent']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=province_df.head(20),  # top 20 provinces if you want\n",
    "    x='percent',\n",
    "    y='province',\n",
    "    hue='province',\n",
    "    palette='viridis',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Songs by Province\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Province\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(province_df.head(20)['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, province_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "region_counts = df['region'].value_counts()\n",
    "print('Regions')\n",
    "print(region_counts)\n",
    "region_percent = (region_counts / region_counts.sum()) * 100\n",
    "region_df = region_percent.reset_index()\n",
    "region_df.columns = ['region', 'percent']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=region_df,\n",
    "    x='percent',\n",
    "    y='region',\n",
    "    hue='region',\n",
    "    palette='coolwarm',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Songs by Region\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Region\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(region_df['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, region_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2f730b2",
   "metadata": {},
   "source": [
    "##### Province/Region – Country Consistency Check\n",
    "\n",
    "This code verifies that Italian provinces and regions are correctly associated with the country \"Italia\""
   ]
  },
  {
   "cell_type": "code",
   "id": "2d313485",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Example mapping of Italian regions to their provinces (from your data)\n",
    "region_provinces = {\n",
    "    \"Lombardia\": [\"Milano\", \"Monza e della Brianza\", \"Brescia\", \"Varese\", \"Lodi\"],\n",
    "    \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\"],\n",
    "    \"Lazio\": [\"Roma\"],\n",
    "    \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "    \"Piemonte\": [\"Torino\"],\n",
    "    \"Sardegna\": [\"Cagliari\", \"Gallura\"],\n",
    "    \"Puglia\": [\"Lecce\", \"Taranto\"],\n",
    "    \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "    \"Sicilia\": [\"Enna\"],\n",
    "    \"Emilia-Romagna\": [\"Bologna\"],\n",
    "    \"Calabria\": [\"Reggio Calabria\"],\n",
    "    \"Marche\": [\"Ancona\", \"Ascoli Piceno\"],\n",
    "    \"Toscana\": [\"Firenze\"]\n",
    "}\n",
    "\n",
    "# Flatten all Italian provinces for quick lookup\n",
    "all_italian_provinces = [prov for provs in region_provinces.values() for prov in provs]\n",
    "\n",
    "# Function to check province/region ↔ country\n",
    "def check_province_region_country(row):\n",
    "    if pd.notna(row['country']):\n",
    "        if pd.notna(row['province']) and row['province'] in all_italian_provinces:\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "        elif pd.notna(row['region']) and row['region'] in region_provinces.keys():\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['province_region_country_mismatch'] = df.apply(check_province_region_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['province_region_country_mismatch'].sum()\n",
    "print(f\"Number of province/region-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['province_region_country_mismatch']]\n",
    "print(mismatched_records[['province', 'region', 'country']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5d57ebb",
   "metadata": {},
   "source": [
    "##### Province/Region vs Birth Place – Consistency Check (doubt)\n",
    "\n",
    "This check compares each artist’s birth_place with the corresponding province and region. Mismatches occur when the province or region does not align with the birth_place. There are 2,901 mismatches between birth_place and province/region."
   ]
  },
  {
   "cell_type": "code",
   "id": "d81e91e5",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Updated mapping of Italian regions to provinces including all birth_places in your data\n",
    "region_provinces = {\n",
    "    \"Lombardia\": [\"Milano\", \"Vimercate\", \"Sesto San Giovanni\", \"Alpignano\", \"Fiumicino\",\n",
    "                  \"Brescia\", \"Grugliasco\", \"Rho\", \"Gallarate\", \"Desenzano del Garda\", \"Lodi\", \"San Siro\"],\n",
    "    \"Lazio\": [\"Roma\"],\n",
    "    \"Piemonte\": [\"Torino\"],\n",
    "    \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\", \"Scafati\", \"Nocera Inferiore\"],\n",
    "    \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "    \"Sardegna\": [\"Cagliari\", \"Olbia\", \"Gallura\"],\n",
    "    \"Puglia\": [\"Lecce\", \"Taranto\", \"Grottaglie\", \"Sternatia\", \"San Benedetto del Tronto\"],\n",
    "    \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "    \"Sicilia\": [\"Enna\", \"Nicosia\"],\n",
    "    \"Emilia-Romagna\": [\"Bologna\"],\n",
    "    \"Calabria\": [\"Reggio Calabria\"],\n",
    "    \"Marche\": [\"Ancona\", \"Senigallia\", \"Ascoli Piceno\"],\n",
    "    \"Toscana\": [\"Firenze\", \"Scampia\", \"Padova\"]\n",
    "}\n",
    "\n",
    "# Flatten province → region mapping\n",
    "province_to_region = {prov: reg for reg, provs in region_provinces.items() for prov in provs}\n",
    "\n",
    "# Function to check birth_place ↔ province/region\n",
    "def check_birth_place_province_region(row):\n",
    "    if pd.notna(row['birth_place']):\n",
    "        # Only check Italian birth_places\n",
    "        if row['birth_place'] in province_to_region:\n",
    "            expected_region = province_to_region[row['birth_place']]\n",
    "            # Compare province and region if available\n",
    "            if (pd.notna(row['province']) and row['province'] != row['birth_place']) or \\\n",
    "               (pd.notna(row['region']) and row['region'] != expected_region):\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_province_region_mismatch'] = df.apply(check_birth_place_province_region, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_province_region_mismatch'].sum()\n",
    "print(f\"Number of birth_place-province/region mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show mismatched records\n",
    "mismatched_records = df[df['birth_place_province_region_mismatch']]\n",
    "print(mismatched_records[['birth_place', 'province', 'region']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78b1267a",
   "metadata": {},
   "source": [
    "##### Geographic Distribution of Artists by Province and Region\n",
    "\n",
    "This analysis aggregates the number of artists by their latitude, longitude, province, and region. The resulting table shows the locations with the highest concentration of artists at the top. For example, Milano (Lombardia) has the most artists with 1,843, followed by Roma (Lazio) with 1,048, and Torino (Piemonte) with 397. The code also generates a map where the size and color of the points reflect the number of artists per location, providing a clear visual of artist density across Italy."
   ]
  },
  {
   "cell_type": "code",
   "id": "c1a7a7a7",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Aggregate by latitude and longitude to count number of artists\n",
    "location_counts = df.groupby(['latitude', 'longitude', 'region', 'province']).size().reset_index(name='num_artists')\n",
    "\n",
    "# Sort by number of artists descending\n",
    "location_counts = location_counts.sort_values(by='num_artists', ascending=False)\n",
    "\n",
    "# Print the sorted table\n",
    "print(location_counts)\n",
    "\n",
    "# Define a color scale\n",
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "\n",
    "# Create the scatter map\n",
    "fig = px.scatter_mapbox(\n",
    "    location_counts,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    hover_data=[\"region\", \"province\", \"num_artists\"],  # show count on hover\n",
    "    size=\"num_artists\",  # size of marker represents number of artists\n",
    "    color=\"num_artists\",  # color also shows density\n",
    "    color_continuous_scale=color_scale,\n",
    "    zoom=5,\n",
    "    height=800,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Outliers Detection",
   "id": "478f6aa3e7344e77"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "df_clean = df.copy()\n",
    "print(f\"Original size: {df_clean.shape}\")\n",
    "\n",
    "# Handle critical NaNs\n",
    "df_clean = df_clean.dropna(subset=['lyrics'])\n",
    "print(f\"Rows with missing 'lyrics' removed.\")\n",
    "print(f\"Size after handling NaNs: {df_clean.shape}\")"
   ],
   "id": "49019d2c5be1dc1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Numerical Feature Definition\n",
    "\n",
    "# List of key numerical columns to analyze\n",
    "numerical_features = [\n",
    "    'stats_pageviews', 'swear_IT', 'swear_EN',\n",
    "    'n_sentences', 'n_tokens', 'tokens_per_sent', 'char_per_tok', 'lexical_density', 'avg_token_per_clause',\n",
    "    'bpm', 'centroid', 'rolloff', 'flux', 'rms', 'zcr', 'flatness', 'spectral_complexity', 'pitch', 'loudness',\n",
    "    'duration_ms', 'popularity'\n",
    "]\n",
    "\n",
    "# Ensure all are numeric and remove any rows with NaNs\n",
    "df_clean[numerical_features] = df_clean[numerical_features].apply(pd.to_numeric, errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=numerical_features)\n",
    "print(f\"DataFrame ready for outlier analysis. Size: {df_clean.shape}\")"
   ],
   "id": "14de4cf6b61f8124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Visual Analysis (Box Plot)\n",
    "\n",
    "for col in numerical_features:\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.boxplot(data=df_clean, x=col, color=\"skyblue\")\n",
    "    plt.title(f\"Box Plot of '{col}'\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ],
   "id": "1d888995ba55ce32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "outlier_report = []\n",
    "\n",
    "for col in numerical_features:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    count_low = df_clean[df_clean[col] < lower_bound].shape[0]\n",
    "    count_high = df_clean[df_clean[col] > upper_bound].shape[0]\n",
    "    total_count = df_clean[col].count()\n",
    "\n",
    "    if total_count > 0:\n",
    "        total_perc = (count_low + count_high) / total_count * 100\n",
    "    else:\n",
    "        total_perc = 0\n",
    "\n",
    "    outlier_report.append({\n",
    "        'feature': col,\n",
    "        'lower_bound': round(lower_bound, 2),\n",
    "        'upper_bound': round(upper_bound, 2),\n",
    "        'outliers_low': count_low,\n",
    "        'outliers_high': count_high,\n",
    "        'total_outliers': count_low + count_high,\n",
    "        'total_perc': round(total_perc, 2)\n",
    "    })\n",
    "\n",
    "# Print the report\n",
    "outlier_df = pd.DataFrame(outlier_report).set_index('feature')\n",
    "print(outlier_df.sort_values(by='total_perc', ascending=False))"
   ],
   "id": "d6287786c35b0181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Filtering and plotting outlier data\n",
    "plot_data = outlier_df[outlier_df['total_outliers'] > 0]\n",
    "\n",
    "# Sort the number of outliers in descending order\n",
    "plot_data = plot_data.sort_values(by='total_outliers', ascending=False)\n",
    "plot_data = plot_data[['outliers_low', 'outliers_high']]\n",
    "\n",
    "print(f\"Found {len(plot_data)} features with outliers.\")\n",
    "\n",
    "if plot_data.empty:\n",
    "    print(\"No outliers to plot.\")\n",
    "else:\n",
    "\n",
    "    colors = ['#d36ba8', '#b51272']\n",
    "\n",
    "    ax = plot_data.plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        figsize=(12, 10),\n",
    "        color=colors,\n",
    "        width=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Outlier counts  (method 1.5 x IQR)', fontsize=16, pad=20)\n",
    "    plt.xlabel('Number of Ouliers', fontsize=12) # 'Ouliers' è probabilmente un refuso per 'Outliers'\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.legend(\n",
    "        title='Type of Outlier',\n",
    "        labels=['Low Outlier (< lower bound.)', 'High Outlier (> Upper bound.)'],\n",
    "        loc='lower right'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "769d42a7e2bbacf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Skewed Features (Needing Log-Transform): The report shows that features like stats_pageviews (8.93% outliers) and swear_EN (8.85%) are not normally distributed; they are highly skewed. The box plots (e.g., image_ce5d57.png for stats_pageviews) visually confirm this, showing the data is \"squashed\" to one side with a long tail of outliers. These are perfect candidates for Strategy 1: Log Transformation.\n",
    "\n",
    "2. Other Outliers (Needing Clipping): The rest of the features (like avg_token_per_clause, lexical_density, loudness, etc.) have a smaller, more manageable percentage of outliers (mostly 1-4%). These represent legitimate but extreme values. They are ideal for Strategy 2: Clipping, which will reduce their influence without deleting them."
   ],
   "id": "7fcfda36fea2bef6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gestione Outlier",
   "id": "8cff37860727204a"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Trasformazione Log\n",
    "\n",
    "skewed_cols = [\n",
    "    'stats_pageviews',\n",
    "    'swear_EN',\n",
    "    'avg_token_per_clause',\n",
    "    'swear_IT',\n",
    "    'rms',\n",
    "    'duration_ms',\n",
    "    'n_sentences',\n",
    "    'n_tokens',\n",
    "    'rolloff',\n",
    "    'zcr'\n",
    "]\n",
    "\n",
    "for col in skewed_cols:\n",
    "    if col in df_clean.columns:\n",
    "        new_col_name = f\"{col}_log\"\n",
    "        df_clean[new_col_name] = np.log1p(df_clean[col])\n",
    "        print(f\"'{new_col_name}' Column created.\")\n"
   ],
   "id": "1bd30ddbce64262",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Visual Check of Transformation\n",
    "for col in skewed_cols:\n",
    "    col_log = f\"{col}_log\"\n",
    "\n",
    "    if col in df_clean.columns and col_log in df_clean.columns:\n",
    "\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # --- Plot 1: Original Distribution ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df_clean[col], kde=True, bins=50, color='#d36ba8')\n",
    "        plt.title(f'Original Distribution of \\n{col}', fontsize=14)\n",
    "        plt.xlabel('Original Value')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        # --- Plot 2: Transformed Distribution ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(df_clean[col_log], kde=True, bins=50, color='#d36ba8')\n",
    "        plt.title(f'Transformed Distribution of \\n{col_log}', fontsize=14) #\n",
    "        plt.xlabel('Log-Transformed Value (log(1+x))')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "\n",
    "        plt.suptitle(f'Transformation Check for: {col}', fontsize=18, y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "3ec236f979bb4048",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Clipping\n",
    "\n",
    "clipping_cols = [col for col in numerical_features if col not in skewed_cols]\n",
    "\n",
    "for col in clipping_cols:\n",
    "    if col in df_clean.columns:\n",
    "\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "        count_low = (df_clean[col] < lower_bound).sum()\n",
    "        count_high = (df_clean[col] > upper_bound).sum()\n",
    "\n",
    "\n",
    "        if (count_low + count_high) > 0:\n",
    "            df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)\n",
    "            print(f\"Clipping applied to '{col}': {count_low+count_high} values clipped.\")"
   ],
   "id": "9d4ec33e302a6787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Final Check After Outlier Handling\n",
    "\n",
    "transformed_cols = [f\"{col}_log\" for col in skewed_cols]\n",
    "\n",
    "# Combine the lists for the final check\n",
    "final_feature_list = transformed_cols + clipping_cols"
   ],
   "id": "33ca119a80a8da6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Box plot\n",
    "\n",
    "for col in final_feature_list:\n",
    "    if col in df_clean.columns:\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        # Usiamo 'color' per evitare il FutureWarning\n",
    "        sns.boxplot(data=df_clean, x=col, color=\"skyblue\")\n",
    "        plt.title(f\"Box Plot Finale di '{col}'\")\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        plt.show()"
   ],
   "id": "afb9a662fc2f0cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Statistical Check (Final IQR Report)\n",
    "\n",
    "final_outlier_report = []\n",
    "\n",
    "for col in final_feature_list:\n",
    "    if col in df_clean.columns and pd.api.types.is_numeric_dtype(df_clean[col]):\n",
    "\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        count_low = (df_clean[col] < lower_bound).sum()\n",
    "        count_high = (df_clean[col] > upper_bound).sum()\n",
    "\n",
    "        total_count = df_clean[col].count()\n",
    "\n",
    "        if total_count > 0:\n",
    "            total_perc = (count_low + count_high) / total_count * 100\n",
    "        else:\n",
    "            total_perc = 0\n",
    "\n",
    "        final_outlier_report.append({\n",
    "            'feature': col,\n",
    "            'lower_bound': round(lower_bound, 2),\n",
    "            'upper_bound': round(upper_bound, 2),\n",
    "            'outliers_low': count_low,\n",
    "            'outliers_high': count_high,\n",
    "            'total_outliers': count_low + count_high,\n",
    "            'total_perc': round(total_perc, 2)\n",
    "        })\n",
    "\n",
    "# Print the final report\n",
    "if final_outlier_report:\n",
    "    final_outlier_df = pd.DataFrame(final_outlier_report).set_index('feature')\n",
    "    print(final_outlier_df.sort_values(by='total_perc', ascending=False))"
   ],
   "id": "777c1803a99bc35f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Filtering and plotting outlier data\n",
    "plot_data = final_outlier_df[final_outlier_df['total_outliers'] > 0]\n",
    "\n",
    "# Sort the number of outliers in descending order\n",
    "plot_data = plot_data.sort_values(by='total_outliers', ascending=False)\n",
    "plot_data = plot_data[['outliers_low', 'outliers_high']]\n",
    "\n",
    "print(f\"Found {len(plot_data)} features with outliers.\")\n",
    "\n",
    "if plot_data.empty:\n",
    "    print(\"No outliers to plot.\")\n",
    "else:\n",
    "\n",
    "    colors = ['#d36ba8', '#b51272']\n",
    "\n",
    "    ax = plot_data.plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        figsize=(12, 10),\n",
    "        color=colors,\n",
    "        width=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Outlier counts  (method 1.5 x IQR)', fontsize=16, pad=20)\n",
    "    plt.xlabel('Number of Ouliers', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.legend(\n",
    "        title='Type of Outlier',\n",
    "        labels=['Low Outlier (< lower bound.)', 'High Outlier (> Upper bound.)'],\n",
    "        loc='lower right'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "db4618cf4d7f4802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. \"Zero-Inflated\" Features (Statistical Artifact)\n",
    "Most of the data is 0. This means Q1=0 and Q3=0, so the IQR is 0. The statistical rule Q3 + (1.5 * IQR) calculates an upper bound of 0.\n",
    "The rule is \"broken\" for this data. It flags every non-zero value as an outlier, even after transformation.\n",
    "\n",
    "2. \"Natural Tails\" of a New Bell-Curve\n",
    "Example: n_sentences_log (711 outliers), n_tokens_log (634), stats_pageviews_log (17)\n",
    "The log transform successfully changed these skewed distributions into a symmetrical, \"bell-shaped\" (normal) distribution, as seen in your histograms.\n",
    "The 1.5 * IQR rule is very strict. The outliers it now finds are not errors; they are simply the natural tails of the new, healthy bell-curve."
   ],
   "id": "e449ab799de2992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multivariate analisys",
   "id": "bb395aeb699d1291"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Ensure there are no NaNs (should be clean already)\n",
    "df_analysis = df_clean[final_feature_list].dropna()\n",
    "print(f\"Data ready for analysis: {df_analysis.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_analysis)"
   ],
   "id": "ddcb773d1ccc852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Applying Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "\n",
    "# We train and get the predictions\n",
    "# The algorithm assigns:\n",
    "#  1 for Inliers (normal points)\n",
    "# -1 for Outliers (anomalous points)\n",
    "predictions = iso_forest.fit_predict(data_scaled)\n",
    "\n",
    "df_analysis['is_outlier_multi'] = predictions\n",
    "\n",
    "df_clean['is_outlier_multi'] = df_analysis['is_outlier_multi'].reindex(df_clean.index)\n",
    "\n",
    "outliers_multi = df_clean[df_clean['is_outlier_multi'] == -1]\n",
    "print(f\"\\nAnalysis completed.\")\n",
    "print(f\"Number of multivariate outliers identified: {len(outliers_multi)}\")\n",
    "\n",
    "# Show some of the records identified as anomalous\n",
    "print(\"\\nExamples of Multivariate Outliers:\")\n",
    "\n",
    "# Show the original columns and our clean columns\n",
    "display(outliers_multi[['full_title', 'primary_artist'] + final_feature_list].head())"
   ],
   "id": "337d2018e18d42be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It identified the 93 (2%) songs that are the most stylistically unique when combining all 21 features.\n",
    "The examples show two clear patterns:\n",
    "\n",
    "Lyrical Anomalies (Rosa Chemical): This artist is flagged repeatedly. The data shows his songs have a very rare combination: they are lyrically complex (avg_token_per_clause_log, lexical_density) AND have high profanity in both Italian and English (swear_IT_log, swear_EN_log). This makes them stand out from all other artists.\n",
    "\n",
    "Audio Anomalies (thasup): The \"thasup\" track is a perfect example of an audio outlier. It has a very slow bpm (82) but is at the maximum loudness (45) and maximum pitch (3191). This combination of \"slow, loud, and high-pitched\" is a very unusual audio profile.\n",
    "\n",
    "Conclusion: These 93 songs are \"stylistic outliers\" (like experimental tracks). They are not errors, but you should remove them before clustering to get clearer, more representative clusters of the main \"rap schools\"."
   ],
   "id": "5e85101ab158bff4"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# We use PCA to reduce the dimension and be able to plot the outliers\n",
    "\n",
    "# Reduce the scaled data to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "data_scaled_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame(data_scaled_2d, columns=['PC1', 'PC2'])\n",
    "plot_df['is_outlier'] = predictions\n",
    "\n",
    "# Create the Scatter Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='is_outlier',\n",
    "    palette={1: 'blue', -1: 'red'}, # Outliers in red\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('Isolation Forest Results (visualized with PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Type', labels=['Outlier (-1)', 'Inlier (1)'])\n",
    "plt.show()"
   ],
   "id": "eb033d244a9d2724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation analysis",
   "id": "9d4360ee2081ea1d"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# Select the clean data (only the inliers)\n",
    "if 'df_clustering' not in locals():\n",
    "    df_clustering = df_clean[df_clean['is_outlier_multi'] != -1]\n",
    "\n",
    "df_corr = df_clustering[final_feature_list].copy()\n",
    "print(f\"Data ready for correlation analysis: {df_corr.shape}\")\n",
    "\n",
    "# Calculate the correlation matrix (Pearson Method)\n",
    "corr_matrix = df_corr.corr(method='pearson')\n",
    "\n",
    "# Visualize the Heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Create a \"mask\" to hide the upper part\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='coolwarm',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9793ab3ba044a76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It clearly shows strong multicollinearity (redundancy) among your features. The data isn't noisy; it just confirms that some features measure the same underlying concept.\n",
    "- Song Length: duration_ms_log, n_tokens_log, and n_sentences_log are highly correlated (0.8 to 0.9), as they all measure \"song length\".\n",
    "\n",
    "- Loudness: rms_log and loudness are almost identical (0.9), measuring \"volume\".\n",
    "\n",
    "- Spectral Brightness: centroid, rolloff_log, and flatness are very strongly correlated (0.9 and -0.8), measuring the \"brightness\" or \"timbre\" of the sound."
   ],
   "id": "d00d5bdb7258dcb3"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "# 2. Select the clean data (BUT INCLUDING the multivariate outliers)\n",
    "df_corr_completo = df_clean[final_feature_list].copy()\n",
    "\n",
    "df_corr_completo = df_corr_completo.dropna()\n",
    "print(f\"Data ready for correlation analysis: {df_corr_completo.shape}\")\n",
    "\n",
    "# 3. Calculate the correlation matrix (Pearson Method)\n",
    "corr_matrix_completo = df_corr_completo.corr(method='pearson')\n",
    "\n",
    "# 4. Visualize the Heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix_completo, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix_completo,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='coolwarm',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Correlation Matrix (Including the 93 outliers from Isolation Forest)', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "eb0adc14441d7855",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
