{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "id": "ae403bba6cd2a766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n"
   ],
   "id": "2a7c4b67593d8eba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Understanding and Preparation"
   ],
   "id": "c5c208766c28da6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reading Data"
   ],
   "id": "89a0cb921b1ae7c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "artists_path = 'data\\\\artists.csv'\n",
    "tracks_path = 'data\\\\tracks.csv'"
   ],
   "id": "d93fa1237a0651b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code automatically detects the correct separator for two dataset files (tracks and artists) by checking which character — comma, semicolon, or tab — appears most in the first line. It then loads each file into a pandas DataFrame using the detected separator, prints their shapes, and displays the first few rows.\n",
    "\n",
    " The tracks dataset has 11,166 rows and 45 columns, while the artists dataset has 104 rows and 14 columns."
   ],
   "id": "d5a840762527683d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Funzione helper per capire il separatore corretto\n",
    "def detect_separator(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample = f.readline()\n",
    "    # Conta quanti separatori compaiono\n",
    "    seps = {',': sample.count(','), ';': sample.count(';'), '\\t': sample.count('\\t')}\n",
    "    best_sep = max(seps, key=seps.get)\n",
    "    print(f\"Detected separator for {filepath}: '{best_sep}'\")\n",
    "    return best_sep\n",
    "\n",
    "# Rileva automaticamente il separatore\n",
    "sep_tracks = detect_separator(tracks_path)\n",
    "sep_artists = detect_separator(artists_path)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "# Carica i dataset in base al separatore rilevato\n",
    "tracks = pd.read_csv(tracks_path, sep=sep_tracks, encoding='utf-8', engine='python')\n",
    "artists = pd.read_csv(artists_path, sep=sep_artists, encoding='utf-8', engine='python')\n",
    "\n",
    "# Mostra alcune info per verifica\n",
    "print(f\"Tracks shape: {tracks.shape}\")\n",
    "print(f\"Artists shape: {artists.shape}\")\n",
    "print('------------------------------------')\n",
    "\n",
    "print('TRACKS')\n",
    "display(tracks.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('ARTISTS')\n",
    "display(artists.head(3))\n"
   ],
   "id": "89bc5bf737c34bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Artists Features\")\n",
    "print(artists.columns.tolist())\n",
    "\n",
    "print(\"Tracks Features\")\n",
    "print(tracks.columns.tolist())\n"
   ],
   "id": "692672822d9c25e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Duplicates"
   ],
   "id": "5ed22847d7495bdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Artists\n",
    "\n",
    "The following code checks the artists dataset for duplicates in two ways: first, it looks for identical full rows to detect any completely repeated entries; then, it checks for duplicates specifically based on the artist ID and artist name columns.\n",
    "<B> After performing both checks, it confirms that there are no duplicate artists in the dataset </B>."
   ],
   "id": "b799d22a8f086f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for duplicated artists rows\n",
    "duplicates_artists = artists[artists.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated Artists rows: {duplicates_artists.shape[0]}\")\n",
    "display(duplicates_artists.head(5))"
   ],
   "id": "512efa93b16d1659"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for duplicated artists based on artist id\n",
    "duplicates_artists_id = artists[artists.duplicated(subset='id_author')]\n",
    "print(f\"Number of duplicated artist based on ID: {duplicates_artists_id.shape[0]}\")\n",
    "display(duplicates_artists_id.head(5))\n",
    "\n"
   ],
   "id": "c54e6ab491dce4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for duplicated artists based on artist name\t\n",
    "duplicates_artists_name = artists[artists.duplicated(subset='name')]\n",
    "print(f\"Number of duplicated artist based on Name: {duplicates_artists_name.shape[0]}\")\n",
    "display(duplicates_artists_name.head(5))"
   ],
   "id": "8477fabc8e727863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tracks\n",
    "Duplicates rows check has been also performed here.\n",
    "No duplicated rows were detected, indicating that all track entries are unique."
   ],
   "id": "8913cc87aba00725"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for duplicated tracks rows\n",
    "duplicates_tracks = tracks[tracks.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated rows: {duplicates_tracks.shape[0]}\")\n",
    "display(duplicates_tracks.head(5))"
   ],
   "id": "e4fe1f4f82ca17b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Duplicated Tracks based on ID"
   ],
   "id": "99c1f82436e92425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code checks the tracks dataset for duplicates based specifically on the track ID column. It identifies all rows where the same ID appears more than once, counts them, and displays them.\n",
    " It first identifies all rows where the same ID appears more than once, counts how many duplicated tracks exist, and displays them. Then, it counts how many times each track ID occurs in the dataset. \n",
    "\n",
    "<B> The result shows that there are 73 duplicated rows based on track IDs. \n",
    "Precisely we have 71  distinct IDs that have duplicates. </B>\n",
    "\n",
    "<B>one track ID is repeated four times, while the others are each repeated twice </B>"
   ],
   "id": "11d660f492c5ca16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for duplicated tracks based on track id\n",
    "duplicates_tracks_id = tracks[tracks.duplicated(subset='id')]\n",
    "print(f\"Number of duplicated Tracks rows based on ID: {duplicates_tracks_id.shape[0]}\")\n",
    "display(duplicates_tracks_id)\n",
    "\n",
    "\n",
    "# Count how many times each id_track appears\n",
    "id_counts = tracks['id'].value_counts()\n",
    "duplicate_id_counts = id_counts[id_counts > 1]\n",
    "\n",
    "print('Number of distinct IDs that have duplicates')\n",
    "print(duplicate_id_counts.size)\n",
    "print(\"Number of tracks for each id:\")\n",
    "print(duplicate_id_counts)\n"
   ],
   "id": "f74b40fc1353bf1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following code lists every full_title associated with each duplicated track ID. The results show 71 duplicated IDs in total. Most of these IDs are linked to two different songs, except for one ID that is associated with four songs (two pairs sharing the same title)."
   ],
   "id": "2e592d54c7244574"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find all duplicated track IDs\n",
    "duplicate_ids = tracks[tracks.duplicated(subset='id', keep=False)]\n",
    "\n",
    "# Group by 'id' and list all titles\n",
    "titles_per_id = duplicate_ids.groupby('id')['full_title'].apply(list)\n",
    "\n",
    "# Display each ID with all titles and the count of unique titles\n",
    "for track_id, titles in titles_per_id.items():\n",
    "    unique_count = len(set(titles))  # number of unique titles\n",
    "    print(f\"Track ID: {track_id} Number(of total songs: {len(titles)})(Unique titles: {unique_count})\")\n",
    "    for title in titles:\n",
    "        print(f\"  - {title}\")\n",
    "    print('----------------------------------------------------------')\n"
   ],
   "id": "52a63b396ffcca83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fixing Duplicated Tracks Id\n",
    "After reviewing the songs associated with the duplicated IDs, we found that each duplicated ID corresponds to different songs, except for one case that will be treated later. Therefore, the most reasonable solution is to modify the duplicated IDs by appending the row number to each one. This approach ensures that all songs are preserved while maintaining unique identifiers for every track."
   ],
   "id": "a8d959a4f0e8bb94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Identify duplicated IDs\n",
    "duplicate_mask = tracks.duplicated(subset='id', keep=False)\n",
    "\n",
    "# Assign new unique IDs only to duplicated rows\n",
    "tracks.loc[duplicate_mask, 'id'] = (\n",
    "    tracks.loc[duplicate_mask]\n",
    "    .apply(lambda x: f\"{x['id']}_{x.name}\", axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Example of updated duplicates:\")\n",
    "display(tracks[duplicate_mask][['id', 'full_title']])\n"
   ],
   "id": "f98575744c68b246"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Duplicated Tracks based on Title\n",
    "The following code identifies tracks that share the same full_title, meaning duplicate song titles. We found four duplicated tracks, corresponding to two pairs of songs with identical titles."
   ],
   "id": "c74be55bad5aed10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find duplicated full_title\n",
    "duplicate_titles = tracks[tracks.duplicated(subset='full_title', keep=False)]\n",
    "\n",
    "# Sort by full_title to see them together\n",
    "duplicate_titles = duplicate_titles.sort_values('full_title')\n",
    "\n",
    "print(f\"Tracks with duplicate track based on full_title: {duplicate_titles.shape[0]}\")\n",
    "display(duplicate_titles)\n"
   ],
   "id": "27a9f4df304879ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fixing Duplicated Tracks full_title"
   ],
   "id": "c341aaafede3ea2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The duplicated titles  — \"BUGIE by Madame (Ft. Carl Brave & Rkomi)\" and \"sentimi by Madame\" — actually refer to the same songs released in two different formats: one from the album and one from the single version. \n",
    "We decided to keep the duplicated tracks in the dataset but add a clear indication in the full_title to show whether each song comes from a single or an album. This way, all versions are preserved while making it easy to distinguish between different releases of the same song"
   ],
   "id": "eb1349afc189601a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Find duplicated full_titles\n",
    "duplicate_mask = tracks.duplicated(subset='full_title', keep=False)\n",
    "\n",
    "# Update only the duplicated titles by appending album_type\n",
    "tracks.loc[duplicate_mask, 'full_title'] = (\n",
    "    tracks.loc[duplicate_mask, 'full_title'] + \n",
    "    \" (\" + tracks.loc[duplicate_mask, 'album_type'].fillna('unknown').str.capitalize() + \")\"\n",
    ")\n",
    "\n",
    "# Verify the changes\n",
    "duplicate_titles = tracks[tracks.duplicated(subset='full_title', keep=False)].sort_values('full_title')\n",
    "display(duplicate_titles[['full_title', 'album_type', 'id']])\n"
   ],
   "id": "bd7fe109c7d1b90d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Merging the Datasets\n",
    "\n",
    "\n",
    "Merging the tracks and artists datasets into a single DataFrame called df. It matches rows where the <B> id_artist column in tracks</B> corresponds to the <B>id_author column in artists</B>, using a left join so that all tracks are kept even if some artists are missing. After merging, it prints the number of rows and columns in the unified dataset and shows the first three rows for inspection."
   ],
   "id": "5f8464fc9c3c5939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = tracks.merge(artists, left_on='id_artist', right_on='id_author', how='left')\n",
    "\n",
    "print(f\"Unified dataset: {df.shape[0]} rows , {df.shape[1]} columns\")\n",
    "display(df.head(3))\n",
    "\n"
   ],
   "id": "27766ac988319ab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Checking if there is a track without an artist"
   ],
   "id": "dd0fbb6aba84ca22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for tracks without a matching artist\n",
    "missing_artists = df[df['id_author'].isna()]\n",
    "\n",
    "print(f\"Number of tracks without an artist: {missing_artists.shape[0]}\")\n",
    "display(missing_artists.head(5))"
   ],
   "id": "8a9b20b30de0c637"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Checking if there is an artist without a track"
   ],
   "id": "3112d5bc1b1db921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "missing_artists = artists[~artists['id_author'].isin(tracks['id_artist'])]\n",
    "print(\"Number of artists without any tracks:\", len(missing_artists))"
   ],
   "id": "44f3d8e5943d1e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "This section explores the linear relationships between numerical features in the datasets.\n",
    "The **Pearson correlation coefficient** is used to measure how strongly two variables move together.\n",
    "\n",
    "The analysis is conducted separately for:\n",
    "- **tracks.csv** → to understand relationships among audio, linguistic, and popularity features;\n",
    "- **artists.csv** → to explore dependencies between demographic and career-related attributes;\n",
    "- and finally on the **merged dataset**, combining both perspectives.\n",
    "\n",
    "Interpreting the heatmaps:\n",
    "- **+1** → strong positive correlation (variables increase together)\n",
    "- **−1** → strong negative correlation (one increases while the other decreases)\n",
    "- **0** → little or no linear relationship\n",
    "\n",
    "This step helps identify redundant variables, reveal patterns between audio and artist features,\n",
    "and guide feature selection and dimensionality reduction before modeling."
   ],
   "id": "bbf16b86282d8868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_correlation_heatmap(df, title):\n",
    "    \"\"\"Plot a Pearson correlation heatmap for numeric features (white–magenta color scheme).\"\"\"\n",
    "\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    if numeric_df.shape[1] < 2:\n",
    "        print(f\"Skipping {title}: not enough numeric columns.\")\n",
    "        return\n",
    "\n",
    "    # Compute Pearson correlation matrix\n",
    "    corr_matrix = numeric_df.corr(method='pearson')\n",
    "\n",
    "    # Define custom color palette (white → light magenta → dark magenta)\n",
    "    cmap = sns.color_palette([\"#F6D6FF\", \"#E5A4FF\", \"#D873FF\", \"#C43EFF\", \"#9B00CC\"])\n",
    "\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        cmap=cmap,         # Custom magenta palette\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'}\n",
    "    )\n",
    "\n",
    "    # Style adjustments\n",
    "    plt.title(f\"Correlation Heatmap – {title}\", fontsize=16, pad=15, color=\"#333333\")\n",
    "    plt.xticks(rotation=45, ha='right', color=\"#333333\")\n",
    "    plt.yticks(rotation=0, color=\"#333333\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "267eb10580a66f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correlation Heatmap for TRACKS dataset\n",
    "print(\"=== Correlation Heatmap: TRACKS dataset ===\")\n",
    "plot_correlation_heatmap(tracks, \"Tracks Dataset\")\n",
    "\n",
    "# Correlation Heatmap for ARTISTS dataset\n",
    "print(\"=== Correlation Heatmap: ARTISTS dataset ===\")\n",
    "plot_correlation_heatmap(artists, \"Artists Dataset\")\n",
    "\n",
    "# Correlation Heatmap for MERGED dataset (df)\n",
    "print(\"=== Correlation Heatmap: MERGED dataset ===\")\n",
    "plot_correlation_heatmap(df, \"Merged Dataset (Tracks + Artists)\")"
   ],
   "id": "9cced56f269e1aab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outliers Detection"
   ],
   "id": "9b09f773659fb951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Numerical Feature Definition\n",
    "\n",
    "# List of key numerical columns to analyze\n",
    "skewed_features = [\n",
    "    'tokens_per_sent', 'avg_token_per_clause', 'duration_ms', 'stats_pageviews', 'swear_EN', 'char_per_tok', 'swear_IT', 'bpm', 'n_sentences', 'n_tokens', 'rolloff', 'zcr', 'lexical_density', 'flatness'\n",
    "]\n",
    "\n",
    "simetric_features =[\n",
    "    'pitch', 'centroid', 'spectral_complexity', 'loudness', 'flux', 'rms'\n",
    "]\n"
   ],
   "id": "f528f89c7cc901c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I removed the following variables from the list, the statistical analysis of outliers (IQR/Z-Score) is semantically wrong for them: disc_number, track_number, Month, Day"
   ],
   "id": "4bb653b2fe2793b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_cols = 3\n",
    "n_rows = -(-len(simetric_features) // n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(simetric_features):\n",
    "    sns.boxplot(x=df[col], ax=axes[i], orient='h', color=\"skyblue\")\n",
    "    axes[i].set_title(col, fontsize=14)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Analisi Box Plot per Rilevazione Outlier\", fontsize=20, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"simetric_box_plots_analysis.png\")\n",
    "plt.show()"
   ],
   "id": "4585f83e4d000f51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All six plots show distributions that appear relatively symmetric. The median line within each box is positioned near the center of the box (the Interquartile Range, or IQR), indicating that the 50th percentile is roughly equidistant from the 25th (Q1) and 75th (Q3) percentiles.\n",
    "\n",
    "Every feature have the presence of candidate outliers.\n",
    "\n",
    "Outlier Distribution:\n",
    "\n",
    "pitch, centroid, flux, and rms all show outliers on both the lower (left) and upper (right) ends of their distributions.\n",
    "\n",
    "spectral_complexity and loudness appear to have outliers almost exclusively on the high end (right side).\n",
    "\n",
    "This visual inspection suggests that while the central tendency of these features is symmetrically distributed, a small number of records possess extremely high and/or low values that fall outside the main data cluster."
   ],
   "id": "8a7d73f1be426cf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_cols = 3\n",
    "n_rows = -(-len(skewed_features) // n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(skewed_features):\n",
    "    sns.boxplot(x=df[col], ax=axes[i], orient='h', color=\"skyblue\")\n",
    "    axes[i].set_title(col, fontsize=14)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Analisi Box Plot per Rilevazione Outlier\", fontsize=20, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"skewed_box_plots_analysis.png\")\n",
    "plt.show()"
   ],
   "id": "e207ed3cb62439d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* **Extreme Skewness:** For many features, particularly `stats_pageviews`, `swear_EN`, `swear_IT`, `tokens_per_sent`, and `avg_token_per_clause`, the main \"box\" is extremely compressed and pushed to the far left. This visually confirms a strong positive skew (right-tailed distribution).\n",
    "\n",
    "* **Vast Number of Outliers:** The most prominent characteristic is the dense cloud of outliers extending far to the right for these positively skewed features. This indicates that a large number of records have values significantly higher than the main cluster of data.\n",
    "\n",
    "* **Specific Cases:**\n",
    "    * **Positively Skewed:** `stats_pageviews` and `swear_EN` are the most extreme examples, where the box is barely visible, and the plot is dominated by a long stream of high-end outliers.\n",
    "    * **Negatively Skewed:** `flatness` is the clear exception, showing the opposite pattern. Its box is compressed to the far right, with a long tail of outliers on the low end (left side), confirming its negative skew.\n",
    "    * **Mixed Outliers:** Features like `duration_ms`, `n_sentences`, and `lexical_density` show outliers on *both* sides, though the high-end outliers are generally more numerous or extreme.\n",
    "\n",
    "The visual analysis shows that these features are heavily skewed and contain a large quantity of extreme values. Simply removing all these outliers would lead to massive data loss."
   ],
   "id": "e940f96a92518bba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analisi Statistica: Metodo IQR (per variabili asimmetriche)\n",
    "outlier_data = []\n",
    "\n",
    "for col in skewed_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    limite_inferiore = Q1 - 1.5 * IQR\n",
    "    limite_superiore = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identifica outlier\n",
    "    outliers_bassi = df[df[col] < limite_inferiore]\n",
    "    outliers_alti = df[df[col] > limite_superiore]\n",
    "\n",
    "    outlier_data.append({\n",
    "        'variable': col,\n",
    "        'Outlier Alti (Sopra)': len(outliers_alti),\n",
    "        'Outlier Bassi (Sotto)': len(outliers_bassi)\n",
    "    })\n",
    "\n",
    "    print(f\"Variabile '{col}':\")\n",
    "    print(f\"  Limiti IQR: [{limite_inferiore:.2f}, {limite_superiore:.2f}]\")\n",
    "    print(f\"  Trovati {len(outliers_bassi)} outlier sotto il limite.\")\n",
    "    print(f\"  Trovati {len(outliers_alti)} outlier sopra il limite.\")"
   ],
   "id": "a9ca9c8e7f1eef44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if outlier_data:\n",
    "    df_outliers = pd.DataFrame(outlier_data)\n",
    "\n",
    "    # Calcola il totale per l'ordinamento\n",
    "    df_outliers['total_outliers'] = df_outliers['Outlier Alti (Sopra)'] + df_outliers['Outlier Bassi (Sotto)']\n",
    "\n",
    "    # Ordina il DataFrame\n",
    "    df_outliers = df_outliers.sort_values(by='total_outliers', ascending=False)\n",
    "\n",
    "    df_melted = df_outliers.melt(\n",
    "        id_vars=['variable', 'total_outliers'],\n",
    "        value_vars=['Outlier Alti (Sopra)', 'Outlier Bassi (Sotto)'],\n",
    "        var_name='Tipo di Outlier',\n",
    "        value_name='Numero di Outlier'\n",
    "    )\n",
    "\n",
    "    # Crea il Grafico a Barre\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    sns.barplot(\n",
    "        data=df_melted,\n",
    "        x='variable',\n",
    "        y='Numero di Outlier',\n",
    "        hue='Tipo di Outlier',\n",
    "        order=df_outliers['variable'],\n",
    "        palette={\"Outlier Alti (Sopra)\": \"#d36ba8\", \"Outlier Bassi (Sotto)\": \"#8cbcd9\"}\n",
    "    )\n",
    "\n",
    "    plt.title('Conteggio Outlier (Metodo IQR) per Variabili Asimmetriche', fontsize=20)\n",
    "    plt.xlabel('Variabile', fontsize=14)\n",
    "    plt.ylabel('Numero di Outlier Rilevati', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title='Tipo di Outlier', fontsize=12, title_fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"outlier_counts_asymmetric_barchart.png\")\n",
    "    print(\"Grafico del conteggio degli outlier generato e salvato come 'outlier_counts_asymmetric_barchart.png'.\")\n",
    "    plt.show()"
   ],
   "id": "f15cc45422e9f088"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analisi Statistica: Metodo Z-Score (per variabili simmetriche)\n",
    "print(\"\\n Metodo Z-Score (per variabili simmetriche)\")\n",
    "outlier_data_z = []\n",
    "soglia_z = 3\n",
    "\n",
    "for col in simetric_features:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "\n",
    "    limite_inferiore_z = mean - (soglia_z * std)\n",
    "    limite_superiore_z = mean + (soglia_z * std)\n",
    "\n",
    "    # Identifica outlier\n",
    "    outliers_bassi_z = df[df[col] < limite_inferiore_z]\n",
    "    outliers_alti_z = df[df[col] > limite_superiore_z]\n",
    "\n",
    "    outlier_data_z.append({\n",
    "        'variable': col,\n",
    "        'Outlier Alti (Sopra)': len(outliers_alti_z),\n",
    "        'Outlier Bassi (Sotto)': len(outliers_bassi_z)\n",
    "    })\n",
    "\n",
    "    print(f\"Variabile '{col}':\")\n",
    "    print(f\"  Limiti Z-Score (soglia={soglia_z}): [{limite_inferiore_z:.2f}, {limite_superiore_z:.2f}]\")\n",
    "    print(f\"  Trovati {len(outliers_bassi_z)} outlier sotto il limite.\")\n",
    "    print(f\"  Trovati {len(outliers_alti_z)} outlier sopra il limite.\")"
   ],
   "id": "7c9c8eb68d2943fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The IQR statistical analysis confirms that **all asymmetric features contain a significant number of outliers**, as clearly visualized in the summary bar chart.\n",
    "\n",
    "* **Massive Outlier Counts:** The most striking case is `swear_EN`, with 2,740 high-end outliers. This is a statistical artifact: its IQR is `[0.00, 0.00]`, meaning any track with even one English swear word is flagged. `swear_IT` (746), `avg_token_per_clause` (605), and `flatness` (507) also show a very high volume of outliers, making simple removal impossible.\n",
    "\n",
    "* **One-Sided Distributions:**\n",
    "    * `stats_pageviews`, `swear_EN`, and `swear_IT` only have **high-end outliers**. This perfectly matches their positive skew (right-tail) and identifies \"hit songs\" or lyrically extreme tracks.\n",
    "    * `flatness` is the only feature with exclusively **low-end outliers**, confirming its negative skew (left-tail).\n",
    "\n",
    "* **Two-Sided Distributions:** Most features, including `duration_ms`, `n_sentences`, `lexical_density`, and `char_per_tok`, show a significant number of outliers on **both sides**. This implies the presence of errors or extreme values at both ends (e.g., for `duration_ms`, this likely includes both \"skit\" tracks and very long songs).\n",
    "\n",
    "* **Isolated Outliers:** `bpm` is a special case, showing **only one high-end outlier**. This is almost certainly an data-entry error (e.g., `bpm = 900`) and will be simple to inspect and correct."
   ],
   "id": "1053da67b5eda494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Converti la lista in un DataFrame\n",
    "if outlier_data_z:\n",
    "    df_outliers_z = pd.DataFrame(outlier_data_z)\n",
    "\n",
    "    # Calcola il totale per l'ordinamento\n",
    "    df_outliers_z['total_outliers'] = df_outliers_z['Outlier Alti (Sopra)'] + df_outliers_z['Outlier Bassi (Sotto)']\n",
    "\n",
    "    # Ordina il DataFrame\n",
    "    df_outliers_z = df_outliers_z.sort_values(by='total_outliers', ascending=False)\n",
    "\n",
    "    df_melted_z = df_outliers_z.melt(\n",
    "        id_vars=['variable', 'total_outliers'],\n",
    "        value_vars=['Outlier Alti (Sopra)', 'Outlier Bassi (Sotto)'],\n",
    "        var_name='Tipo di Outlier',\n",
    "        value_name='Numero di Outlier'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.barplot(\n",
    "        data=df_melted_z,\n",
    "        x='variable',\n",
    "        y='Numero di Outlier',\n",
    "        hue='Tipo di Outlier',\n",
    "        order=df_outliers_z['variable'],\n",
    "        palette={\"Outlier Alti (Sopra)\": \"#d36ba8\", \"Outlier Bassi (Sotto)\": \"#8cbcd9\"}\n",
    "    )\n",
    "\n",
    "    plt.title('Conteggio Outlier (Metodo Z-Score) per Variabili Simmetriche', fontsize=18)\n",
    "    plt.xlabel('Variabile', fontsize=12)\n",
    "    plt.ylabel('Numero di Outlier Rilevati', fontsize=12)\n",
    "    plt.xticks(rotation=0, ha='center', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend(title='Tipo di Outlier', fontsize=11, title_fontsize=13)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"outlier_counts_symmetric_barchart.png\")\n",
    "    print(\"\\nGrafico del conteggio degli outlier generato e salvato come 'outlier_counts_symmetric_barchart.png'.\")\n",
    "    plt.show()"
   ],
   "id": "4e087c0c8036fd18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* **Quantification of Outliers:** The analysis provides precise counts for these extreme values. `pitch` emerges as the feature with the most outliers (68 total), while `spectral_complexity` has the fewest (22). `centroid` (57) and `flux` (49) also show a notable number of outliers.\n",
    "\n",
    "* **Distribution of Outliers:**\n",
    "    * For most features (`pitch`, `centroid`, `spectral_complexity`, `flux`, and `rms`), the outliers are **distributed on both the high and low ends**. This indicates that there are tracks with values that are exceptionally high *and* exceptionally low for these audio characteristics.\n",
    "    * The most significant exception is `loudness`, which has 27 identified outliers, all of which are **exclusively on the high side** (above the 50.17 limit). This statistical result perfectly matches its box plot, which only showed an upper tail.\n",
    "\n",
    "This analysis confirms that while these features are symmetrically distributed, a small number of extreme values are present. These outliers are likely real but rare data points (e.g., unusually loud or high-pitched songs) that must be handled before clustering to prevent them from skewing the model."
   ],
   "id": "47099f76cc7d1503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multivariate analisys"
   ],
   "id": "e7a8962d66e7faaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DBSCAN to identify multidimensional outliers\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Select ALL the numerical features you want to analyze\n",
    "numerical_features = skewed_features + simetric_features\n",
    "\n",
    "# Prepare the data: distance-based algorithms REQUIRE standardization\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numerical_features].dropna())\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=3.0, min_samples=10)\n",
    "clusters = dbscan.fit_predict(df_scaled)\n",
    "\n",
    "# Extract the outliers\n",
    "# Points labeled as -1 are the outliers (noise)\n",
    "outlier_indices = np.where(clusters == -1)[0]\n",
    "n_outliers = len(outlier_indices)\n",
    "\n",
    "print(f\"\\n--- Algorithmic Analysis (DBSCAN) ---\")\n",
    "print(f\"Features analyzed: {len(numerical_features)}\")\n",
    "print(f\"Records analyzed: {len(df_scaled)}\")\n",
    "print(f\"Parameters: eps=3.0, min_samples=10\")\n",
    "print(f\"Found {n_outliers} multidimensional outliers (label -1).\")\n",
    "\n",
    "# 5. (Optional) Print some of the found outliers\n",
    "print(\"\\nFirst 10 multidimensional outliers found:\")\n",
    "display(df.iloc[outlier_indices].head(10))"
   ],
   "id": "9e3939ec54d04909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure there are no NaNs (should be clean already)\n",
    "df_analysis = df[numerical_features].dropna()\n",
    "print(f\"Data ready for analysis: {df_analysis.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_analysis)\n",
    "\n",
    "# Applying Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "\n",
    "# We train and get the predictions\n",
    "# The algorithm assigns:\n",
    "#  1 for Inliers (normal points)\n",
    "# -1 for Outliers (anomalous points)\n",
    "predictions = iso_forest.fit_predict(data_scaled)\n",
    "\n",
    "df_analysis['is_outlier_multi'] = predictions\n",
    "\n",
    "df['is_outlier_multi'] = df_analysis['is_outlier_multi'].reindex(df.index)\n",
    "\n",
    "outliers_multi = df[df['is_outlier_multi'] == -1]\n",
    "print(f\"\\nAnalysis completed.\")\n",
    "print(f\"Number of multivariate outliers identified: {len(outliers_multi)}\")\n",
    "\n",
    "# Show some of the records identified as anomalous\n",
    "print(\"\\nExamples of Multivariate Outliers:\")\n",
    "\n",
    "# Show the original columns and our clean columns\n",
    "display(outliers_multi[['full_title', 'primary_artist'] + numerical_features].head())"
   ],
   "id": "209c7ead21b0c44a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It identified the 93 (2%) songs that are the most stylistically unique when combining all 21 features.\n",
    "The examples show two clear patterns:\n",
    "\n",
    "Lyrical Anomalies (Rosa Chemical): This artist is flagged repeatedly. The data shows his songs have a very rare combination: they are lyrically complex (avg_token_per_clause_log, lexical_density) AND have high profanity in both Italian and English (swear_IT_log, swear_EN_log). This makes them stand out from all other artists.\n",
    "\n",
    "Audio Anomalies (thasup): The \"thasup\" track is a perfect example of an audio outlier. It has a very slow bpm (82) but is at the maximum loudness (45) and maximum pitch (3191). This combination of \"slow, loud, and high-pitched\" is a very unusual audio profile.\n",
    "\n",
    "Conclusion: These 93 songs are \"stylistic outliers\" (like experimental tracks). They are not errors, but you should remove them before clustering to get clearer, more representative clusters of the main \"rap schools\"."
   ],
   "id": "309e25b50eb1940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We use PCA to reduce the dimension and be able to plot the outliers\n",
    "\n",
    "# Reduce the scaled data to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "data_scaled_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame(data_scaled_2d, columns=['PC1', 'PC2'])\n",
    "plot_df['is_outlier'] = predictions\n",
    "\n",
    "# Create the Scatter Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='is_outlier',\n",
    "    palette={1: 'blue', -1: 'red'}, # Outliers in red\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('Isolation Forest Results (visualized with PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Type', labels=['Outlier (-1)', 'Inlier (1)'])\n",
    "plt.show()"
   ],
   "id": "dfb2028cfa7750f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Correlation Analysis of Outliers"
   ],
   "id": "87a68efd73b0c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select the clean data (only the inliers)\n",
    "if 'df_clustering' not in locals():\n",
    "    df_clustering = df[df['is_outlier_multi'] != -1]\n",
    "\n",
    "df_corr = df_clustering[numerical_features].copy()\n",
    "print(f\"Data ready for correlation analysis: {df_corr.shape}\")\n",
    "\n",
    "# Calculate the correlation matrix (Pearson Method)\n",
    "corr_matrix = df_corr.corr(method='pearson')\n",
    "\n",
    "# Visualize the Heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Create a \"mask\" to hide the upper part\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='coolwarm',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9b6274d51eaa5db2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It clearly shows strong multicollinearity (redundancy) among your features. The data isn't noisy; it just confirms that some features measure the same underlying concept.\n",
    "- Song Length: duration_ms_log, n_tokens_log, and n_sentences_log are highly correlated (0.8 to 0.9), as they all measure \"song length\".\n",
    "\n",
    "- Loudness: rms_log and loudness are almost identical (0.9), measuring \"volume\".\n",
    "\n",
    "- Spectral Brightness: centroid, rolloff_log, and flatness are very strongly correlated (0.9 and -0.8), measuring the \"brightness\" or \"timbre\" of the sound."
   ],
   "id": "ff44497a082696d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Select the clean data (BUT INCLUDING the multivariate outliers)\n",
    "df_corr_completo = df[numerical_features].copy()\n",
    "\n",
    "df_corr_completo = df_corr_completo.dropna()\n",
    "print(f\"Data ready for correlation analysis: {df_corr_completo.shape}\")\n",
    "\n",
    "# 3. Calculate the correlation matrix (Pearson Method)\n",
    "corr_matrix_completo = df_corr_completo.corr(method='pearson')\n",
    "\n",
    "# 4. Visualize the Heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix_completo, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix_completo,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='coolwarm',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Correlation Matrix (Including the 93 outliers from Isolation Forest)', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "eedf8053a8ac1f25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Missing Values\n",
    "This code analyzes missing values in the DataFrame by counting how many entries are NaN for each column and calculating the corresponding percentage. It creates a summary table showing only columns with missing data, sorted by the highest percentage."
   ],
   "id": "4f4ad899f4ecba39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calcolo missing values e percentuali\n",
    "missing_count = df.isna().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "missing_df = (\n",
    "    pd.DataFrame({'missing_count': missing_count, 'missing_percent': missing_percent})\n",
    "    .sort_values('missing_percent', ascending=False)\n",
    "    .query('missing_percent > 0')\n",
    ")\n",
    "\n",
    "# Mostra tabella riepilogativa (gradiente rosso-magenta)\n",
    "display(\n",
    "    missing_df\n",
    "    .style.background_gradient(subset=['missing_percent'], cmap='RdPu')  \n",
    "    .format({'missing_percent': '{:.2f}%'})\n",
    ")\n"
   ],
   "id": "9a788bb2f828ebaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following heatmap visualizes missing values in the dataset, with each row representing a record and each column a feature. Colored cells indicate missing entries, providing a clear overview of where data is incomplete."
   ],
   "id": "89bdef3c33728490"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
    "plt.title(\"Missing Values Matrix (Overview)\", fontsize=20, pad=12, color=\"#000000\")\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "6ef81ecb9a5031f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following bar plot shows the percentage of missing values per feature, with the top 20 features that have the most missing data"
   ],
   "id": "539dc9289d8fb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=missing_df.head(20),\n",
    "    x='missing_percent',\n",
    "    y=missing_df.head(20).index,\n",
    "    hue=missing_df.head(20).index,  \n",
    "    palette='RdPu_r'  \n",
    ")\n",
    "plt.title(\"Percentage of Missing Values by Feature\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Missing values (%)\", fontsize=12)\n",
    "plt.ylabel(\"Feature name\", fontsize=12)\n",
    "\n",
    "# Etichette percentuali\n",
    "for index, value in enumerate(missing_df.head(20)['missing_percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#b30059')\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9e6cacc56b4b911f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Missing Values Propagation After Merge"
   ],
   "id": "27315cf7f85f5eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "artists_missing = artists.isna().mean().sort_values(ascending=False) * 100\n",
    "print(artists_missing)"
   ],
   "id": "5cea40bb41a53b96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The visualization highlights that missing values in attributes such as active_start, region, and birth_place have increased after merging due to the replication of incomplete artist metadata across multiple tracks.\n",
    "This confirms that the merge process did not introduce new nulls, but propagated pre-existing ones."
   ],
   "id": "906d7e09a4989a88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Colonne provenienti dal dataset artists \n",
    "artist_cols =list(artists.columns)\n",
    "\n",
    "# Conta i NaN prima e dopo il merge\n",
    "missing_before = artists[artist_cols].isna().sum()\n",
    "missing_after = df[artist_cols].isna().sum()\n",
    "\n",
    "# Differenza assoluta e percentuale\n",
    "missing_diff = missing_after - missing_before\n",
    "increase_percent = (missing_diff / missing_before.replace(0, pd.NA)) * 100\n",
    "\n",
    "# Tabella riepilogativa\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"missing_before\": missing_before,\n",
    "        \"missing_after\": missing_after,\n",
    "        \"difference\": missing_diff,\n",
    "        \"increase_%\": increase_percent\n",
    "    })\n",
    "    .sort_values(\"difference\", ascending=False)\n",
    ")\n",
    "\n",
    "plot_df = missing_summary[missing_summary['difference'] > 0].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x='difference',\n",
    "    y=plot_df.index,\n",
    "    hue=plot_df.index,\n",
    "    palette='RdPu_r'\n",
    ")\n",
    "plt.title(\"Increase in Missing Values After Merge\", fontsize=15, pad=12, color=\"#000000\")\n",
    "plt.xlabel(\"Increase in number of missing values\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "\n",
    "# Etichette numeriche a fianco delle barre\n",
    "for index, value in enumerate(plot_df['difference']):\n",
    "    plt.text(value + 50, index, f\"{int(value):,}\", va='center', fontsize=9, color=\"#000000\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d8be0701ee874846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After analyzing the percentage of missing values in each column, We need to better understand the overall data quality before applying any filling strategies. Cleaning and validating the data first ensures that missing values are handled correctly and that no incorrect or misleading information is introduced during imputation."
   ],
   "id": "fc603b5cb075cba9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Quality"
   ],
   "id": "e9294295ba1123e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "id": "15f61d42153bf391"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initial Data Cleaning\n",
    "\n",
    "Based on the initial exploration of the dataset, we:\n",
    "\n",
    "- **Removed empty column (`active_end`)** since it contained no useful information.  \n",
    "- **Converted `popularity` and `year`** to numeric types to ensure consistency and enable statistical analysis.  \n",
    "- **Transformed date-related columns** (`album_release_date`, `birth_date`, `active_start`, ) into proper datetime format for easier time-based operations.\n",
    "\n",
    "Before directly converting year and popularity from objects to numeric and album_release_date, birth_date, and active_start from objects to datetime, we need to inspect the data to check if all values can be converted correctly and handle those that cannot be converted\n"
   ],
   "id": "5e227243603308f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Remove empty column\n",
    "df.drop(columns=['active_end'], inplace=True)  # drop the 'active_end' column because it's empty\n",
    "df.info()"
   ],
   "id": "715dafc68604a0e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Objects to Numeric\n",
    "Inspecting the values in popularity and year columns to see the values that cannot be converted to numbers directly"
   ],
   "id": "a396ad47ce37dfe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_cols = ['popularity','year']\n",
    "\n",
    "\n",
    "# --- Check numeric columns ---\n",
    "for col in numeric_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_numeric(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to numeric:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to numeric.\")\n",
    "    print('----------------------------------------------------------------')"
   ],
   "id": "a46849dfabbc723a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Looking at the values of the  `popularity` column, we noticed that some entries contained **non-numeric characters**, percent signs (`%`), abbreviations like `K` (thousands) or `M` (millions), and words such as `\"views\"` appended to the numbers.  \n",
    "\n",
    "Instead of converting the column directly to numeric using pd.to_numeric(errors='coerce'), which would have turned all invalid entries into NaN, we applied a cleaning function to preserve and correctly interpret useful numeric information before conversion. The function:\n",
    "\n",
    "- Removed non-numeric characters and words like `\"views\"` and `%`.\n",
    "- Converted abbreviations (`K → 1,000`, `M → 1,000,000`) to numeric values.\n",
    "- Extracted the first numeric part if extra text was present.\n",
    "- Converted the cleaned values to floats, marking any remaining invalid entries as `NaN`.\n"
   ],
   "id": "a158f9896043445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_popularity(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value_str = str(value).strip().lower()  # normalize\n",
    "    \n",
    "    # Remove common words like 'views'\n",
    "    value_str = value_str.replace('views','').replace('%','').strip()\n",
    "    value_str = value_str.lower()  \n",
    "    # Handle K and M\n",
    "    multiplier = 1\n",
    "    if value_str.endswith('k'):\n",
    "        multiplier = 1_000\n",
    "        value_str = value_str[:-1]\n",
    "    elif value_str.endswith('m'):\n",
    "        multiplier = 1_000_000\n",
    "        value_str = value_str[:-1]\n",
    "    \n",
    "    # Take only first token if words remain\n",
    "    value_str = value_str.split()[0]\n",
    "    \n",
    "    # Try converting to float\n",
    "    try:\n",
    "        return (float(value_str) * multiplier)\n",
    "    except:\n",
    "        return None  # invalid entries become None/NaN\n",
    "    \n",
    "df['popularity'].apply(clean_popularity)\n",
    "\n",
    "\n"
   ],
   "id": "21a42657abafec14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Inspecting the values in the year column, we observed that while most entries were numerical, some contained unexpected or non-numeric characters. To handle this, we converted the column directly to a numeric type using pd.to_numeric() with the errors='coerce' parameter, which automatically transforms any invalid or non-numeric values into NaN."
   ],
   "id": "8f5c10635a4c44e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce') \n",
    "df.info()"
   ],
   "id": "a7b886521816c083"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Objects to DateTime\n",
    "\n",
    "Inspecting the values in 'album_release_date', 'birth_date', 'active_start' columns to see the values that cannot be converted to DateTime directly"
   ],
   "id": "b2053ebf0eb2453d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "date_cols = ['album_release_date', 'birth_date', 'active_start']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')"
   ],
   "id": "4e8cc9c37ff4c656"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Looking at the values in the album_release_date column that could not be converted to datetime, we noticed that many of them were just years (e.g., \"2004\"). If we used pd.to_datetime(errors='coerce') directly, these entries would have been turned into NaT. However, we wanted to keep this information by assigning a default month and day — the first day of the year.\n",
    "\n",
    "- Instead of converting the column directly, we applied a cleaning function that:\n",
    "\n",
    "- Detected values that were only a year (e.g., \"2004\") and changed them to a full date (\"2004-01-01\").\n",
    "\n",
    "- Kept valid full dates (e.g., \"2021-04-09\") unchanged.\n",
    "\n",
    "- Left missing values as they are.\n",
    "\n",
    "- Finally, converted everything into proper datetime format for consistency."
   ],
   "id": "f6ee762e97b372bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fix_year_only_dates(val):\n",
    "    \"\"\"\n",
    "    If the value looks like a 4-digit year, convert it to 'YYYY-01-01'.\n",
    "    Otherwise, return the original value.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val_str = str(val).strip()\n",
    "    if re.fullmatch(r'\\d{4}', val_str):\n",
    "        return f\"{val_str}-01-01\"\n",
    "    return val_str\n",
    "\n",
    "# Apply to album_release_date\n",
    "df['album_release_date'] = df['album_release_date'].apply(fix_year_only_dates)\n",
    "\n",
    "# Convert to datetime\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    "df.info()"
   ],
   "id": "b5ba3f74c908e0a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on the values that could not be converted to datetime, we found that the birth_date column contained several invalid entries, such as URLs (e.g., \"http://www.wikidata.org/.well-known/genid/...\") instead of actual dates. Since these values do not represent meaningful or recoverable information, there is nothing worth preserving. Therefore, we are going to apply the pd.to_datetime(errors='coerce') function directly, allowing all invalid entries to be converted to NaT.\n",
    "\n",
    "For the active_start column, all non-missing entries are  already in a valid date format, so they are going to be  successfully converted to datetime without any issues."
   ],
   "id": "909b3b3b0dda3e03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "date_cols = ['birth_date', 'active_start', ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "df.info()\n"
   ],
   "id": "96f29a9e1af3d93f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Distribution\n",
    "The following table and histogram show the numerical data distribution in the dataset:\n",
    "\n",
    "- **Most features** (`n_sentences`, `n_tokens`, `tokens_per_sent`, `char_per_tok`, `lexical_density`, `avg_token_per_clause`, `centroid`, `rolloff`, `rms`, `zcr`, `flatness`, `flux`, `spectral_complexity`, `pitch`, `loudness`) show **bell-shaped or near-normal distributions**.\n",
    "\n",
    "- **Highly skewed features** (`stats_pageviews`, `bpm`, `tokens_per_sent`, `duration_ms`, `popularity`) have a **long right tail**, indicating a few extreme values or outliers (common in popularity or count-based features).\n",
    "\n",
    "- **Temporal features** (`year`, `month`, `day`) display **non-uniform distributions**; e.g., `year` is concentrated around recent decades, showing most songs are modern.\n",
    "\n",
    "- **Geographical features** (`latitude`, `longitude`) have **peaks corresponding to specific locations**, likely representing where artists or tracks are clustered.\n"
   ],
   "id": "a3fca69730e7a97d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Select numeric columns\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# --- Summary statistics table ---\n",
    "display(df[num_cols].describe().T.style.background_gradient(cmap='RdPu'))\n",
    "\n",
    "# --- Histograms for each numeric column ---\n",
    "n_cols = 4\n",
    "n_rows = -(-len(num_cols) // n_cols)  # ceil division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col].dropna(), bins=30, kde=True, color=\"#d36ba8\", ax=axes[i])\n",
    "    axes[i].set_title(col, fontsize=18, color=\"#b30059\")   # larger title font\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "    axes[i].tick_params(axis='both', labelsize=12)          # larger tick labels\n",
    "\n",
    "# Remove unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Distribution of Numerical Features\", fontsize=24, color=\"#000000\", y=1.02)  # larger main title\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "fbc1d8bf37c12d8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The data distribution and the statistics presented above reveal some anomalies and irregularities in the dataset. These issues will be examined and addressed in the following section.\n"
   ],
   "id": "d846e613ba359b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate basic statistics (mean and median)\n",
    "statistics = df[num_cols].describe().T\n",
    "\n",
    "# Calculate skewness\n",
    "skews = df[num_cols].skew()\n",
    "\n",
    "skew_analysis = pd.DataFrame({\n",
    "    'mean': statistics['mean'],\n",
    "    'median': statistics['50%'],\n",
    "    'skewness_value': skews\n",
    "})\n",
    "\n",
    "# Define a function to classify skewness\n",
    "# These are standard thresholds used in statistics:\n",
    "# > +0.5 = Positive Skew (Right-tailed)\n",
    "# < -0.5 = Negative Skew (Left-tailed)\n",
    "# Between -0.5 and +0.5 = Substantially Symmetric\n",
    "def classify_skew(skew_value):\n",
    "    if skew_value > 0.5:\n",
    "        return \"Positive (Right Skew)\"\n",
    "    elif skew_value < -0.5:\n",
    "        return \"Negative (Left Skew)\"\n",
    "    else:\n",
    "        return \"Symmetric\"\n",
    "\n",
    "skew_analysis['skew_type'] = skew_analysis['skewness_value'].apply(classify_skew)\n",
    "\n",
    "print(\"Skewness Analysis of Numerical Features\")\n",
    "\n",
    "display(skew_analysis.sort_values(by='skewness_value', ascending=False))"
   ],
   "id": "7a4a7e3c2741900f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Features Inspection Anomalies Detection"
   ],
   "id": "458b2897453d34d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Artists Names\n",
    "This code groups the dataset by artist name to count how many songs each artist has, then displays the total number of unique artists and sorts them by song count. The analysis shows that there are 104 unique artists in the dataset. Among them, Mondo Marcio, Guè Pequeno, and Gemitaiz are the most prolific, each with over 300 songs. Other highly represented artists include Bassi Maestro, Fabri Fibra, and Vacca, each contributing more than 250 songs. On the other hand, a few artists such as O Zulù, Joey Funboy, and Hindaco have only a handful of tracks. Overall, the distribution highlights a few artists dominating the dataset while many others have significantly fewer songs."
   ],
   "id": "e338d622ab9ceb1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(df.shape)\n",
    "# The result is a Pandas Series where the index is the artist name and the values are the counts.\n",
    "artist_song_counts = df.groupby('name').size().sort_values(ascending=True)\n",
    "\n",
    "# 2. Convert the Series to a DataFrame for cleaner display\n",
    "artist_counts_df = artist_song_counts.reset_index(name='song_count')\n",
    "# To see the total number of unique artists:\n",
    "print(f\"\\nTotal number of unique artists: {len(artist_counts_df)}\")\n",
    "\n",
    "# 3. Print the results\n",
    "print(\"Unique Artists and Their Song Count:\")\n",
    "print(artist_counts_df.sort_values(by='song_count',\n",
    "    ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "# Sort by song count (descending for better view)\n",
    "artist_counts_df = artist_counts_df.sort_values(by='song_count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "bars = plt.barh(artist_counts_df['name'], artist_counts_df['song_count'], color='skyblue')\n",
    "plt.xlabel('Number of Songs')\n",
    "plt.ylabel('Artist')\n",
    "plt.title('Number of Songs per Artist')\n",
    "\n",
    "# Add numbers on top of each bar\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.3,               # x position (a bit after the end of the bar)\n",
    "             bar.get_y() + bar.get_height()/2,  # y position (middle of bar)\n",
    "             str(int(width)),           # text (integer value)\n",
    "             va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "8ea701afedbfdbe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Artists Description\n",
    "The folowing code counts how many times each unique description appears in the dataset. This helps identify which artist descriptions are the most common or repeated, showing patterns such as groups of artists sharing the same description or potential duplicates.\n",
    "\n",
    "The results show how the most frequent artist descriptions in the dataset. Most entries describe Italian rappers, producers, or singer-songwriters, reflecting that the dataset mainly focuses on Italian music artists.\n",
    "\n",
    "For instance, “gruppo musicale italiano” (Italian music group) appears 620 times, making it the most common description.\n",
    "\n",
    "Interestingly, there are also some non-musical or unrelated entries, like “dio indiano della distruzione e della trasformazione” (Indian god of destruction and transformation) or “tipo di barca a vela usata nel XVIII e XIX secolo” (type of sailing ship used in the 18th–19th century). These seem to be data errors.\n",
    "\n",
    "We also noticed an entry labeled “gruppo musicale canadese” (Canadian music group). Upon checking, this description is incorrectly assigned to the Italian rapper Priestess. Further research revealed a mix-up with a Canadian band that shares the same name. This confusion becomes evident when comparing the active_start year in the dataset, which matches that of the Canadian group rather than the Italian artist."
   ],
   "id": "fe49662466aadae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['description'].value_counts()"
   ],
   "id": "4ba29b36a9d8cf66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Correcting \"Priestess\" Entry"
   ],
   "id": "ed5d55a9be7fd7d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('Before')\n",
    "display( df[df['description'].str.contains('gruppo musicale canadese', case=False, na=False)]\n",
    "    .drop_duplicates(subset=['name'])\n",
    "    .sort_values(by='name'))\n",
    "\n",
    "print('After')\n",
    "# Fix Priestess' incorrect description and active_start date\n",
    "df.loc[df['name'].str.lower() == 'priestess', ['description', 'active_start']] = [\n",
    "    'cantante e rapper italiana',\n",
    "    '2027-01-01'\n",
    "]\n",
    "\n",
    "# Verify the update\n",
    "print(df[df['name'].str.lower() == 'priestess'][['name', 'description', 'active_start']])\n"
   ],
   "id": "df8a3ee8ab1a882a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Identifying groups in the dataset\n",
    "\n",
    "This filter identifies all artists whose description includes the word \"gruppo\", which typically refers to musical groups or bands. The resulting list contains 7 well-known Italian music groups, such as 99 Posse, Articolo 31, Club Dogo, Colle Der Fomento, Cor Veleno, Dark Polo Gang, and Sottotono."
   ],
   "id": "e70b8934cf9566db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows where 'description' contains 'gruppo'\n",
    "artists_with_gruppo = (\n",
    "    df[df['description'].str.contains('gruppo', case=False, na=False)]\n",
    "[['name','description','birth_date','active_start']]\n",
    "    .drop_duplicates(subset=['name'])\n",
    "    .sort_values(by='name')\n",
    ")\n",
    "\n",
    "print(\"Artists with 'grupoo' in their description:\",artists_with_gruppo.shape )\n",
    "display(artists_with_gruppo)\n"
   ],
   "id": "4cbab2f40c65f873"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Artist's BirthDate\n"
   ],
   "id": "2fa0f1f278bb8965"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Distribution of artist birth_date\n",
    "\n",
    "This code extracts each artist’s birth year and groups them by decade (e.g., 1960s, 1970s, 1980s, etc.) to analyze how artists are distributed over time. It calculates the percentage of artists born in each decade and visualizes it with a bar chart. The results show that most artists were born between the 1980s and 1990s, indicating that the majority belong to the Millennial generation, while fewer artists were born in the 1960s or after 2000.\n",
    "\n",
    "It also shows the histogram of the birth year"
   ],
   "id": "40cc7fd2acf2ab1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count missing values in 'birthdate' column\n",
    "missing_birthdates = df['birth_date'].isna().sum()\n",
    "\n",
    "print(f\"Number of missing values in 'birthdate' column: {missing_birthdates}\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_birth_decades(df ,title1, title2):\n",
    "    \"\"\"\n",
    "    Plots:\n",
    "      1. Histogram of birth years\n",
    "      2. Percentage of unique artists by decade of birth\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least 'name' and 'birth_date' columns.\n",
    "    \"\"\"\n",
    "    # --- 1. Keep only unique artists ---\n",
    "    unique_artists = df.drop_duplicates(subset=['name'])\n",
    "\n",
    "    # --- 2. Extract birth year ---\n",
    "    birth_year = unique_artists['birth_date'].dt.year\n",
    "\n",
    "\n",
    "    # Drop missing values\n",
    "    birth_year = birth_year.dropna()\n",
    "\n",
    "    # --- 4. Create decade bins ---\n",
    "    start = (int(birth_year.min()) // 10) * 10\n",
    "    end = (int(birth_year.max()) // 10 + 1) * 10\n",
    "    bins = list(range(start, end + 1, 10))\n",
    "    labels = [f\"{b}s\" for b in bins[:-1]]\n",
    "\n",
    "    # --- 5. Assign decades ---\n",
    "    decades = pd.cut(birth_year, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # --- 6. Calculate percentage per decade ---\n",
    "    group_percent = decades.value_counts(normalize=True).sort_index() * 100\n",
    "    group_df = pd.DataFrame({'decade': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "    # --- 7. Plot both charts side by side ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Histogram of birth years\n",
    "    sns.histplot(birth_year, bins=20, kde=True, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title(title1, fontsize=16, pad=15)\n",
    "    axes[0].set_xlabel(\"Birth Year\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Count\", fontsize=12)\n",
    "\n",
    "    # Percentage per decade\n",
    "    sns.barplot(data=group_df, x='decade', y='percent', hue='decade', palette='coolwarm', legend=False, ax=axes[1])\n",
    "    for i, val in enumerate(group_df['percent']):\n",
    "        axes[1].text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "    axes[1].set_title(title2, fontsize=16, pad=15)\n",
    "    axes[1].set_xlabel(\"Decade\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_birth_decades(df, \"Distribution of Artists' Birth Years  before Cleaning\",'Percentage of Unique Artists by Decade of Birth before cleaning')\n",
    "\n",
    "# Confirm df is unchanged\n",
    "print(df.columns)\n"
   ],
   "id": "c01ac61a2d8d30d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Distribution of artist ages\n",
    "The results show the distribution of unique artists’ ages in the dataset, ranging from 22 to 58 years old. Most artists fall between their late 20s and mid-40s, with small peaks around ages 32, 36, and 46, each having between 6 and 7 artists. Younger artists under 25 and older ones above 50 are less represented. Overall, the majority of artists are in their thirties and early forties, reflecting the typical active and productive age range in the music industry."
   ],
   "id": "bc33a09702d823a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def plot_artist_ages(df,title):\n",
    "    \"\"\"\n",
    "    Calculates and plots the number of unique artists by age.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least 'name' and 'birth_date' columns.\n",
    "    \"\"\"\n",
    "    # --- 1. Keep only unique artists ---\n",
    "    unique_artists = df.drop_duplicates(subset=['name'])\n",
    "\n",
    "    # --- 2. Calculate current age ---\n",
    "    today = pd.Timestamp.today()\n",
    "    artist_age = today.year - unique_artists['birth_date'].dt.year\n",
    "\n",
    "    # --- 3. Drop missing or invalid ages ---\n",
    "    artist_age = artist_age.dropna().astype(int)\n",
    "\n",
    "    # --- 4. Count how many unique artists have each exact age ---\n",
    "    age_counts = artist_age.value_counts().sort_index()\n",
    "    print(\"Number of unique artists per age:\")\n",
    "    print(age_counts)\n",
    "\n",
    "    # --- 5. Plot the distribution ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=age_counts.index, y=age_counts.values, color=\"#6A5ACD\")\n",
    "\n",
    "    plt.title(title, fontsize=16, color=\"#333\")\n",
    "    plt.xlabel(\"Age (years)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Artists\", fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_artist_ages(df,\"Number of Unique Artists by Age (Before Cleaning)\")\n"
   ],
   "id": "9f4570876089ddc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Artists with no birthdate\n",
    "This code shows the number of the artist that doesn't have a birthdate. They are 32."
   ],
   "id": "1bfbb42528d3a831"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29a1d35a0749d9f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_birth_artists = df[df['birth_date'].isna()]['name'].drop_duplicates()\n",
    "print(f\"Number of artists with missing birth date: {missing_birth_artists.shape[0]}\")\n",
    "\n",
    "print(\"Artists with missing birth date:\")\n",
    "print(missing_birth_artists.tolist())"
   ],
   "id": "9d95fde9fbe9eb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Filling Birthdates\n",
    "\n",
    "This code manually fills missing birth dates for specific artists in the dataset. It first defines a dictionary mapping artist names to their known or estimated birth dates.\n",
    "\n",
    "9 entries couldn’t be filled because their birth dates are intentionally left blank in the dictionary. For Miss Keta, the birth date is unknown, so no accurate value can be provided. The others — Bushwaka, Sottotono, Dark Polo Gang, Cor Veleno, Colle Der Fomento, Club Dogo, Articolo 31, and 99 Posse — are all music groups or duos, not individual artists, meaning they don’t have a single birth date associated with them."
   ],
   "id": "da8bcd35083ed60a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- 1. Define the Missing Dates as a Dictionary ---\n",
    "# Source of truth for the manual fill\n",
    "birth_dates_to_fill = {\n",
    "    'alfa': '2000-08-22',\n",
    "    'anna pepe': '2003-08-15',\n",
    "    'beba': '1994-10-10',\n",
    "    'bigmama': '2000-03-10',\n",
    "    'brusco': '1974-01-04',\n",
    "    'caneda': '1976-09-30',\n",
    "    'dargen d_amico': '1980-11-29',\n",
    "    'guè pequeno': '1980-12-25',\n",
    "    'johnny marsiglia': '1986-08-05',\n",
    "    'nerone': '1991-05-23',\n",
    "    'priestess': '1996-08-20',\n",
    "    'samuel heron': '1991-01-01',\n",
    "    'shiva': '1999-08-27',\n",
    "    'yeиdry': '1993-07-27',\n",
    "    'o zulù': '1970-11-15',\n",
    "    'skioffi':'1992-06-05',\n",
    "    'eva rea':'1993-01-01',\n",
    "    'hindaco':'1996-01-01',\n",
    "    'joey funboy':'1995-01-01',\n",
    "    'mistico':'1982-01-01',\n",
    "    'mike24':'1985-08-02',\n",
    "    'doll kill':'1996-01-01',\n",
    "    'miss simpatia':'1986-03-23',\n",
    "    'miss keta':'',#unknown\n",
    "    'bushwaka':'',#duo\n",
    "    'sottotono':'',#group\n",
    "    'dark polo gang':'',#group\n",
    "    'cor veleno':'',#group\n",
    "    'colle der fomento':'',#group\n",
    "    'club dogo':'',#group\n",
    "    'articolo 31':'',#group\n",
    "    '99 posse':''#gruppo\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "# --- 2. Fill the Missing Data (Imputation) ---\n",
    "\n",
    "# Convert the dictionary to a Pandas Series for easy lookup and indexing\n",
    "birth_date_series = pd.Series(birth_dates_to_fill)\n",
    "\n",
    "# Iterate through the artists in your fill list and update the DataFrame\n",
    "for artist, bday in birth_date_series.items():\n",
    "    # Use .loc to find rows where 'artist_name' matches and update 'birth_date'\n",
    "    # The second part of the condition (artist_df['birth_date'].isna()) ensures\n",
    "    # we only overwrite if the date was previously missing (NaN).\n",
    "    df.loc[\n",
    "        (df['name'] == artist) & (df['birth_date'].isna()),\n",
    "        'birth_date'\n",
    "    ] = bday\n",
    "\n",
    "# --- 3. Final Conversion and Verification ---\n",
    "\n",
    "# Convert the 'birth_date' column to the proper datetime format again\n",
    "# (This is crucial for accurate age calculation)\n",
    "df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "\n",
    "# Optional: Print out the affected rows to verify the fix\n",
    "print(\"--- Verification of Filled Birth Dates  ---\")\n",
    "# Filter the DataFrame to show only the artists we just updated\n",
    "filled_artists = df[df['name'].isin(birth_dates_to_fill.keys())]\n",
    "\n",
    "# Show the unique artist names and their newly filled birth dates\n",
    "print(filled_artists[['name', 'birth_date']].drop_duplicates().to_string(index=False))"
   ],
   "id": "f03beeebd9b1fa7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Artists Names and their ages\n",
    "This code generates a table showing each unique artist and their corresponding age, calculated from their birth date. Artists are listed from oldest to youngest, highlighting ages from 22 to 58 in this dataset."
   ],
   "id": "61dd905ad0ff8d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Calculate age without adding column to df ---\n",
    "today = pd.Timestamp.today()\n",
    "artist_age = (today - df['birth_date']).dt.days // 365\n",
    "\n",
    "# --- Create a temporary DataFrame with unique artists and their age ---\n",
    "artist_age_df = pd.DataFrame({\n",
    "    'name': df['name'],\n",
    "    'age': artist_age\n",
    "}).drop_duplicates().sort_values(by='age', ascending=False)\n",
    "\n",
    "# --- Display the table ---\n",
    "print(\"Unique Artists and Their Age:\")\n",
    "display(artist_age_df.style.background_gradient(cmap='coolwarm'))\n",
    "\n",
    "# --- Total number of unique artists ---\n",
    "print(f\"\\nTotal number of unique artists: {artist_age_df['name'].nunique()}\")\n",
    "\n"
   ],
   "id": "7cf692f9051adc5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking distribution after filling Bithdate"
   ],
   "id": "50e77e678aceb491"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_birth_decades(df, \"Distribution of Artists' Birth Years  After Cleaning\",'Percentage of Unique Artists by Decade of Birth After cleaning')\n",
    "plot_artist_ages(df,'Number of Unique Artists by Age (After Filling NaN)')"
   ],
   "id": "49a96a786d0eeeaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Active start"
   ],
   "id": "96d17b0005a45dd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Percentage of Artists by Active Start Decade\n",
    "The dataset contains 4,601 missing values in the active_start column, meaning a significant number of artists have no recorded career start date. Considering only unique artists, the distribution across decades shows that the 1990s (32%), 2000s (30%), and 2010s (32%) were the most common periods for artists to begin their careers, indicating a fairly even spread among these decades. Earlier decades like the 1980s (4%) and the 2020s (2%) have much fewer entries, likely reflecting fewer documented artists or incomplete data for those periods."
   ],
   "id": "8411ce8febe0bdc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def plot_active_start_decades(df, title, active_col='active_start', name_col='name'):\n",
    "    \"\"\"\n",
    "    Plot percentage of unique artists by active start decade.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Keep only unique artists based on name ---\n",
    "    unique_df = df.drop_duplicates(subset=[name_col])\n",
    "\n",
    "    # --- Extract year values, drop missing ---\n",
    "    years = unique_df[active_col].dropna().dt.year\n",
    "\n",
    "    # --- Define decade bins (e.g., 1960, 1970, ..., 2020) ---\n",
    "    start = int(years.min() // 10 * 10)\n",
    "    end = int(years.max() // 10 * 10 + 10)\n",
    "    bins = list(range(start, end + 10, 10))\n",
    "    labels = [f\"{b}s\" for b in bins[:-1]]\n",
    "\n",
    "    # --- Bin into decades ---\n",
    "    decade_groups = pd.cut(years, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # --- Calculate percentages per decade ---\n",
    "    group_percent = decade_groups.value_counts(normalize=True).sort_index() * 100\n",
    "    group_df = pd.DataFrame({'decade': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "    # --- Print results ---\n",
    "    print(title)\n",
    "    print(group_df)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=group_df, x='decade', y='percent', hue='decade', palette='coolwarm', legend=False)\n",
    "\n",
    "    # --- Add percentage labels ---\n",
    "    for i, val in enumerate(group_df['percent']):\n",
    "        plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "    plt.title(title, fontsize=18, pad=15)\n",
    "    plt.xlabel(\"Decade\", fontsize=12)\n",
    "    plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of missing values in 'active_start': {df['active_start'].isna().sum()}\")\n",
    "plot_active_start_decades(df,'Percentage of Unique Artists by Active Start Decade before Cleaning')\n"
   ],
   "id": "79a2f8e9cd500c3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Ages of artists when they started their career\n",
    "The distribution of ages of unique artists when they started their careers shows that most began between 17 and 22 years old, which is reasonable. However, there are outliers, such as one artist listed as starting at age 1 and another at age 10, which clearly do not make sense. These anomalous values indicate potential data errors, and we need to investigate these specific cases to determine the best way to clean or correct the dataset."
   ],
   "id": "1150fa40afea5c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def plot_age_at_career_start(df, title, birth_col='birth_date', active_col='active_start', name_col='name'):\n",
    "    \"\"\"\n",
    "    Plot the distribution of unique artists' ages when they started their career.\n",
    "    Does not modify the dataset or filter any values.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Keep only unique artists based on name ---\n",
    "    unique_df = df.drop_duplicates(subset=[name_col])\n",
    "\n",
    "    # --- Compute age at career start ---\n",
    "    age_at_start = (unique_df[active_col].dt.year - unique_df[birth_col].dt.year).dropna().astype(int)\n",
    "\n",
    "    # --- Count occurrences ---\n",
    "    age_counts = age_at_start.value_counts().sort_index()\n",
    "    print(title)\n",
    "    print(age_counts)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(age_counts.index, age_counts.values, color=\"#6A5ACD\")\n",
    "\n",
    "    # --- Add counts on top of each bar ---\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.5, f'{int(height)}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.title(title, fontsize=16, color=\"#333\")\n",
    "    plt.xlabel(\"Age at Career Start (years)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Artists\", fontsize=12)\n",
    "    plt.grid(alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_age_at_career_start(df,'Age of unique Artists When They Started Their Career Before Cleaning')\n"
   ],
   "id": "4807f4de583ee66c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Ckecking artist whose age was 1, 10, 13, 27, or 31 when they started their career\n",
    "\n",
    "Among the unique artists, several had unusual ages at career start. Bigmama (age 1) and Nesli (age 10) had incorrect active_start dates, while Salmo (age 13) and Mudimbi (age 27) were correct. However, Priestess (age 31) had an obviously wrong start year. We will correct the errors by updating Bigmama’s start year to 2016, Nesli’s to 1999, and Priestess’ to a 2017, leaving Salmo and Mudimbi unchanged."
   ],
   "id": "6c8164f2cb5f81f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate age at career start without adding a column\n",
    "ages = df['active_start'].dt.year - df['birth_date'].dt.year\n",
    "\n",
    "# Filter rows where age is 1, 10, or 13\n",
    "outliers = df[ages.isin([1, 10, 13,27,31])].copy()\n",
    "\n",
    "# Keep only unique artists based on name\n",
    "unique_outliers = outliers.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Print relevant information including age\n",
    "print(\"Unique artists with age 1, 10, 13, 27, or 31 at career start:\")\n",
    "print(unique_outliers[['name', 'birth_date', 'active_start']].assign(age_at_start=ages))\n"
   ],
   "id": "3a97bd81ddbd9a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Correcting wrong active start"
   ],
   "id": "2c645a96892d489"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correcting wrong active_start dates\n",
    "df.loc[df['name'] == 'bigmama', 'active_start'] = pd.to_datetime('2016-01-01')\n",
    "df.loc[df['name'] == 'nesli', 'active_start'] = pd.to_datetime('1999-01-01')\n",
    "df.loc[df['name'] == 'priestess', 'active_start'] = pd.to_datetime('2017-01-01')\n",
    "\n",
    "# Verify the changes\n",
    "outliers_corrected = df[df['name'].isin(['bigmama', 'nesli', 'salmo','priestess'])]\n",
    "print(outliers_corrected[['name', 'birth_date', 'active_start']])\n"
   ],
   "id": "2845dcbcfe24b3ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Artists with no active start date\n",
    "This code shows the number of the artist that doesn't have a active start date. They are 54 out 104."
   ],
   "id": "cc4ed7caf0afa93b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Filter rows where 'active_start' is missing ---\n",
    "missing_active_start = df[df['active_start'].isna()]\n",
    "\n",
    "# --- Compute earliest full release date (from year, month, day) per artist ---\n",
    "release_dates = pd.to_datetime(df[['year', 'month', 'day']], errors='coerce')\n",
    "album_dates = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    "# Group by artist and get earliest song date and earliest album release date\n",
    "earliest_dates = (\n",
    "    df.assign(_release_date=release_dates, _album_date=album_dates)\n",
    "      .groupby('name', as_index=False)\n",
    "      .agg({'_release_date': 'min', '_album_date': 'min'})\n",
    "      .rename(columns={'_release_date': 'earliest_song_date', '_album_date': 'earliest_album_date'})\n",
    ")\n",
    "\n",
    "# --- Merge with artists missing 'active_start' ---\n",
    "artists_missing_active = (\n",
    "    missing_active_start[['name', 'active_start', 'birth_date']]\n",
    "    .drop_duplicates()\n",
    "    .merge(earliest_dates, on='name', how='left')\n",
    "    .sort_values(by='name')\n",
    ")\n",
    "\n",
    "# --- Print the result ---\n",
    "print(\"Artists without 'active_start' information (with birth dates, earliest song date, and earliest album release date):\")\n",
    "print(artists_missing_active.to_string(index=False))\n",
    "\n",
    "# --- Optional count ---\n",
    "print(f\"\\nTotal number of unique artists missing 'active_start': {artists_missing_active['name'].nunique()}\")\n"
   ],
   "id": "3fd442d87fdfedc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# --- 1. Define the Missing Dates to be Filled ---\n",
    "# --- Define the researched Active Start Dates ---\n",
    "# Use the earliest known year/month of official activity.\n",
    "\n",
    "\n",
    "# Total number of unique artists missing 'active_start': 54\n",
    "\n",
    "active_starts_consolidated = {\n",
    "    'alfa': '01-01-2017',\n",
    "    'anna pepe': '01-01-2018',\n",
    "    'babaman': '01-01-1996',\n",
    "    'beba': '11-01-2015',\n",
    "    'brusco': '01-01-1991',\n",
    "    'capo plaza': '01-01-2013',\n",
    "    'chadia rodriguez': '01-01-2017',\n",
    "    'clementino': '01-01-2004',\n",
    "    'dargen d_amico': '01-01-1999',\n",
    "    'don joe': '01-01-1999',\n",
    "    'fred de palma': '01-01-2008',\n",
    "    'geolier': '01-01-2018',\n",
    "    'guè pequeno': '01-01-1997',\n",
    "    'miss keta': '01-01-2013',\n",
    "    'shiva': '01-01-2014',\n",
    "    'tedua': '01-01-2013',\n",
    "    'tony effe': '01-01-2014',\n",
    "    'sottotono': '01-01-1994',\n",
    "    'bushwaka': '01-01-2007',\n",
    "    'mike24': '01-01-2013',\n",
    "    'mistico': '01-01-2008',\n",
    "    'skioffi': '01-01-2014',\n",
    "    \"caneda\": \"01-01-1993\",\n",
    "    \"club dogo\": \"01-01-2002\",\n",
    "    \"colle der fomento\": \"01-01-1994\",\n",
    "    \"dani faiv\": \"01-01-2014\",\n",
    "    \"doll kill\": \"01-01-2012\",\n",
    "    \"drefgold\": \"01-01-2012\",\n",
    "    \"entics\": \"01-01-2004\",\n",
    "    \"eva rea\": \"12-18-2014\",\n",
    "    \"hell raton\": \"01-01-2010\",\n",
    "    \"hindaco\": \"02-21-2020\",\n",
    "    \"jack the smoker\": \"01-01-2000\",\n",
    "    \"joey funboy\": \"01-01-2016\",\n",
    "    \"johnny marsiglia\": \"01-01-2013\",\n",
    "    \"la pina\": \"01-01-1994\",\n",
    "    \"luchè\": \"01-01-1997\",\n",
    "    \"mambolosco\": \"02-10-2017\",\n",
    "    \"massimo pericolo\": \"01-01-2016\",\n",
    "    \"miss simpatia\": \"03-31-2023\",\n",
    "    \"mistaman\": \"01-01-1994\",\n",
    "    \"mondo marcio\": \"01-01-2003\",\n",
    "    \"nerone\": \"01-01-2008\",\n",
    "    \"niky savage\": \"01-01-2021\",\n",
    "    \"o zulù\": \"01-01-1991\",\n",
    "    \"papa v\": \"01-01-2020\",\n",
    "    \"rondodasosa\": \"01-01-2020\",\n",
    "    \"samuel heron\": \"01-01-2012\",\n",
    "    \"shablo\": \"01-01-1999\",\n",
    "    \"slait\": \"01-01-2010\",\n",
    "    \"tony boy\": \"01-01-2018\",\n",
    "    \"tormento\": \"01-01-1991\",\n",
    "    \"yeиdry\": \"01-01-2012\",\n",
    "    \"yung snapp\": \"01-01-2012\",\n",
    "}\n",
    "print(active_starts_imputed.length())\n",
    "\n",
    "# Convert the dictionary to a Pandas Series for efficient filling\n",
    "start_date_series = pd.Series(active_starts_to_fill)\n",
    "\n",
    "# Iterate and fill the missing data in the 'active_start' column\n",
    "for artist, start_date in start_date_series.items():\n",
    "    # Use .loc to find rows where 'artist_name' matches and update 'active_start'\n",
    "    df.loc[\n",
    "        df['artist_name'] == artist,\n",
    "        'active_start'\n",
    "    ] = start_date\n",
    "\n",
    "# Ensure the 'active_start' column is a proper datetime object\n",
    "df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "print(\"Active start dates have been filled in the 'active_start' column.\")"
   ],
   "id": "552da0850ca833c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Track Year and Album release date\n",
    "Looking at the distribution of values in the track year in the previous section, we notice some entries before 1950 and after 2025, which don’t make much sense. Similarly, there are album release dates after 2025 that seem unrealistic. Therefore, we will investigate these cases further to understand the cause and decide how to correct them."
   ],
   "id": "ce090ed1966d4b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Looking at the distribution of values in the track year in the previous section, we notice some entries before 1950 and after 2025, which don’t make much sense. Similarly, there are album release dates after 2025 that seem unrealistic. Therefore, we will investigate these cases further to understand the cause and decide how to correct them."
   ],
   "id": "9aa2f53aa4c860ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Convert 'year' to numeric ---\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "\n",
    "# --- Drop missing years and make a copy ---\n",
    "df_years = df.dropna(subset=['year']).copy()\n",
    "\n",
    "# --- Create 20-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_years['year'].min())\n",
    "end = int(df_years['year'].max())\n",
    "bins = list(range(start, end, 20)) + [end]  # ensure last bin ends exactly at max\n",
    "labels = [f\"{b}-{min(b+19, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_years['year_group'] = pd.cut(df_years['year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_years['year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='year_group', y='percent', hue='year_group', palette='viridis', legend=False)\n",
    "\n",
    "# --- Add percentage labels ---\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Songs by 20-Year Intervals\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Convert 'album_release_date' to datetime and extract year ---\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "df['album_year'] = df['album_release_date'].dt.year\n",
    "\n",
    "# --- Drop missing album years and make a copy ---\n",
    "df_album_years = df.dropna(subset=['album_year']).copy()\n",
    "\n",
    "# --- Create 20-year bins ensuring last bin includes the max year ---\n",
    "start = int(df_album_years['album_year'].min())\n",
    "end = int(df_album_years['album_year'].max())\n",
    "bins = list(range(start, end, 20)) + [end]\n",
    "labels = [f\"{b}-{min(b+19, end)}\" for b in bins[:-1]]\n",
    "\n",
    "df_album_years['album_year_group'] = pd.cut(df_album_years['album_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Calculate percentage per group ---\n",
    "group_percent = df_album_years['album_year_group'].value_counts(normalize=True).sort_index() * 100\n",
    "group_df = pd.DataFrame({'album_year_group': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=group_df, x='album_year_group', y='percent', hue='album_year_group', palette='mako', legend=False)\n",
    "\n",
    "for i, val in enumerate(group_df['percent']):\n",
    "    plt.text(i, val + 0.5, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Percentage of Songs by Album Release Year (20-Year Intervals)\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Album Release Year Range\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.info()"
   ],
   "id": "14659bc6a459399f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Descriptive Statistics\n",
    "The summary statistics show that the song release years range from 1900 to 2100, with an average around 2013, indicating some unrealistic future values.\n",
    "For album release years, the range is 1962 to 2025, with an average around 2017, which is more reasonable and reflects that most albums were released in the last decade."
   ],
   "id": "1382945e05d5dc8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For the 'year' column\n",
    "print(df['year'].describe())  \n",
    "\n",
    "# For 'album_release_date' (datetime type)\n",
    "df['album_release_year'] = df['album_release_date'].dt.year\n",
    "print(df['album_release_year'].describe())"
   ],
   "id": "39ed2a3960ffe77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Number of Songs before 1950 and after 2025 "
   ],
   "id": "4c064f768930d1e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tracks['year'] = pd.to_numeric(tracks['year'], errors='coerce')\n",
    "\n",
    "# Count songs released before 1950 and after 2025\n",
    "songs_before_1950 = tracks[tracks['year'] < 1950].shape[0]\n",
    "songs_after_2025 = tracks[tracks['year'] > 2025].shape[0]\n",
    "\n",
    "print(f\"Number of songs before 1950: {songs_before_1950}\")\n",
    "# Filter songs with year > 2025\n",
    "future_songs = tracks[tracks['year'] <1950 ][['full_title', 'album_release_date', 'year']]\n",
    "# Display the results\n",
    "display(future_songs)\n",
    "\n",
    "print(f\"Number of songs after 2025: {songs_after_2025}\")\n",
    "# Filter songs with year > 2025\n",
    "future_songs = tracks[tracks['year'] > 2025][['full_title', 'album_release_date', 'year']]\n",
    "# Display the results\n",
    "display(future_songs)\n"
   ],
   "id": "8f656b2316151154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tracks['album_release_date'] = pd.to_datetime(tracks['album_release_date'], errors='coerce')\n",
    "\n",
    "cutoff_after = pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "album_release_date_after_2025 = tracks[tracks['album_release_date'] > cutoff_after].shape[0]\n",
    "\n",
    "print(f\"Number of album_release_date after 2025: {album_release_date_after_2025}\")\n"
   ],
   "id": "fc8ad50937d3c97e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Inconsistency with years"
   ],
   "id": "463e8d1a835a4f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking if the active_start date is earlier than the artist’s birth_date"
   ],
   "id": "a67ddd4c643d4989"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure both columns are datetime\n",
    "df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "# Find rows where the artist started before their birth date\n",
    "invalid_dates = df[df['active_start'] < df['birth_date']]\n",
    "\n",
    "print(f\"Found {len(invalid_dates)} artists with 'active_start' earlier than 'birth_date'.\")\n",
    "display(invalid_dates[['id_artist', 'name_artist', 'birth_date', 'active_start']])\n"
   ],
   "id": "2ef933fe061a2d83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Ckecking if the career started at an unrealistically young age"
   ],
   "id": "7e1c8a14ab67c5f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure the columns are datetime\n",
    "df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "# Calculate the age at career start\n",
    "df['career_start_age'] = df['active_start'].dt.year - df['birth_date'].dt.year\n",
    "\n",
    "# Find anomalies: artists who started younger than 12\n",
    "young_start_anomalies = df[df['career_start_age'] < 12]\n",
    "\n",
    "# Display them\n",
    "print(\"Artists with unrealistically young career start:\", young_start_anomalies.shape)\n",
    "print(young_start_anomalies[['birth_date', 'active_start', 'career_start_age','name_artist']])\n"
   ],
   "id": "53a2671a74fe7100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for tracks released (year) before the artist’s career started (active start)"
   ],
   "id": "a70e29536d285e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Find inconsistencies: track released before career start\n",
    "inconsistency_release_before_career = df[df['year'] < df['active_start'].dt.year]\n",
    "\n",
    "# Display the inconsistent rows\n",
    "print('Number of records where a song was released before the artist started',inconsistency_release_before_career.shape)\n",
    "display(inconsistency_release_before_career[['name_artist','full_title','year','active_start','album_release_year']])\n"
   ],
   "id": "335297e3effad4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for tracks released before the artist’s birth"
   ],
   "id": "45856c4a831d8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Find tracks released before artist's birth\n",
    "tracks_before_birth = df[df['year'] < df['birth_date'].dt.year]\n",
    "\n",
    "# Display the anomalies\n",
    "print(f\"Number of tracks released before artist's birth: {len(tracks_before_birth)}\")\n",
    "display(tracks_before_birth[['full_title', 'year', 'birth_date','album_release_date', 'name_artist']])\n"
   ],
   "id": "827a71241b02e020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for album released before career start"
   ],
   "id": "c612444df46a4812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "album_before_career = df[df['album_release_date'] < df['active_start']]\n",
    "print(f\"Albums released before artist's career start: {len(album_before_career)}\")\n",
    "display(album_before_career[['full_title', 'album_release_date', 'active_start', 'name_artist']])"
   ],
   "id": "43ae9d5dd248c3a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for Albums released before artist's birth"
   ],
   "id": "c0aea2f757b44a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "album_before_birth = df[df['album_release_date'] < df['birth_date']]\n",
    "print(f\"Albums released before artist's birth: {len(album_before_birth)}\")\n",
    "display(album_before_birth[['full_title', 'album_release_date', 'birth_date', 'name_artist']])"
   ],
   "id": "27d8e7154c8712b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for Tracks released before album release excluding singles"
   ],
   "id": "365067a3c98ca44a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Tracks released before album (excluding singles)\n",
    "tracks_before_album = df[\n",
    "    (df['year'] < df['album_release_date'].dt.year) &\n",
    "    (df['album_type'] != 'single')\n",
    "]\n",
    "\n",
    "print(f\"Tracks released before the album (excluding singles): {len(tracks_before_album)}\")\n",
    "display(tracks_before_album[['full_title', 'year', 'album_release_date', 'album_type', 'name_artist']])\n"
   ],
   "id": "138ed1e93d564730"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Popularity"
   ],
   "id": "c865f6aedf2cfe08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Count popularity values ---\n",
    "pop_counts = (df['popularity'].astype(str)).value_counts().sort_index()  # sort index for ascending y-axis\n",
    "\n",
    "# --- Horizontal bar plot ---\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=pop_counts.values, y=pop_counts.index,hue=pop_counts.index, palette='viridis')\n",
    "plt.xlabel(\"Number of Songs\", fontsize=12)\n",
    "plt.ylabel(\"Popularity\", fontsize=12)\n",
    "plt.title(\"Distribution of Song Popularity\", fontsize=16, pad=15)\n",
    "\n",
    "# --- Add count labels ---\n",
    "for i, val in enumerate(pop_counts.values):\n",
    "    plt.text(val + 0.5, i, f\"{val}\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "db7867e00c56e913"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####  Artists Location Statistics\n"
   ],
   "id": "43a58e6eb35b7e43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking if all the coordinates of the artists are inside italy's coordinates"
   ],
   "id": "a31a7623b4130270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "geo_outliers = df[(df['latitude'] < 35.5) | (df['latitude'] > 47.1) |\n",
    "                  (df['longitude'] < 6.6) | (df['longitude'] > 18.5)]\n",
    "print(f\"Number of Geographic coordinates outside Italy range: {len(geo_outliers)} records\")\n",
    "display(geo_outliers[['name_artist', 'latitude', 'longitude', 'birth_place']].head(10))"
   ],
   "id": "1f2c21c7761abdfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Artists' Country Values\n",
    "\n",
    "All the countries have the value of Italia"
   ],
   "id": "a45b040f2a9a1d26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count the occurrences of each country\n",
    "country_counts = df['country'].value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "country_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Distribution of Artists by Country', fontsize=14, pad=12)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Artists', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "63e1ee33098db165"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking if there is an artist his/her country not Italy but his/her coordinates are in Italy"
   ],
   "id": "5e4b1638235968a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows where country is not Italy and coordinates are present\n",
    "non_italy_with_coords = df[\n",
    "    (df['country'].notna()) & \n",
    "    (df['country'] != \"Italia\") & \n",
    "    (df['latitude'].notna()) & \n",
    "    (df['longitude'].notna())\n",
    "]\n",
    "\n",
    "# Count the number of such records\n",
    "num_records = len(non_italy_with_coords)\n",
    "print(f\"Number of non-Italy records with coordinates: {num_records}\")\n",
    "\n",
    "# Show the records\n",
    "print(non_italy_with_coords[['country', 'latitude', 'longitude']])\n"
   ],
   "id": "4dddf188c9ebf58a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Artists Nationality Distribution\n",
    "\n",
    "Almost all artists are Italian (99.5%), with a small minority from Argentina (0.5%)."
   ],
   "id": "2e1ad2bab415658f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(df['nationality'].value_counts())\n",
    "# Count and calculate percentages\n",
    "nat_counts = df['nationality'].value_counts()\n",
    "nat_percent = (nat_counts / nat_counts.sum()) * 100\n",
    "nat_df = nat_percent.reset_index()\n",
    "nat_df.columns = ['nationality', 'percent']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=nat_df.head(20),  # show top 20 nationalities\n",
    "    x='percent',\n",
    "    y='nationality',\n",
    "    hue='nationality',\n",
    "    palette='crest',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Artists by Nationality\", fontsize=18, pad=15)\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Nationality\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(nat_df.head(20)['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, nat_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ffce33f7e8dc77af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking if there are artists with Non-Italian Nationality and Italian Coordinates (doubt)\n",
    "\n",
    "There are 40 artists with a nationality other than Italian (all Argentinian) but also have italian geographic coordinates. All these 40 artists share the same coordinates (After searching for this coordinates refers to the province of Parma)."
   ],
   "id": "bc52334e857943df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows where country is not Italy and coordinates are present\n",
    "non_italy_with_coords = df[\n",
    "    (df['nationality'].notna()) & \n",
    "    (df['nationality'] != \"Italia\") & \n",
    "    (df['latitude'].notna()) & \n",
    "    (df['longitude'].notna())\n",
    "]\n",
    "\n",
    "# Count the number of such records\n",
    "num_records = len(non_italy_with_coords)\n",
    "print(f\"Number of non-Italy Nationality records with coordinates: {num_records}\")\n",
    "\n",
    "# Show the records\n",
    "print(non_italy_with_coords[['nationality','latitude', 'longitude']])\n"
   ],
   "id": "622313e10022ddd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Nationality and Country Coherence Check  (doubt)\n",
    "\n",
    "This check ensures that each artist’s nationality matches the country. For example, artists from Italy should have nationality Italia, and those from Argentine should have Argentina.\n",
    "The results show no mismatches, meaning all records have consistent country–nationality relationships."
   ],
   "id": "2f96370251574eee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Example mapping of country → expected nationality\n",
    "country_to_nationality = {\n",
    "    \"Italy\": \"Italia\",\n",
    "    \"Argentine\": \"Argentina\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to check nationality vs country\n",
    "def check_nationality_country(row):\n",
    "    if pd.notna(row['country']) and pd.notna(row['nationality']):\n",
    "        expected_nationality = country_to_nationality.get(row['country'])\n",
    "        if expected_nationality and expected_nationality != row['nationality']:\n",
    "            return True  # incoherent\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['nationality_country_mismatch'] = df.apply(check_nationality_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['nationality_country_mismatch'].sum()\n",
    "print(f\"Number of nationality-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['nationality_country_mismatch']]\n",
    "print(mismatched_records[['country', 'nationality']])\n"
   ],
   "id": "d09bb3190a9bcbb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Distribution of Artist's Birth Places\n",
    "\n",
    "The majority of artists were born in major Italian cities, with Milano (1,843) and Roma (1,048) being the most frequent birthplaces, indicating a strong concentration of artists from these cultural and economic centers.\n",
    "\n",
    "Smaller Italian towns such as Senigallia (443), Torino (397), and Avellino (329) also show notable representation, suggesting a widespread national distribution beyond just the biggest cities.\n",
    "\n",
    "Only a few artists were born outside Italy — such as Buenos Aires (40) and Almería (26) — representing less than 1% of the total, which confirms that the dataset is predominantly composed of Italian-born artists."
   ],
   "id": "cbee6397db5a1076"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count occurrences and calculate percentages\n",
    "birth_place_counts = df['birth_place'].value_counts()\n",
    "print(birth_place_counts)\n",
    "birth_place_percent = (birth_place_counts / len(df)) * 100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.bar(birth_place_counts.index, birth_place_counts.values, color='skyblue')\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Distribution of Birth Places', fontsize=14)\n",
    "plt.xlabel('Birth Place')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add both count and percentage labels above bars\n",
    "for i, (count, percent) in enumerate(zip(birth_place_counts.values, birth_place_percent.values)):\n",
    "    plt.text(i, count + 10, f\"{count:,} \\n({percent:.1f}%)\", \n",
    "             ha='center', va='bottom', fontsize=6, color='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "24b6a028c75594c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking Birth Place–Country Consistency (doubt)\n",
    "\n",
    "This section verifies whether each artist’s birth place matches their country. It defines a list of known Italian cities and maps a few foreign cities to their respective countries. T The result shows the number of mismatches and lists the inconsistent records. The results show 26 mismatches, all involving artists born in Almería (Spain) but recorded with the country Italia"
   ],
   "id": "ff9539c2135a6dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Numerical Feature Definition\n",
    "\n",
    "# List of key numerical columns to analyze\n",
    "skewed_features = [\n",
    "    'tokens_per_sent', 'avg_token_per_clause', 'duration_ms', 'stats_pageviews', 'swear_EN', 'char_per_tok', 'swear_IT', 'bpm', 'n_sentences', 'n_tokens', 'rolloff', 'zcr', 'lexical_density', 'flatness'\n",
    "]\n",
    "\n",
    "simetric_features =[\n",
    "    'pitch', 'centroid', 'spectral_complexity', 'loudness', 'flux', 'rms'\n",
    "]\n"
   ],
   "id": "819c481e41fc3207",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# List of Italian cities from the data\n",
    "italian_cities = [\n",
    "    \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "    \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "    \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "    \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "    \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "    \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "    \"Lodi\"\n",
    "]\n",
    "\n",
    "# Map known foreign cities to their countries\n",
    "foreign_cities_to_country = {\n",
    "    \"Singapore\": \"Singapore\",\n",
    "    \"Buenos Aires\": \"Argentina\",\n",
    "    \"Almería\": \"Spagna\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to check birth_place vs country\n",
    "def check_birth_place_country(row):\n",
    "    if pd.notna(row['birth_place']) and pd.notna(row['country']):\n",
    "        if row['birth_place'] in italian_cities and row['country'] != \"Italia\":\n",
    "            return True  # mismatch\n",
    "        elif row['birth_place'] in foreign_cities_to_country:\n",
    "            if row['country'] != foreign_cities_to_country[row['birth_place']]:\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_country_mismatch'] = df.apply(check_birth_place_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_country_mismatch'].sum()\n",
    "print(f\"Number of birth_place-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['birth_place_country_mismatch']]\n",
    "display(mismatched_records[['birth_place', 'country',]])\n"
   ],
   "id": "4f9d275ca6ed453e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Birth Place vs Nationality Consistency Check\n",
    "\n",
    "This step verifies that each artist’s birth place aligns with their nationality. A list of Italian cities and a mapping of known foreign cities (like Almería, Buenos Aires, and Singapore) were used for comparison.\n",
    "\n",
    "The results show 107 mismatches, mainly involving artists born in Almería or Singapore but labeled with the nationality Italia, indicating possible errors or inconsistencies in the dataset."
   ],
   "id": "fbe563bb17ddb846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List of Italian cities\n",
    "italian_cities = [\n",
    "    \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "    \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "    \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "    \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "    \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "    \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "    \"Lodi\"\n",
    "]\n",
    "\n",
    "\n",
    "# Map special foreign cities to nationality\n",
    "foreign_cities_to_nationality = {\n",
    "    \"Singapore\": \"Singapore\",\n",
    "    \"Buenos Aires\": \"Argentina\",\n",
    "    \"Almería\": \"Spagna\",\n",
    "}\n",
    "\n",
    "# Function to check birth_place vs nationality\n",
    "def check_birth_place_nationality(row):\n",
    "    if pd.notna(row['birth_place']) and pd.notna(row['nationality']):\n",
    "        if row['birth_place'] in italian_cities and row['nationality'] != \"Italia\":\n",
    "            return True  # mismatch\n",
    "        elif row['birth_place'] in foreign_cities_to_nationality:\n",
    "            if row['nationality'] != foreign_cities_to_nationality[row['birth_place']]:\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_nationality_mismatch'] = df.apply(check_birth_place_nationality, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_nationality_mismatch'].sum()\n",
    "print(f\"Number of birth_place-nationality mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['birth_place_nationality_mismatch']]\n",
    "print(mismatched_records[['name','birth_place', 'nationality','country']])\n"
   ],
   "id": "6b58882cfb92733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Distribution of Songs by Province and Region\n",
    "\n",
    "This code calculates and visualizes the percentage distribution of songs by province and region. It counts occurrences, converts them to percentages, and displays bar charts with labeled values to show which areas have the highest song representation"
   ],
   "id": "170253cc47976957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Count occurrences and convert to percentages\n",
    "province_counts = df['province'].value_counts()\n",
    "province_percent = (province_counts / province_counts.sum()) * 100\n",
    "print('Provinces')\n",
    "print(province_counts)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "province_df = province_percent.reset_index()\n",
    "province_df.columns = ['province', 'percent']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=province_df.head(20),  # top 20 provinces if you want\n",
    "    x='percent',\n",
    "    y='province',\n",
    "    hue='province',\n",
    "    palette='viridis',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Songs by Province\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Province\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(province_df.head(20)['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, province_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "region_counts = df['region'].value_counts()\n",
    "print('Regions')\n",
    "print(region_counts)\n",
    "region_percent = (region_counts / region_counts.sum()) * 100\n",
    "region_df = region_percent.reset_index()\n",
    "region_df.columns = ['region', 'percent']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    data=region_df,\n",
    "    x='percent',\n",
    "    y='region',\n",
    "    hue='region',\n",
    "    palette='coolwarm',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Songs by Region\", fontsize=20, pad=15, color=\"#000000\")\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Region\", fontsize=12)\n",
    "\n",
    "# Add percentage labels\n",
    "for index, value in enumerate(region_df['percent']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "plt.xlim(0, region_df['percent'].max() + 5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c6e7f9b396ba5c43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Province/Region – Country Consistency Check\n",
    "\n",
    "This code verifies that Italian provinces and regions are correctly associated with the country \"Italia\""
   ],
   "id": "9d124e0d7e39d044"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example mapping of Italian regions to their provinces (from your data)\n",
    "region_provinces = {\n",
    "    \"Lombardia\": [\"Milano\", \"Monza e della Brianza\", \"Brescia\", \"Varese\", \"Lodi\"],\n",
    "    \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\"],\n",
    "    \"Lazio\": [\"Roma\"],\n",
    "    \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "    \"Piemonte\": [\"Torino\"],\n",
    "    \"Sardegna\": [\"Cagliari\", \"Gallura\"],\n",
    "    \"Puglia\": [\"Lecce\", \"Taranto\"],\n",
    "    \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "    \"Sicilia\": [\"Enna\"],\n",
    "    \"Emilia-Romagna\": [\"Bologna\"],\n",
    "    \"Calabria\": [\"Reggio Calabria\"],\n",
    "    \"Marche\": [\"Ancona\", \"Ascoli Piceno\"],\n",
    "    \"Toscana\": [\"Firenze\"]\n",
    "}\n",
    "\n",
    "# Flatten all Italian provinces for quick lookup\n",
    "all_italian_provinces = [prov for provs in region_provinces.values() for prov in provs]\n",
    "\n",
    "# Function to check province/region ↔ country\n",
    "def check_province_region_country(row):\n",
    "    if pd.notna(row['country']):\n",
    "        if pd.notna(row['province']) and row['province'] in all_italian_provinces:\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "        elif pd.notna(row['region']) and row['region'] in region_provinces.keys():\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['province_region_country_mismatch'] = df.apply(check_province_region_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['province_region_country_mismatch'].sum()\n",
    "print(f\"Number of province/region-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['province_region_country_mismatch']]\n",
    "print(mismatched_records[['province', 'region', 'country']])\n"
   ],
   "id": "186a2718405444b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Province/Region vs Birth Place – Consistency Check (doubt)\n",
    "\n",
    "This check compares each artist’s birth_place with the corresponding province and region. Mismatches occur when the province or region does not align with the birth_place. There are 2,901 mismatches between birth_place and province/region."
   ],
   "id": "74bba7a0d5ab3d7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Updated mapping of Italian regions to provinces including all birth_places in your data\n",
    "region_provinces = {\n",
    "    \"Lombardia\": [\"Milano\", \"Vimercate\", \"Sesto San Giovanni\", \"Alpignano\", \"Fiumicino\",\n",
    "                  \"Brescia\", \"Grugliasco\", \"Rho\", \"Gallarate\", \"Desenzano del Garda\", \"Lodi\", \"San Siro\"],\n",
    "    \"Lazio\": [\"Roma\"],\n",
    "    \"Piemonte\": [\"Torino\"],\n",
    "    \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\", \"Scafati\", \"Nocera Inferiore\"],\n",
    "    \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "    \"Sardegna\": [\"Cagliari\", \"Olbia\", \"Gallura\"],\n",
    "    \"Puglia\": [\"Lecce\", \"Taranto\", \"Grottaglie\", \"Sternatia\", \"San Benedetto del Tronto\"],\n",
    "    \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "    \"Sicilia\": [\"Enna\", \"Nicosia\"],\n",
    "    \"Emilia-Romagna\": [\"Bologna\"],\n",
    "    \"Calabria\": [\"Reggio Calabria\"],\n",
    "    \"Marche\": [\"Ancona\", \"Senigallia\", \"Ascoli Piceno\"],\n",
    "    \"Toscana\": [\"Firenze\", \"Scampia\", \"Padova\"]\n",
    "}\n",
    "\n",
    "# Flatten province → region mapping\n",
    "province_to_region = {prov: reg for reg, provs in region_provinces.items() for prov in provs}\n",
    "\n",
    "# Function to check birth_place ↔ province/region\n",
    "def check_birth_place_province_region(row):\n",
    "    if pd.notna(row['birth_place']):\n",
    "        # Only check Italian birth_places\n",
    "        if row['birth_place'] in province_to_region:\n",
    "            expected_region = province_to_region[row['birth_place']]\n",
    "            # Compare province and region if available\n",
    "            if (pd.notna(row['province']) and row['province'] != row['birth_place']) or \\\n",
    "               (pd.notna(row['region']) and row['region'] != expected_region):\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['birth_place_province_region_mismatch'] = df.apply(check_birth_place_province_region, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['birth_place_province_region_mismatch'].sum()\n",
    "print(f\"Number of birth_place-province/region mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show mismatched records\n",
    "mismatched_records = df[df['birth_place_province_region_mismatch']]\n",
    "print(mismatched_records[['birth_place', 'province', 'region']])\n"
   ],
   "id": "4c700f071cd5837e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Geographic Distribution of Artists by Province and Region\n",
    "\n",
    "This analysis aggregates the number of artists by their latitude, longitude, province, and region. The resulting table shows the locations with the highest concentration of artists at the top. For example, Milano (Lombardia) has the most artists with 1,843, followed by Roma (Lazio) with 1,048, and Torino (Piemonte) with 397. The code also generates a map where the size and color of the points reflect the number of artists per location, providing a clear visual of artist density across Italy."
   ],
   "id": "aa08356017541132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Aggregate by latitude and longitude to count number of artists\n",
    "location_counts = df.groupby(['latitude', 'longitude', 'region', 'province']).size().reset_index(name='num_artists')\n",
    "\n",
    "# Sort by number of artists descending\n",
    "location_counts = location_counts.sort_values(by='num_artists', ascending=False)\n",
    "\n",
    "# Print the sorted table\n",
    "print(location_counts)\n",
    "\n",
    "# Define a color scale\n",
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "\n",
    "# Create the scatter map\n",
    "fig = px.scatter_mapbox(\n",
    "    location_counts,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    hover_data=[\"region\", \"province\", \"num_artists\"],  # show count on hover\n",
    "    size=\"num_artists\",  # size of marker represents number of artists\n",
    "    color=\"num_artists\",  # color also shows density\n",
    "    color_continuous_scale=color_scale,\n",
    "    zoom=5,\n",
    "    height=800,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ],
   "id": "fb2adbafdca8174c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Lyrics"
   ],
   "id": "72040ce56404dc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dtale\n",
    "\n",
    "d = dtale.show(tracks, notebook=True)\n"
   ],
   "id": "72f208d74b313589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Investigating Missing Lyrics (NaNs vs. Empty Strings)"
   ],
   "id": "88336c7f88a0c9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows with 'lyrics' = NaN\n",
    "nan_lyrics_rows = df[df['lyrics'].isna()]\n",
    "print(f\"Number of rows with 'lyrics' = NaN: {len(nan_lyrics_rows)}\")\n",
    "if not nan_lyrics_rows.empty:\n",
    "    print(\"Examples of rows with 'lyrics' = NaN:\")\n",
    "    display(nan_lyrics_rows[['id', 'name_artist', 'full_title']])\n",
    "\n",
    "# Find rows with 'lyrics' = Empty String (\"\")\n",
    "empty_string_lyrics_rows = df[df['lyrics'].str.strip() == '']\n",
    "print(f\"\\nNumber of rows with 'lyrics' = empty string: {len(empty_string_lyrics_rows)}\")"
   ],
   "id": "f2a3257888fc528c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.dropna(subset=['lyrics'], inplace=True)"
   ],
   "id": "5b5b8a9cc8502e30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Found patterns in lyrics and delete it"
   ],
   "id": "7b0211fac7f292ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find songs with suspiciously short lyrics\n",
    "word_threshold = 20\n",
    "short_lyrics = df[df['n_tokens'] < word_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with suspiciously short lyrics (less than {word_threshold} words)\")\n",
    "print(f\"Total number: {len(short_lyrics)}\")\n",
    "\n",
    "# Inspecting the first 20 to see if we find patterns\n",
    "if not short_lyrics.empty:\n",
    "    print(\"\\nInspecting the short lyrics:\")\n",
    "    display(short_lyrics[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(20))"
   ],
   "id": "1015a70644bd0483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specific search for the found patterns\n",
    "pattern = 'Contributors|Contributor|Lyrics'\n",
    "\n",
    "pattern_matches = df[df['lyrics'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing 'junk' words\")\n",
    "print(f\"Total number: {len(pattern_matches)}\")\n",
    "if not pattern_matches.empty:\n",
    "    display(pattern_matches[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(23))"
   ],
   "id": "3c6d09a542ad4c54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for index, row in pattern_matches.iterrows():\n",
    "\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "\n",
    "    # Print the title for context, if present\n",
    "    if 'full_title' in row:\n",
    "        print(f\"TITLE: {row['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in row:\n",
    "        print(f\"Tokens: {row['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    full_text = row['lyrics']\n",
    "    print(full_text)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"End. Displayed {len(pattern_matches)} lyrics.\")"
   ],
   "id": "8fb933f7ea8721de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# maximum tokens for junk lyrics: 36\n",
    "# minimum tokens for actual lyrics: 108\n",
    "\n",
    "token_threshold = 60  # Safe threshold based on your analysis\n",
    "\n",
    "# Find the indices of rows that meet BOTH conditions\n",
    "indices_to_drop = df[\n",
    "    (df['lyrics'].str.contains(pattern, case=False, na=False)) &\n",
    "    (df['n_tokens'] < token_threshold)\n",
    "    ].index\n",
    "\n",
    "print(f\"DataFrame shape BEFORE dropping: {df.shape}\")\n",
    "print(f\"Number of 'junk AND short' rows (< {token_threshold} tokens) to drop: {len(indices_to_drop)}\")\n",
    "\n",
    "# Drop the rows\n",
    "if len(indices_to_drop) > 0:\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "    print(f\"DataFrame shape AFTER dropping: {df.shape}\")\n",
    "else:\n",
    "    print(\"No rows matched the criteria, no deletion performed.\")"
   ],
   "id": "71ed93ab98f0580"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specific search for \"junk\" words\n",
    "contributor_pattern = 'Contributors|Contributor'\n",
    "contributor_matches = df[df['lyrics'].str.contains(contributor_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing Contributors words\")\n",
    "print(f\"Total number: {len(contributor_matches)}\")\n",
    "if not contributor_matches.empty:\n",
    "    display(contributor_matches[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(23))"
   ],
   "id": "86d8607791850252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove the found pattern from the beginning of the lyrics\n",
    "\n",
    "indices_to_clean = df[df['lyrics'].str.contains(contributor_pattern, case=False, na=False)].index\n",
    "\n",
    "print(f\"Number of rows to clean: {len(indices_to_clean)}\")\n",
    "\n",
    "# Define the regex for the header: Number + \"Contributor\" (or \"Contributors\") + \"Lyrics\"\n",
    "header_regex = r\"^\\s*\\d+\\s+Contributor(s)?.*?\\s+Lyrics\\s*\"\n",
    "\n",
    "# replaces the found pattern with \"\"\n",
    "df.loc[indices_to_clean, 'lyrics'] = df.loc[indices_to_clean, 'lyrics'].str.replace(\n",
    "    header_regex, '', regex=True, flags=re.IGNORECASE\n",
    ").str.strip()\n",
    "\n",
    "print(\"\\nStart headers removed from the identified rows.\")\n",
    "\n",
    "print(\"\\nVerifying the cleaning (first 5 modified lyrics):\")\n",
    "for index in indices_to_clean[:5]:\n",
    "    if index in df.index:\n",
    "        print(\"==============================================\")\n",
    "        print(f\"INDEX: {index}\")\n",
    "        print(f\"CLEANED TEXT (preview):\\n'{str(df.loc[index, 'lyrics'])[:200]}...'\")"
   ],
   "id": "705bda322c8c97b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specific search for the word \"Lyrics\"\n",
    "lyrics_pattern = 'Lyrics'\n",
    "lyrics_matches = df[df['lyrics'].str.contains(lyrics_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing lyrics words\")\n",
    "print(f\"Total number: {len(lyrics_matches)}\")\n",
    "if not lyrics_matches.empty:\n",
    "    display(lyrics_matches[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(23))"
   ],
   "id": "c6b69a61b3c29f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specific search for the word \"Coming soon\"\n",
    "coming_soon_pattern = 'COMING SOON'\n",
    "coming_soon_matches = df[df['lyrics'].str.contains(coming_soon_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing coming soon words\")\n",
    "print(f\"Numero totale: {len(coming_soon_matches)}\")\n",
    "if not coming_soon_matches.empty:\n",
    "    display(coming_soon_matches[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(20))"
   ],
   "id": "df2d0ccf8059f49d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for index, riga in coming_soon_matches.iterrows():\n",
    "\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "\n",
    "    # Print the title for context, if present\n",
    "    if 'full_title' in row:\n",
    "        print(f\"TITLE: {row['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in row:\n",
    "        print(f\"Tokens: {row['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    full_text = row['lyrics']\n",
    "    print(full_text)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"End. Displayed {len(pattern_matches)}  lyrics.\")"
   ],
   "id": "12c4d45f1c652f58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Duplicate lyrics (different versions)"
   ],
   "id": "e00a71eef62b2808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows with duplicate lyrics\n",
    "duplicate_lyrics = df[df.duplicated(subset=['lyrics'], keep=False)]\n",
    "\n",
    "# Sort the duplicate rows by lyrics for easier comparison\n",
    "duplicate_lyrics_sorted = duplicate_lyrics.sort_values(by='lyrics')\n",
    "\n",
    "print(f\"--- Rows with Duplicate Lyrics ('lyrics') ---\")\n",
    "print(f\"Total number of rows involved: {len(duplicate_lyrics_sorted)}\")\n",
    "\n",
    "if not duplicate_lyrics_sorted.empty:\n",
    "    print(\"\\nExamples of duplicate lyrics:\")\n",
    "\n",
    "    display(duplicate_lyrics_sorted[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']].head(20))"
   ],
   "id": "fa687512b629af94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Intro songs"
   ],
   "id": "6407d057f0d0ff65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specific search for the word \"Intro\"\n",
    "intro_pattern = 'Intro'\n",
    "intro_songs = df[df['full_title'].str.contains(intro_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing intro words\")\n",
    "print(f\"Total number: {len(intro_songs)}\")\n",
    "if not intro_songs.empty:\n",
    "    display(intro_songs[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'album_type']].head(20))"
   ],
   "id": "f76585dcc75c7feb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "short_songs = intro_songs.sort_values(by='n_tokens', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in intro_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}...'\")"
   ],
   "id": "bf6a69b50169c9b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze the 'n_tokens' statistics for this group\n",
    "print(\"\\n'n_tokens' statistics for 'Intro' songs:\")\n",
    "print(intro_songs['n_tokens'].describe())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(intro_songs['n_tokens'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of 'n_tokens' ONLY for 'Intro' songs\")\n",
    "plt.xlabel(\"Number of Tokens (words)\")\n",
    "plt.show()"
   ],
   "id": "71ab74c3229c74dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We could remove the intros that are very short. The long ones are the lyrics for the entire song."
   ],
   "id": "ce35c5e1d0d1c6d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### n_token"
   ],
   "id": "c1258208e72d41d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows where 'n_tokens' is NaN\n",
    "nan_token_rows = df[df['n_tokens'].isna()]\n",
    "\n",
    "print(f\"Number of rows with 'n_tokens' = NaN: {len(nan_token_rows)}\")\n",
    "if not nan_token_rows.empty:\n",
    "    print(\"Examples of rows with 'n_token' = NaN:\")\n",
    "    display(nan_token_rows[['id', 'name_artist', 'full_title', 'lyrics']])\n",
    "\n",
    "# Find rows where 'n_tokens' is 0\n",
    "testi_zero_token = df[df['n_tokens'] <= 0]\n",
    "\n",
    "print(f\"Righe con n_tokens <= 0 \")\n",
    "print(f\"Numero totale: {len(testi_zero_token)}\")\n"
   ],
   "id": "e7fa27669d8f09cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(df['n_tokens'].describe())\n",
    "\n",
    "print(\"\\nDisplaying Box Plot to identify outliers...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['n_tokens'])\n",
    "plt.title('Box Plot of n_tokens (to find high and low outliers)')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.show()"
   ],
   "id": "ecdf96bbbd476174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Statistical Analysis: IQR Method ('n_tokens' is skewed)\n",
    "col = 'n_tokens'\n",
    "\n",
    "Q1 = df[col].quantile(0.25)\n",
    "Q3 = df[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "low_outliers = df[df[col] < lower_bound]\n",
    "high_outliers = df[df[col] > upper_bound]\n",
    "\n",
    "print(f\"Variable '{col}':\")\n",
    "print(f\"  IQR Limits: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"  Found {len(low_outliers)} outliers below the limit.\")\n",
    "print(f\"  Found {len(high_outliers)} outliers above the limit.\")"
   ],
   "id": "1c40ba3c5af80b0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find songs with short lyrics (this limit is taken from the lower IQR bound)\n",
    "word_threshold = 17\n",
    "short_lyrics = df[df['n_tokens'] < word_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with short lyrics (less than {word_threshold} words)\")\n",
    "print(f\"Total number: {len(short_lyrics)}\")\n",
    "\n",
    "if not short_lyrics.empty:\n",
    "    print(\"\\nInspecting the short lyrics:\")\n",
    "    display(short_lyrics[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']].head(20))"
   ],
   "id": "b2217195a81713a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "short_songs = short_lyrics.sort_values(by='n_tokens', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}...'\")"
   ],
   "id": "3ed4c631386e2225"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "They could be removed since they are very short and the duration in ms is too high."
   ],
   "id": "62ceecb21f428515"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# long songs by n_tokens (from lowest to highest)\n",
    "high_token_threshold = 977\n",
    "\n",
    "very_long_songs = df[(df['n_tokens'] > high_token_threshold) & (df['n_tokens'].notna())]\n",
    "\n",
    "print(f\"Songs with n_tokens > {high_token_threshold} (High Outliers)\")\n",
    "print(f\"Total number: {len(very_long_songs)}\")\n",
    "\n",
    "if not very_long_songs.empty:\n",
    "    print(\"\\nExamples of very long songs:\")\n",
    "\n",
    "    display(very_long_songs[['id', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']].head(20))"
   ],
   "id": "2627e8b2e12a0f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the 159 very long songs and look for those with a very short duration\n",
    "low_duration_threshold = 60000 # 60 seconds\n",
    "duration_anomalies = very_long_songs[very_long_songs['duration_ms'] < low_duration_threshold]\n",
    "\n",
    "print(f\"Songs with n_tokens > 977 BUT duration_ms < {low_duration_threshold/1000} seconds \")\n",
    "print(f\"Number found: {len(duration_anomalies)}\")\n",
    "if not duration_anomalies.empty:\n",
    "    display(duration_anomalies[['id', 'name_artist', 'full_title', 'n_tokens', 'duration_ms']])"
   ],
   "id": "a923d7fa952af440"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the 159 very long songs\n",
    "# Look for those with very low lexical density\n",
    "low_density_threshold = 0.1\n",
    "density_anomalies = very_long_songs[very_long_songs['lexical_density'] < low_density_threshold]\n",
    "\n",
    "print(f\"\\nSongs with n_tokens > 977 BUT lexical_density < {low_density_threshold}\")\n",
    "print(f\"Number found: {len(density_anomalies)}\")\n",
    "if not density_anomalies.empty:\n",
    "    display(density_anomalies[['id', 'name_artist', 'full_title', 'n_tokens', 'lexical_density', 'lyrics']].head())"
   ],
   "id": "600de45f6a5faec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nRandom inspection of long songs (near the threshold)\")\n",
    "display(very_long_songs.sort_values('n_tokens').head(5)[['id', 'name_artist', 'full_title', 'n_tokens', 'lyrics']])"
   ],
   "id": "6601654cb32cbf34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### char_per_tok"
   ],
   "id": "b65d1c9416717da6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows where 'char_per_tok' is NaN\n",
    "nan_char_per_tok_rows = df[df['char_per_tok'].isna()]\n",
    "\n",
    "print(f\"Number of rows with 'char_per_tokens' = NaN: {len(nan_char_per_tok_rows)}\")\n",
    "if not nan_char_per_tok_rows.empty:\n",
    "    print(\"Examples of rows with 'char_per_token' = NaN:\")\n",
    "    display(nan_char_per_tok_rows[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens']])"
   ],
   "id": "ce6cae189ca1c5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Plotting distribution for 'char_per_tok'...\")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 1. Histogram\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n",
    "sns.histplot(df['char_per_tok'].dropna(), kde=True, bins=30)\n",
    "plt.title('Histogram of char_per_tok')\n",
    "plt.xlabel('Average Characters per Token')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Box Plot\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n",
    "sns.boxplot(x=df['char_per_tok'].dropna())\n",
    "plt.title('Box Plot of char_per_tok')\n",
    "plt.xlabel('Average Characters per Token')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics for char_per_tok ---\")\n",
    "print(df['char_per_tok'].describe())"
   ],
   "id": "b74bc55b2425c009"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The descriptive statistics show mean=4.02, std=0.28, min=2.0, max=12.0. The mean and median (50%=4.01) are almost identical.\n",
    "\n",
    "Meaning: The distribution is highly concentrated around an average word length of 4 characters, with low variability (low standard deviation). The histogram confirms a shape that is very close to a normal distribution (bell curve)."
   ],
   "id": "9ae49f0afec2e1a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Statistical Analysis: IQR Method (''char_per_tok' is skewed)\n",
    "col = 'char_per_tok'\n",
    "\n",
    "Q1 = df[col].quantile(0.25)\n",
    "Q3 = df[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "low_outliers = df[df[col] < lower_bound]\n",
    "high_outliers = df[df[col] > upper_bound]\n",
    "\n",
    "print(f\"Variable '{col}':\")\n",
    "print(f\"  IQR Limits: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"  Found {len(low_outliers)} outliers below the limit.\")\n",
    "print(f\"  Found {len(high_outliers)} outliers above the limit.\")"
   ],
   "id": "e064f0bc59eb141f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the threshold for average characters per token\n",
    "char_tok_threshold = 3\n",
    "low_char_tok_rows = df[df['char_per_tok'] < char_tok_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with few characters per token (less than {char_tok_threshold} characters)\")\n",
    "print(f\"Total number: {len(low_char_tok_rows)}\")\n",
    "\n",
    "if not low_char_tok_rows.empty:\n",
    "    print(\"\\nInspecting lyrics with low 'char_per_tok':\")\n",
    "    display(low_char_tok_rows[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'char_per_tok']].head(20))"
   ],
   "id": "6e0d8d398647ef48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "short_songs = low_char_tok_rows.sort_values(by='char_per_tok', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in low_char_tok_rows.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"TOKENS: {row['n_tokens']}\")\n",
    "    print(f\"CHARACTER PER TOKEN: {row['char_per_tok']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "id": "419e9b121477b7fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the threshold for average characters per token\n",
    "char_tok_threshold = 4.61\n",
    "high_char_tok_rows = df[df['char_per_tok'] > char_tok_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with characters per token (more than {char_tok_threshold} characters)\")\n",
    "print(f\"Total number: {len(high_char_tok_rows)}\")\n",
    "\n",
    "if not high_char_tok_rows.empty:\n",
    "    print(\"\\nInspecting lyrics with low 'char_per_tok':\")\n",
    "    display(high_char_tok_rows[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']].head(20))"
   ],
   "id": "3e40a3d07a0ba8ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "short_songs = high_char_tok_rows.sort_values(by='char_per_tok', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in low_char_tok_rows.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"TOKENS: {row['n_tokens']}\")\n",
    "    print(f\"CHARACTER PER TOKEN: {row['char_per_tok']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "id": "2b468cc751aafbad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Limits: The IQR method identified outliers below 3.42 and above 4.61.\n",
    "\n",
    "High Outliers (> 4.61):\n",
    "\n",
    "- Result: You found 221 high outliers. The maximum value is 12.0.\n",
    "\n",
    "- Meaning: These represent songs using words that are, on average, longer than usual (more than 4.6 characters). The max value of 12.0 is high but plausible (think of technical terms, complex vocabulary, or languages with naturally longer words). These 221 outliers likely represent valid data reflecting specific lyrical styles.\n",
    "\n",
    "Low Outliers (< 3.42):\n",
    "\n",
    "- Result: You found 121 low outliers. The minimum value is 2.0. You specifically inspected 11 rows with char_per_tok < 3.\n",
    "\n",
    "- Meaning: These represent songs with, on average, very short words. Your inspection of values < 3 confirms this:\n",
    "Some have very few tokens (Indices 3473, 3586), making a low average plausible.\n",
    "Some contain mainly interjections or simple words (Index 3999: \"Ah ah\"). Plausible.\n",
    "Some are very repetitive (Index 10316: \"Mangio al Mc\"). Plausible.\n",
    "Others seem like normal songs. A low char_per_tok might indicate a very colloquial, simple style, or many monosyllabic words. Seems plausible."
   ],
   "id": "1ef75d530ff37cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### n_sentences"
   ],
   "id": "4fdf1550ee699403"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows where 'n_sentences' is NaN\n",
    "nan_n_sentences_rows = df[df['n_sentences'].isna()]\n",
    "\n",
    "print(f\"Number of rows with 'n_sentences' = NaN: {len(nan_n_sentences_rows)}\")\n",
    "if not nan_n_sentences_rows.empty:\n",
    "    print(\"Examples of rows with 'n_sentences' = NaN:\")\n",
    "    display(nan_n_sentences_rows[['id', 'name_artist', 'full_title', 'lyrics']])"
   ],
   "id": "5d9a0356af4a6e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Plotting distribution for 'n_sentences'\")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 1. Histogram\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n",
    "sns.histplot(df['n_sentences'].dropna(), kde=True, bins=30)\n",
    "plt.title('Histogram of n_sentences')\n",
    "plt.xlabel('Average Characters per Token')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Box Plot\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n",
    "sns.boxplot(x=df['n_sentences'].dropna())\n",
    "plt.title('Box Plot of n_sentences')\n",
    "plt.xlabel('Average Characters per Token')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics for n_sentences ---\")\n",
    "print(df['n_sentences'].describe())"
   ],
   "id": "5fc4ff4390a25ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Statistical Analysis: IQR Method\n",
    "col = 'n_sentences'\n",
    "\n",
    "Q1 = df[col].quantile(0.25)\n",
    "Q3 = df[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "low_outliers = df[df[col] < lower_bound]\n",
    "high_outliers = df[df[col] > upper_bound]\n",
    "\n",
    "print(f\"Variable '{col}':\")\n",
    "print(f\"  IQR Limits: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"  Found {len(low_outliers)} outliers below the limit.\")\n",
    "print(f\"  Found {len(high_outliers)} outliers above the limit.\")"
   ],
   "id": "b7b3313b50b661c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the threshold for the number of sentence\n",
    "n_sentences_threshold = 5\n",
    "low_n_sentences_rows = df[df['n_sentences'] <= n_sentences_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with few number of sentence (less than {n_sentences_threshold} sentence)\")\n",
    "print(f\"Total number: {len(low_n_sentences_rows)}\")\n",
    "\n",
    "if not low_n_sentences_rows.empty:\n",
    "    print(\"\\nInspecting lyrics with low 'n_sentences':\")\n",
    "    display(low_n_sentences_rows[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences']].head(20))"
   ],
   "id": "f4fd10f64b72ecbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "short_songs = low_n_sentences_rows.sort_values(by='n_sentences', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in low_n_sentences_rows.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"TOKENS: {row['n_tokens']}\")\n",
    "    print(f\"CHARACTER PER TOKEN: {row['char_per_tok']}\")\n",
    "    print(f\"NUMBER PER SENTENCE: {row['n_sentences']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "id": "99ce5864051c0847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the threshold for average characters per token\n",
    "n_sentences_threshold = 113\n",
    "high_n_sentences_rows = df[df['n_sentences'] > n_sentences_threshold]\n",
    "\n",
    "print(f\"\\nTotal rows with characters per token (more than {n_sentences_threshold} characters)\")\n",
    "print(f\"Total number: {len(high_n_sentences_rows)}\")\n",
    "\n",
    "if not high_n_sentences_rows.empty:\n",
    "    print(\"\\nInspecting lyrics with low 'n_sentences':\")\n",
    "    display(high_n_sentences_rows[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']].head(20))"
   ],
   "id": "55ec6136f6743878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "short_songs = high_n_sentences_rows.sort_values(by='n_sentences', ascending=True)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in high_n_sentences_rows.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"TOKENS: {row['n_tokens']}\")\n",
    "    print(f\"CHARACTER PER TOKEN: {row['char_per_tok']}\")\n",
    "    print(f\"NUMBER PER SENTENCE: {row['n_sentences']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "id": "606be40baac63011"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.  **Missing Values (NaN):**\n",
    "    * **Result:** You have confirmed there are **73 NaN**\n",
    "    * **Meaning:** This is **not** actual missing data where lyrics are absent. It's a **processing error** where the sentence calculation failed or was skipped for these specific tracks, even though the `lyrics` text exists.\n",
    "    * **Action (Data Cleaning):** These 73 NaN values will need to be **recalculated** during the data cleaning phase by analyzing the corresponding `lyrics` content.\n",
    "\n",
    "2.  **Distribution and Statistics:**\n",
    "    * **Result:** The descriptive statistics show `mean=60.0`, `median (50%)=59.0`, `min=1.0`, `max=437.0`. The mean and median are very close, suggesting a relatively centered distribution for the bulk of the data.\n",
    "    * **Distribution Shape:** Your skewness analysis (`skewness = 1.46`, Positive) indicates a slight positive skew (right tail). The histogram you provided visually confirms this: the distribution is largely unimodal and somewhat bell-shaped but stretches out towards higher sentence counts.\n",
    "    * **Meaning:** Most songs contain between 46 (`25%`) and 73 (`75%`) sentences. The slight right skew is expected, as some songs are naturally much longer or structurally more fragmented (e.g., storytelling, cyphers) than the average track.\n",
    "\n",
    "3.  **Outliers (IQR Method):**\n",
    "    * **Limits:** The IQR method defined outliers as values below `5.50` and above `113.50`.\n",
    "    * **High Outliers (> 113.50):**\n",
    "        * **Result:** You found **205 high outliers**. The maximum value is 437.\n",
    "        * **Meaning:** These represent songs with a very large number of detected sentences. Considering that you also found valid songs with very high `n_tokens` (up to ~3000), having a high sentence count (up to 437) is **plausible** for these longer tracks. High sentence counts might also arise from texts with very short sentences or many line breaks interpreted as sentence endings. These outliers likely represent **valid data** reflecting longer or structurally distinct songs (like spoken word intros, storytelling tracks).\n",
    "    * **Low Outliers (< 5.50):**\n",
    "        * **Result:** You found **56 low outliers**. The minimum value is 1.0.\n",
    "        * **Meaning:** These represent songs identified as having very few sentences (1 to 5). This strongly correlates with the findings for `n_tokens` (where 18 outliers had `< 17` tokens). These low counts are **highly plausible** for very short texts such as skits, brief intros/outros, or tracks that are primarily instrumental with minimal lyrics. Your inspection of lyrics for `n_tokens < 17` likely included many of these cases (e.g., indices 758, 2671, 3172 from your previous inspection had few sentences). These low outliers appear to be **valid data** representing short lyrical content.\n",
    "\n",
    "* **Data Quality Interaction:** Extremely low `n_sentences` values (like 1) combined with high `n_tokens` are **indicators of poor punctuation or formatting in the source `lyrics` data**, which also caused the unrealistic high outliers in `tokens_per_sent`. These specific low `n_sentences` values, while artifacts, correctly reflect how the text was likely processed.\n"
   ],
   "id": "92d9521b6cc4d666"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### tokens_per_sent"
   ],
   "id": "aa4dec539077f0e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find rows where 'n_sentences' is NaN\n",
    "nan_tokens_per_sent_rows = df[df['tokens_per_sent'].isna()]\n",
    "\n",
    "print(f\"Number of rows with 'tokens_per_sent' = NaN: {len(nan_tokens_per_sent_rows)}\")\n",
    "if not nan_tokens_per_sent_rows.empty:\n",
    "    print(\"Examples of rows with 'tokens_per_sent' = NaN:\")\n",
    "    display(nan_tokens_per_sent_rows[['id', 'name_artist', 'full_title', 'lyrics']])"
   ],
   "id": "b5963453ac1fd2ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### avg_token_per_clause"
   ],
   "id": "17bedb37b98c07ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### lexical_density"
   ],
   "id": "521bf1539b70133e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Verifying if NaN locations match across lyrical features...\")\n",
    "\n",
    "# 1. Get indices where n_tokens is NaN\n",
    "nan_indices_ntokens = df[df['n_tokens'].isna()].index\n",
    "\n",
    "# 2. Define the other columns to check\n",
    "other_lyrical_cols = ['tokens_per_sent', 'char_per_tok', 'lexical_density', 'avg_token_per_clause']\n",
    "\n",
    "# 3. Check each column against n_tokens\n",
    "all_match = True\n",
    "for col in other_lyrical_cols:\n",
    "    nan_indices_col = df[df[col].isna()].index\n",
    "\n",
    "    # Compare the index sets (convert to sets for easy comparison)\n",
    "    if set(nan_indices_ntokens) != set(nan_indices_col):\n",
    "        all_match = False\n",
    "        print(f\"\\nMismatch found for column: '{col}'\")\n",
    "        # Optional: Find which indices differ\n",
    "        diff1 = set(nan_indices_ntokens) - set(nan_indices_col)\n",
    "        diff2 = set(nan_indices_col) - set(nan_indices_ntokens)\n",
    "        if diff1:\n",
    "            print(f\"  Indices NaN in 'n_tokens' but not in '{col}': {list(diff1)}\")\n",
    "        if diff2:\n",
    "            print(f\"  Indices NaN in '{col}' but not in 'n_tokens': {list(diff2)}\")\n",
    "    else:\n",
    "        print(f\"  NaN indices match for 'n_tokens' and '{col}'. Count: {len(nan_indices_col)}\")\n",
    "\n",
    "\n",
    "# 4. Final confirmation\n",
    "if all_match:\n",
    "    print(\"\\nConfirmation: The NaN values occur in exactly the same rows for 'n_tokens' and the other derived lyrical features.\")\n",
    "    print(f\"Total number of rows with NaNs in these columns: {len(nan_indices_ntokens)}\")\n",
    "else:\n",
    "    print(\"\\nWarning: There is a mismatch in the location of NaN values between 'n_tokens' and at least one other derived lyrical feature.\")"
   ],
   "id": "98a861d9a480f445"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### swear_IT and swear_IT_words"
   ],
   "id": "e1cd3113024282ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### swear_EN and swear_EN_words"
   ],
   "id": "77211a714ae4a3e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####  Feature lyriche"
   ],
   "id": "96c324fc1f4c1fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 1. Investiga tokens_per_sent > 12.58 (specifically the highest values) ---\n",
    "soglia_alta_tps = 12.58\n",
    "outlier_tps_alti = df[df['tokens_per_sent'] > soglia_alta_tps].sort_values('tokens_per_sent', ascending=False)\n",
    "\n",
    "print(f\"--- Righe con tokens_per_sent > {soglia_alta_tps} (Valori più alti) ---\")\n",
    "print(f\"Numero totale outlier alti: {len(outlier_tps_alti)}\")\n",
    "if not outlier_tps_alti.empty:\n",
    "    print(\"\\nEsempi con tokens_per_sent più alto:\")\n",
    "    display(outlier_tps_alti[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'tokens_per_sent']].head(20))"
   ],
   "id": "918dda076184e43b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for index, riga in outlier_tps_alti.iterrows():\n",
    "\n",
    "    # Aggiungiamo dei separatori per leggibilità\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDICE (originale): {index}\")\n",
    "\n",
    "    # Stampiamo anche il titolo per contesto, se presente\n",
    "    if 'full_title' in riga:\n",
    "        print(f\"TITOLO: {riga['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in riga:\n",
    "        print(f\"Token: {riga['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    testo_completo = riga['lyrics']\n",
    "    print(testo_completo)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"Fine. Visualizzati {len(outlier_tps_alti)} outlier_tps_alti'.\")"
   ],
   "id": "60dadcee2e170016"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 2. Investiga avg_token_per_clause > 11.44 (specifically the highest values) ---\n",
    "soglia_alta_atpc = 11.44\n",
    "outlier_atpc_alti = df[df['avg_token_per_clause'] > soglia_alta_atpc].sort_values('avg_token_per_clause', ascending=False)\n",
    "\n",
    "print(f\"\\n--- Righe con avg_token_per_clause > {soglia_alta_atpc} (Valori più alti) ---\")\n",
    "print(f\"Numero totale outlier alti: {len(outlier_atpc_alti)}\")\n",
    "if not outlier_atpc_alti.empty:\n",
    "    print(\"\\nTop 5 esempi con avg_token_per_clause più alto:\")\n",
    "    display(outlier_atpc_alti[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'avg_token_per_clause']].head())"
   ],
   "id": "d4825023eba069c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 3. Investiga avg_token_per_clause == 0 ---\n",
    "zero_atpc = df[df['avg_token_per_clause'] == 0]\n",
    "\n",
    "print(f\"\\n--- Righe con avg_token_per_clause == 0 ---\")\n",
    "print(f\"Numero totale: {len(zero_atpc)}\")\n",
    "if not zero_atpc.empty:\n",
    "    print(\"\\nEsempi di righe con avg_token_per_clause == 0:\")\n",
    "    display(zero_atpc[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'avg_token_per_clause']].head())\n",
    "\n"
   ],
   "id": "94777a7426194527"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
