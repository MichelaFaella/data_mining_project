{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f0f1be6fbd9d07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Understanding & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Show full column contents (no truncation)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Don’t wrap long output lines\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import altair as alt\n",
    "import dtale\n",
    "import plotly.express as px\n",
    "\n",
    "import project_fuctions as functions\n",
    "\n",
    "import ast\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fff8d452860ce15a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e2a5a2820eb7c52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "artists_path = 'data\\\\artists.csv'\n",
    "tracks_path = 'data\\\\tracks.csv'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e207175b03f0b164",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This code automatically detects the correct separator for two dataset files (tracks and artists) by checking which character — comma, semicolon, or tab — appears most in the first line. It then loads each file into a pandas DataFrame using the detected separator, prints their shapes, and displays the first few rows.\n",
    "\n",
    " The tracks dataset has 11,166 rows and 45 columns, while the artists dataset has 104 rows and 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "id": "3acb41c87405bc1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Funzione helper per capire il separatore corretto\n",
    "def detect_separator(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample = f.readline()\n",
    "    # Conta quanti separatori compaiono\n",
    "    seps = {',': sample.count(','), ';': sample.count(';'), '\\t': sample.count('\\t')}\n",
    "    best_sep = max(seps, key=seps.get)\n",
    "    print(f\"Detected separator for {filepath}: '{best_sep}'\")\n",
    "    return best_sep\n",
    "\n",
    "# Rileva automaticamente il separatore\n",
    "sep_tracks = detect_separator(tracks_path)\n",
    "sep_artists = detect_separator(artists_path)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "# Carica i dataset in base al separatore rilevato\n",
    "tracks = pd.read_csv(tracks_path, sep=sep_tracks, encoding='utf-8', engine='python')\n",
    "artists = pd.read_csv(artists_path, sep=sep_artists, encoding='utf-8', engine='python')\n",
    "\n",
    "df = tracks.merge(\n",
    "    artists,\n",
    "    left_on=\"id_artist\",\n",
    "    right_on=\"id_author\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_artist\")\n",
    ")\n",
    "\n",
    "# Mostra alcune info per verifica\n",
    "print(f\"Tracks shape: {tracks.shape}\")\n",
    "print(f\"Artists shape: {artists.shape}\")\n",
    "print(\"Shape df (merged):\", df.shape)\n",
    "print('------------------------------------')\n",
    "\n",
    "print('TRACKS')\n",
    "display(tracks.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('ARTISTS')\n",
    "display(artists.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('MERGERD')\n",
    "display(df.head(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c056920bf2b98f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DATA UNDERSTANDING\n",
    "\n",
    "Objectives:\n",
    "- Describe the `all_tracks.csv` and `artist.csv` datasets.\n",
    "- Document the meaning of the variables (data dictionary).\n",
    "- Analyze data quality: types, missing values, duplicates.\n",
    "- Explore distributions and relationships between relevant features.\n",
    "\n",
    "All of this will be included in the \"Data Understanding\" section of the report."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a385e9766247958",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"=== INFO all_tracks ===\")\n",
    "tracks.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e02e77ff24d944c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"\\n=== INFO artist ===\")\n",
    "artists.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "117b12b44b1cf601",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"\\n=== INFO df (merged) ===\")\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58c38165b1623d4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Distribution\n",
    "The following table and histogram show the numerical data distribution in the dataset:\n",
    "\n",
    "- **Most features** (`n_sentences`, `n_tokens`, `tokens_per_sent`, `char_per_tok`, `lexical_density`, `avg_token_per_clause`, `centroid`, `rolloff`, `rms`, `zcr`, `flatness`, `flux`, `spectral_complexity`, `pitch`, `loudness`) show **bell-shaped or near-normal distributions**.\n",
    "\n",
    "- **Highly skewed features** (`stats_pageviews`, `bpm`, `tokens_per_sent`, `duration_ms`, `popularity`) have a **long right tail**, indicating a few extreme values or outliers (common in popularity or count-based features).\n",
    "\n",
    "- **Temporal features** (`year`, `month`, `day`) display **non-uniform distributions**; e.g., `year` is concentrated around recent decades, showing most songs are modern.\n",
    "\n",
    "- **Geographical features** (`latitude`, `longitude`) have **peaks corresponding to specific locations**, likely representing where artists or tracks are clustered.\n",
    "\n",
    "The data distribution and the statistics presented above reveal some anomalies and irregularities in the dataset. These issues will be examined and addressed in the following section."
   ]
  },
  {
   "cell_type": "code",
   "id": "5dea1f3a0ab2d21a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Select numeric columns\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# --- Summary statistics table ---\n",
    "display(df[num_cols].describe().T.style.background_gradient(cmap='RdPu'))\n",
    "\n",
    "# --- Histograms for each numeric column ---\n",
    "n_cols = 4\n",
    "n_rows = -(-len(num_cols) // n_cols)  # ceil division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col].dropna(), bins=30, kde=True, color=\"#d36ba8\", ax=axes[i])\n",
    "    axes[i].set_title(col, fontsize=18, color=\"#b30059\")   # larger title font\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "    axes[i].tick_params(axis='both', labelsize=12)          # larger tick labels\n",
    "\n",
    "# Remove unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Distribution of Numerical Features\", fontsize=24, color=\"#000000\", y=1.02)  # larger main title\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96c28ca8b139ecc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate basic statistics (mean and median)\n",
    "statistics = df[num_cols].describe().T\n",
    "\n",
    "# Calculate skewness\n",
    "skews = df[num_cols].skew()\n",
    "\n",
    "skew_analysis = pd.DataFrame({\n",
    "    'mean': statistics['mean'],\n",
    "    'median': statistics['50%'],\n",
    "    'skewness_value': skews\n",
    "})\n",
    "\n",
    "# Define a function to classify skewness\n",
    "# These are standard thresholds used in statistics:\n",
    "# > +0.5 = Positive Skew (Right-tailed)\n",
    "# < -0.5 = Negative Skew (Left-tailed)\n",
    "# Between -0.5 and +0.5 = Substantially Symmetric\n",
    "def classify_skew(skew_value):\n",
    "    if skew_value > 0.5:\n",
    "        return \"Positive (Right Skew)\"\n",
    "    elif skew_value < -0.5:\n",
    "        return \"Negative (Left Skew)\"\n",
    "    else:\n",
    "        return \"Symmetric\"\n",
    "\n",
    "skew_analysis['skew_type'] = skew_analysis['skewness_value'].apply(classify_skew)\n",
    "\n",
    "print(\"Skewness Analysis of Numerical Features\")\n",
    "\n",
    "display(skew_analysis.sort_values(by='skewness_value', ascending=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c4c43595aa22254",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DATA QUALITY CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8837d1fe94af1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Discovering Missing Value\n",
    "\n",
    "This code analyzes missing values in the DataFrame by counting how many entries are NaN for each column and calculating the corresponding percentage. It creates a summary table showing only columns with missing data, sorted by the highest percentage."
   ]
  },
  {
   "cell_type": "code",
   "id": "62409edac452a620",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calcolo missing values e percentuali\n",
    "missing_count = df.isna().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "missing_df = (\n",
    "    pd.DataFrame({'missing_count': missing_count, 'missing_percent': missing_percent})\n",
    "    .sort_values('missing_percent', ascending=False)\n",
    "    .query('missing_percent > 0')\n",
    ")\n",
    "\n",
    "# Mostra tabella riepilogativa (gradiente rosso-magenta)\n",
    "display(\n",
    "    missing_df\n",
    "    .style.background_gradient(subset=['missing_percent'], cmap='RdPu')  \n",
    "    .format({'missing_percent': '{:.2f}%'})\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f7f6ab4176146d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following heatmap visualizes missing values in the dataset, with each row representing a record and each column a feature. Colored cells indicate missing entries, providing a clear overview of where data is incomplete.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "82353fc9511e52f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trasformiamo il dataframe in formato long (necessario per Altair)\n",
    "na_df = df.isna().reset_index().melt(id_vars='index')\n",
    "na_df.columns = ['row', 'column', 'na']\n",
    "\n",
    "chart = alt.Chart(na_df).mark_rect().encode(\n",
    "    x=alt.X('column:N', title=None),\n",
    "    y=alt.Y('row:O', title=None, axis=None),  # rimuove etichette righe come seaborn\n",
    "    color=alt.Color('na:N',\n",
    "        scale=alt.Scale(range=[\"#8e44ad\", \"#f3e5f5\"]),  # viola scuro → viola chiaro\n",
    "        legend=alt.Legend(title=\"Missing\")\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title=\"Missing Values Matrix (Altair)\"\n",
    ")\n",
    "\n",
    "chart\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f000923e46df66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following bar plot shows the percentage of missing values per feature, with the top 20 features that have the most missing data."
   ]
  },
  {
   "cell_type": "code",
   "id": "7ce4a679448a86f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prendiamo solo le prime 20 feature (come nel seaborn)\n",
    "missing_top20 = missing_df.reset_index().rename(columns={'index':'feature'})\n",
    "\n",
    "# Grafico: barre orizzontali viola\n",
    "bars = (\n",
    "    alt.Chart(missing_top20)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"missing_percent:Q\", title=\"Missing values (%)\"),\n",
    "        y=alt.Y(\"feature:N\", sort='-x', title=\"Feature name\"),\n",
    "        color=alt.Color(\n",
    "            \"feature:N\",\n",
    "            scale=alt.Scale(range=[\"#f3e5f5\", \"#e1bee7\", \"#ce93d8\", \"#ba68c8\", \"#9c27b0\"][::-1]),   # palette viola\n",
    "            legend=None\n",
    "        ),\n",
    "        tooltip=[\n",
    "            alt.Tooltip(\"feature\", title=\"Feature\"),\n",
    "            alt.Tooltip(\"missing_percent\", title=\"Missing (%)\", format=\".2f\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Testo con la percentuale sulla barra\n",
    "labels = (\n",
    "    alt.Chart(missing_top20)\n",
    "    .mark_text(align=\"left\", baseline=\"middle\", dx=5, color=\"#4A0038\")\n",
    "    .encode(\n",
    "        x=\"missing_percent:Q\",\n",
    "        y=\"feature:N\",\n",
    "        text=alt.Text(\"missing_percent:Q\", format=\".1f\")\n",
    "    )\n",
    ")\n",
    "\n",
    "chart = (bars + labels).properties(\n",
    "    title=\"Percentage of Missing Values by Feature\",\n",
    "    width=550,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "chart"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9285bb527bd3b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Missing Values Propagation After Merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "1db77fce246e2cdd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "artists_missing = artists.isna().mean().sort_values(ascending=False) * 100\n",
    "print(artists_missing)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "239f28a82cb09974",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The visualization highlights that missing values in attributes such as active_start, region, and birth_place have increased after merging due to the replication of incomplete artist metadata across multiple tracks.\n",
    "This confirms that the merge process did not introduce new nulls, but propagated pre-existing ones."
   ]
  },
  {
   "cell_type": "code",
   "id": "e24905a193bc14ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Colonne provenienti dal dataset artists \n",
    "artist_cols = list(artists.columns)\n",
    "\n",
    "# Conta i NaN prima e dopo il merge\n",
    "missing_before = artists[artist_cols].isna().sum()\n",
    "missing_after = df[artist_cols].isna().sum()\n",
    "\n",
    "# Differenza assoluta e percentuale\n",
    "missing_diff = missing_after - missing_before\n",
    "increase_percent = (missing_diff / missing_before.replace(0, pd.NA)) * 100\n",
    "\n",
    "# Tabella riepilogativa\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"missing_before\": missing_before,\n",
    "        \"missing_after\": missing_after,\n",
    "        \"difference\": missing_diff,\n",
    "        \"increase_%\": increase_percent\n",
    "    })\n",
    "    .sort_values(\"difference\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- PRENDI TUTTE LE COLONNE, non solo difference > 0 ---\n",
    "plot_df = missing_summary.copy()\n",
    "\n",
    "plot_df = plot_df.reset_index().rename(columns={\"index\": \"feature\"})\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(plot_df)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"difference:Q\", title=\"Variation in number of missing values (after - before)\"),\n",
    "        y=alt.Y(\"feature:N\", sort='-x', title=\"Feature\"),\n",
    "        color=alt.Color(\n",
    "            \"feature:N\",\n",
    "            scale=alt.Scale(range=[\"#f3e5f5\", \"#e1bee7\", \"#ce93d8\", \"#ba68c8\", \"#9c27b0\"][::-1])\n",
    "        ),\n",
    "        tooltip=[\"feature\", \"difference\", \"missing_before\", \"missing_after\", \"increase_%\"]\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=300,\n",
    "        title=\"Missing Values Variation After Merge (All Features)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "chart\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dda0477a8712f0f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After merging the datasets, several artist-related features experienced a significant increase in missing values. This indicates that many records in the merged dataset do not have matching information in the original artists' dataset. Additionally, some features (e.g., active_end, featured_artists, stats_pageviews) show very high percentages of missing data, making them unreliable for analysis. Columns with moderate missing rates (around 20–30%) may still be usable after applying appropriate imputation techniques, while highly incomplete features should be removed or excluded from modeling to avoid introducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f098c2ec3131b16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee07644e677689",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Artists\n",
    "\n",
    "The following code checks the artists dataset for duplicates in two ways: first, it looks for identical full rows to detect any completely repeated entries; then, it checks for duplicates specifically based on the artist ID and artist name columns.\n",
    "<B> After performing both checks, it confirms that there are no duplicate artists in the dataset </B>."
   ]
  },
  {
   "cell_type": "code",
   "id": "c7c9f8c11b6cc149",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check for duplicated artists rows\n",
    "duplicates_artists = artists[artists.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated Artists rows: {duplicates_artists.shape[0]}\")\n",
    "display(duplicates_artists.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db28e64fac608cd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check for duplicated artists based on artist id\n",
    "duplicates_artists_id = artists[artists.duplicated(subset='id_author')]\n",
    "print(f\"Number of duplicated artist based on ID: {duplicates_artists_id.shape[0]}\")\n",
    "display(duplicates_artists_id.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5db7f85f20e8a520",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check for duplicated artists based on artist name\t\n",
    "duplicates_artists_name = artists[artists.duplicated(subset='name')]\n",
    "print(f\"Number of duplicated artist based on Name: {duplicates_artists_name.shape[0]}\")\n",
    "display(duplicates_artists_name.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0b99c5b549cfd66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Tracks\n",
    "Duplicates rows check has been also performed here.\n",
    "No duplicated rows were detected, indicating that all track entries are unique."
   ]
  },
  {
   "cell_type": "code",
   "id": "77ed386472357f2e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check for duplicated tracks rows\n",
    "duplicates_tracks = tracks[tracks.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicated rows: {duplicates_tracks.shape[0]}\")\n",
    "display(duplicates_tracks.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfeebc889859df04",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Duplicated Tracks based on ID\n",
    "\n",
    "This code checks the tracks dataset for duplicates based specifically on the track ID column. It identifies all rows where the same ID appears more than once, counts them, and displays them.\n",
    " It first identifies all rows where the same ID appears more than once, counts how many duplicated tracks exist, and displays them. Then, it counts how many times each track ID occurs in the dataset. \n",
    "\n",
    "<B> The result shows that there are 73 duplicated rows based on track IDs. \n",
    "Precisely we have 71  distinct IDs that have duplicates. </B>\n",
    "\n",
    "<B>one track ID is repeated four times, while the others are each repeated twice </B>"
   ]
  },
  {
   "cell_type": "code",
   "id": "656e70861e43cd57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check for duplicated tracks based on track id\n",
    "duplicates_tracks_id = tracks[tracks.duplicated(subset='id')]\n",
    "print(f\"Number of duplicated Tracks rows based on ID: {duplicates_tracks_id.shape[0]}\")\n",
    "display(duplicates_tracks_id)\n",
    "\n",
    "\n",
    "# Count how many times each id_track appears\n",
    "id_counts = tracks['id'].value_counts()\n",
    "duplicate_id_counts = id_counts[id_counts > 1]\n",
    "\n",
    "print('Number of distinct IDs that have duplicates')\n",
    "print(duplicate_id_counts.size)\n",
    "print(\"Number of tracks for each id:\")\n",
    "print(duplicate_id_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e853ec7dfdde202",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code lists every full_title associated with each duplicated track ID. The results show 71 duplicated IDs in total. Most of these IDs are linked to two different songs, except for one ID that is associated with four songs (two pairs sharing the same title)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9885124a97e982ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Find all duplicated track IDs\n",
    "duplicate_ids = tracks[tracks.duplicated(subset='id', keep=False)]\n",
    "\n",
    "# Group by 'id' and list all titles\n",
    "titles_per_id = duplicate_ids.groupby('id')['full_title'].apply(list)\n",
    "\n",
    "# Display each ID with all titles and the count of unique titles\n",
    "for track_id, titles in titles_per_id.items():\n",
    "    unique_count = len(set(titles))  # number of unique titles\n",
    "    print(f\"Track ID: {track_id} Number(of total songs: {len(titles)})(Unique titles: {unique_count})\")\n",
    "    for title in titles:\n",
    "        print(f\"  - {title}\")\n",
    "    print('----------------------------------------------------------')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff4620fd266760d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Duplicated Tracks based on Title\n",
    "The following code identifies tracks that share the same full_title, meaning duplicate song titles. We found four duplicated tracks, corresponding to two pairs of songs with identical titles."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad7683f83b789f04",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Find duplicated full_title\n",
    "duplicate_titles = tracks[tracks.duplicated(subset='full_title', keep=False)]\n",
    "\n",
    "# Sort by full_title to see them together\n",
    "duplicate_titles = duplicate_titles.sort_values('full_title')\n",
    "\n",
    "print(f\"Tracks with duplicate track based on full_title: {duplicate_titles.shape[0]}\")\n",
    "display(duplicate_titles)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Duplicates Tracks base on Lyrics\n",
    "This code checks the tracks dataset for duplicates based specifically on the lyrics content. As identical lyrics create identical values for all derived numeric features (n_tokens, lexical_density, swear_IT, etc.).\n",
    "\n",
    "<B> The result shows that there are 12 duplicate rows. </B>\n",
    "\n",
    "<B> A deeper inspection of the 23 total rows involved reveals two distinct issues: </B>\n",
    "\n",
    "10 unique sets of lyrics (totaling 20 rows) are logical duplicates. As observed, these include both inter-album duplicates (e.g., 'album' vs 'single' versions) and intra-album variations (e.g., HOT and HOT (Opera) on the same album, or Risatatà RMX and its original).\n",
    "\n",
    "3 rows are identified as duplicates because their lyrics value is NaN.\n",
    "\n",
    "<B> Therefore, the 12 duplicates are composed of 10 text-based logical duplicates and 2 NaN duplicates (from the group of 3). Removing these is essential to prevent skewing all derived textual features and the clustering analysis. </B>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7a5cbe728a3bb9"
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Checking for duplicates based on 'lyrics' content...\")\n",
    "\n",
    "# Count the number of rows that are duplicates based on lyrics\n",
    "lyrics_duplicates_count = df.duplicated(subset=['lyrics']).sum()\n",
    "print(f\"Number of duplicate rows (same 'lyrics'): {lyrics_duplicates_count}\")\n",
    "\n",
    "# Get all rows that have a duplicated lyric to inspect them\n",
    "duplicated_lyrics_rows = df[df.duplicated(subset=['lyrics'], keep=False)]\n",
    "\n",
    "if not duplicated_lyrics_rows.empty:\n",
    "    print(f\"\\nFound {duplicated_lyrics_rows.shape[0]} total rows involved in lyric duplication.\")\n",
    "    print(\"Showing some examples, sorted by lyrics to group them:\")\n",
    "\n",
    "    # Display relevant columns to confirm they are logical duplicates\n",
    "    relevant_cols = [\n",
    "        'full_title', 'primary_artist', 'album_name', 'album_type',\n",
    "        'lyrics', 'n_tokens', 'lexical_density', 'swear_IT'\n",
    "    ]\n",
    "\n",
    "    # Filter for columns that actually exist in the dataframe\n",
    "    display_cols = [col for col in relevant_cols if col in df.columns]\n",
    "\n",
    "    display(\n",
    "        duplicated_lyrics_rows.sort_values('lyrics')[display_cols].head(23)\n",
    "    )\n",
    "else:\n",
    "    print(\"OK: No duplicate lyrics found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c150987681da6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Redundant Features",
   "id": "c422227261580bd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Album and Album name\n",
    "\n",
    "The goal is to understand if the two coloums have the same information. In order to do it we analize:\n",
    "1. quality and consistency of `album` and `album_name`\n",
    "2. the relationship with `id_album`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa703d9026bd9b8c"
  },
  {
   "cell_type": "code",
   "source": [
    "if \"album\" in df.columns and \"album_name\" in df.columns:\n",
    "    same_raw = (df[\"album\"] == df[\"album_name\"])\n",
    "    print(f\"Match (raw): {same_raw.mean():.4f} ({same_raw.sum()} / {len(df)})\")\n",
    "\n",
    "    album_norm = df[\"album\"].astype(str).str.strip().str.lower()\n",
    "    album_name_norm = df[\"album_name\"].astype(str).str.strip().str.lower()\n",
    "    same_norm = (album_norm == album_name_norm)\n",
    "    print(f\"Normalized match: {same_norm.mean():.4f} ({same_norm.sum()} / {len(df)})\")\n",
    "\n",
    "    # Esempi di righe discordanti\n",
    "    diff_mask = (~same_norm) & (~df[\"album\"].isna()) & (~df[\"album_name\"].isna())\n",
    "    df_diff = df.loc[diff_mask, [\"album\", \"album_name\"]].head(30)\n",
    "    print(\"\\nSome examples (normalized):\")\n",
    "    display(df_diff)\n",
    "\n",
    "    # -------------------------\n",
    "    # Altair bar plot con colori custom\n",
    "    # -------------------------\n",
    "    stats = pd.DataFrame({\n",
    "        \"type\": [\n",
    "            \"raw_match\",\n",
    "            \"normalized_match\",\n",
    "            \"normalized_mismatch\"\n",
    "        ],\n",
    "        \"count\": [\n",
    "            int(same_raw.sum()),\n",
    "            int(same_norm.sum()),\n",
    "            int(len(df) - same_norm.sum())\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    chart = (\n",
    "        alt.Chart(stats)\n",
    "        .mark_bar()\n",
    "        .encode(\n",
    "            x=alt.X(\"type:N\", title=\"Comparison type\"),\n",
    "            y=alt.Y(\"count:Q\", title=\"Number of rows\"),\n",
    "            color=alt.Color(\n",
    "                \"type:N\",\n",
    "                title=\"Type\",\n",
    "                scale=alt.Scale(\n",
    "                    domain=[\"raw_match\", \"normalized_match\", \"normalized_mismatch\"],\n",
    "                    range=[\"#ce93d8\", \"#9c27b0\", \"#ba68c8\"]\n",
    "                )\n",
    "            ),\n",
    "            tooltip=[\"type\", \"count\"]\n",
    "        )\n",
    "        .properties(\n",
    "            width=400,\n",
    "            height=300,\n",
    "            title=\"Album vs Album Name - Match overview\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chart.display()\n",
    "\n",
    "else:\n",
    "    print(\"There aren't `album` and `album_name`.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50412e9b17c7fe1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Consistency with id_album\n",
    "\n",
    "For each `album id`, we check: \n",
    "- how many distinct `album name` entries\n",
    "- how many distinct `album` entries\n",
    "This helps us understand which column is the most stable as a textual representation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52f8440a5e6d0bd"
  },
  {
   "cell_type": "code",
   "source": [
    "if \"id_album\" in df.columns:\n",
    "    # --- Distribuzione album_name per id_album ---\n",
    "    if \"album_name\" in df.columns:\n",
    "        album_name_per_id = df.groupby(\"id_album\")[\"album_name\"].nunique()\n",
    "        dist_album_name = (\n",
    "            album_name_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(name=\"n_id_album\")\n",
    "            .rename_axis(\"n_unique_album_name\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_album_name.head(10))\n",
    "\n",
    "    # --- Distribuzione album per id_album ---\n",
    "    if \"album\" in df.columns:\n",
    "        album_per_id = df.groupby(\"id_album\")[\"album\"].nunique()\n",
    "        dist_album = (\n",
    "            album_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(name=\"n_id_album\")\n",
    "            .rename_axis(\"n_unique_album\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_album.head(10))\n",
    "\n",
    "    # --- Casi sospetti: stesso id_album con più nomi diversi ---\n",
    "    if \"album_name\" in df.columns:\n",
    "        suspicious_albumname = album_name_per_id[album_name_per_id > 1]\n",
    "        print(f\"\\n`id_album` with more than one `album_name`: {len(suspicious_albumname)}\")\n",
    "        display(\n",
    "            df[df[\"id_album\"].isin(suspicious_albumname.index)]\n",
    "            [[\"id_album\", \"album_name\"]]\n",
    "            .drop_duplicates()\n",
    "            .head(30)\n",
    "        )\n",
    "\n",
    "    if \"album\" in df.columns:\n",
    "        suspicious_album = album_per_id[album_per_id > 1]\n",
    "        print(f\"\\n`id_album` with more than one `album`: {len(suspicious_album)}\")\n",
    "        display(\n",
    "            df[df[\"id_album\"].isin(suspicious_album.index)]\n",
    "            [[\"id_album\", \"album\"]]\n",
    "            .drop_duplicates()\n",
    "            .head(30)\n",
    "        )\n",
    "else:\n",
    "    print(\"Missing id_album\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd2ef74c6744e29e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After this checks, we found that:\n",
    "- album_name is reliable: almost all album_ids have only one album_name (only 5 duplicates), so you use that as the official album name.\n",
    "\n",
    "- album is dirty: many album_ids have multiple different values ​​(or NaN), so you treat it as the original album_raw and don't use it for keys, groupbys, or deduplications.\n",
    "\n",
    "This information will be taken in consideration in the next sections."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153f027aaf14f8ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Title and Full_title\n",
    "\n",
    "I am going to do the same made with album and album_name.\n",
    "The goal is to understand if the two coloums have the same information. In order to do it we analize:\n",
    "1. quality and consistency of `title` and `full_title`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb17cb4512c2db4d"
  },
  {
   "cell_type": "code",
   "source": [
    "def aggressive_clean(s):\n",
    "    \"\"\"\n",
    "    Normalizza una stringa per un confronto affidabile:\n",
    "    1. Converte in stringa e minuscolo.\n",
    "    2. Rimuove caratteri invisibili (es. zero-width space).\n",
    "    3. Sostituisce spazi non-secabili (NBSP) con spazi normali.\n",
    "    4. Normalizza tutti i tipi di apostrofi (curvi, accenti) in un apostrofo dritto.\n",
    "    5. NUOVO: Normalizza tutti i tipi di virgolette (curve) in virgolette dritte.\n",
    "    6. Collassa tutti i tipi di whitespace (spazi, tab, \\n) in un singolo spazio.\n",
    "    7. Rimuove spazi bianchi all'inizio e alla fine.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return ''\n",
    "    s = str(s).lower()\n",
    "    s = s.replace('\\u200b', '')\n",
    "    s = s.replace(u'\\xa0', ' ')\n",
    "\n",
    "    s = re.sub(r'[\\’\\‘`´]', \"'\", s)\n",
    "\n",
    "    s = re.sub(r'[\\“\\”\\„]', '\"', s)\n",
    "\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "print(\"--- Containment Check: 'full_title' vs. ('title' 'artist' 'featuring') ---\")\n",
    "print(\"Running with APOSTROPHE + QUOTATION MARK NORMALIZATION...\")\n",
    "\n",
    "# --- 2. Definisci la funzione di controllo di contenimento ---\n",
    "def check_containment(row):\n",
    "    \"\"\"\n",
    "    Controlla se le versioni pulite di title, artist e features\n",
    "    sono tutte contenute nella versione pulita di full_title.\n",
    "    \"\"\"\n",
    "    clean_full = aggressive_clean(row['full_title'])\n",
    "    clean_title = aggressive_clean(row['title'])\n",
    "    clean_artist = aggressive_clean(row['name_artist'])\n",
    "    # clean_features = aggressive_clean(row['featured_artists'])\n",
    "\n",
    "    if not clean_full or not clean_title or not clean_artist:\n",
    "        return pd.NA\n",
    "\n",
    "    if clean_title not in clean_full:\n",
    "        return False\n",
    "\n",
    "    if clean_artist not in clean_full:\n",
    "        return False\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "print(\"Applying containment check to all rows...\")\n",
    "containment_mask = df.apply(check_containment, axis=1)\n",
    "\n",
    "valid_checks = containment_mask.dropna()\n",
    "num_mismatches = (valid_checks == False).sum()\n",
    "num_matches = (valid_checks == True).sum()\n",
    "num_skipped = containment_mask.isna().sum()\n",
    "\n",
    "print(f\"\\nTotal rows checked: {len(df)}\")\n",
    "print(f\"Rows skipped (missing title, artist, or full_title): {num_skipped}\")\n",
    "print(f\"Rows where 'full_title' (pulito) CONTAINS all info: {num_matches}\")\n",
    "print(f\"Rows where 'full_title' (pulito) IS MISSING some info: {num_mismatches}\")\n",
    "\n",
    "if num_mismatches > 0:\n",
    "    print(\"\\n--- Inspecting Mismatches (Rows missing info) ---\")\n",
    "    print(\"Queste sono le righe in cui 'full_title' (pulito) NON contiene\")\n",
    "    print(\"il 'title', 'name_artist'  (tutti puliti).\")\n",
    "\n",
    "    mismatch_df = df[containment_mask == False].copy()\n",
    "\n",
    "    mismatch_df['check_full_title'] = mismatch_df['full_title'].apply(aggressive_clean)\n",
    "    mismatch_df['check_title'] = mismatch_df['title'].apply(aggressive_clean)\n",
    "    mismatch_df['check_artist'] = mismatch_df['name_artist'].apply(aggressive_clean)\n",
    "\n",
    "\n",
    "    display(mismatch_df[[\n",
    "        'full_title',\n",
    "        'check_full_title',\n",
    "        'check_title',\n",
    "        'check_artist',\n",
    "    ]].head(20))\n",
    "else:\n",
    "    print(\"\\nSUCCESS: All 'full_title' rows (with valid data) contain the required info.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2a48776238edc04",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Consistency with id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b75bcce6bee911e"
  },
  {
   "cell_type": "code",
   "source": [
    "if \"id\" in df.columns and {\"title\", \"full_title\"}.issubset(df.columns):\n",
    "    title_per_id = df.groupby(\"id\")[\"title\"].nunique()\n",
    "    full_per_id = df.groupby(\"id\")[\"full_title\"].nunique()\n",
    "\n",
    "    display(\n",
    "        title_per_id.value_counts()\n",
    "        .sort_index()\n",
    "        .to_frame(\"n_id\")\n",
    "        .rename_axis(\"n_unique_title\")\n",
    "        .reset_index()\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        full_per_id.value_counts()\n",
    "        .sort_index()\n",
    "        .to_frame(\"n_id\")\n",
    "        .rename_axis(\"n_unique_full_title\")\n",
    "        .reset_index()\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    # id con problemi: più di un title o più di un full_title\n",
    "    bad_id = title_per_id[title_per_id > 1].index.union(\n",
    "        full_per_id[full_per_id > 1].index\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        df[df[\"id\"].isin(bad_id)]\n",
    "        [[\"id\", \"title\", \"full_title\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values([\"id\", \"title\", \"full_title\"])\n",
    "        .head(50)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d63ac61dc45aee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "They track the same information. We will take full_name."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e33882a782420c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Name and Name_artist\n",
    "Objective:\n",
    "- Verify name consistency for each `id_artist`\n",
    "- Determine whether `name_artist` is redundant with `name`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd3082a07ac623e7"
  },
  {
   "cell_type": "code",
   "source": [
    "if \"id_artist\" in df.columns:\n",
    "    # name_artist per id_artist\n",
    "    if \"name_artist\" in df.columns:\n",
    "        na_per_id = df.groupby(\"id_artist\")[\"name_artist\"].nunique()\n",
    "        dist_na = (\n",
    "            na_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(\"n_id_artist\")\n",
    "            .rename_axis(\"n_unique_name_artist\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_na.head(10))\n",
    "\n",
    "    # name per id_artist (dopo merge con artist.csv)\n",
    "    if \"name\" in df.columns:\n",
    "        n_per_id = df.groupby(\"id_artist\")[\"name\"].nunique()\n",
    "        dist_n = (\n",
    "            n_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(\"n_id_artist\")\n",
    "            .rename_axis(\"n_unique_name\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_n.head(10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1cba80493ce0070",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display(df[['name', 'name_artist']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d336262d80e90a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check which one is more stable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "267593499fface6e"
  },
  {
   "cell_type": "code",
   "source": [
    "if \"id_artist\" in df.columns:\n",
    "    # --- Distribuzione `name` per id_artist ---\n",
    "    if \"name\" in df.columns:\n",
    "        name_per_id = df.groupby(\"id_artist\")[\"name\"].nunique()\n",
    "        dist_name = (\n",
    "            name_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(name=\"n_id_artist\")\n",
    "            .rename_axis(\"n_unique_name\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_name.head(10))\n",
    "\n",
    "    # --- Distribuzione `name_artist` per id_artist ---\n",
    "    if \"name_artist\" in df.columns:\n",
    "        name_artist_per_id = df.groupby(\"id_artist\")[\"name_artist\"].nunique()\n",
    "        dist_name_artist = (\n",
    "            name_artist_per_id.value_counts()\n",
    "            .sort_index()\n",
    "            .to_frame(name=\"n_id_artist\")\n",
    "            .rename_axis(\"n_unique_name_artist\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        display(dist_name_artist.head(10))\n",
    "\n",
    "    # --- Casi sospetti: più nomi per lo stesso id_artist ---\n",
    "    if \"name\" in df.columns:\n",
    "        suspicious_name = name_per_id[name_per_id > 1]\n",
    "        print(f\"\\n`id_artist` with more than one `name`: {len(suspicious_name)}\")\n",
    "        display(\n",
    "            df[df[\"id_artist\"].isin(suspicious_name.index)]\n",
    "            [[\"id_artist\", \"name\"]]\n",
    "            .drop_duplicates()\n",
    "            .head(30)\n",
    "        )\n",
    "\n",
    "    if \"name_artist\" in df.columns:\n",
    "        suspicious_name_artist = name_artist_per_id[name_artist_per_id > 1]\n",
    "        print(f\"\\n`id_artist` with more than one `name_artist`: {len(suspicious_name_artist)}\")\n",
    "        display(\n",
    "            df[df[\"id_artist\"].isin(suspicious_name_artist.index)]\n",
    "            [[\"id_artist\", \"name_artist\"]]\n",
    "            .drop_duplicates()\n",
    "            .head(30)\n",
    "        )\n",
    "else:\n",
    "    print(\"Missing id_artist\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66894713e49386b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This to coloums are identical. We will remove artist_name."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9b7c81b49830107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Name vs Primary Artists",
   "id": "4365cb5b512e4b0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize both columns for comparison (temporary variables)\n",
    "name_clean = df['name'].str.lower().str.strip()\n",
    "primary_clean = df['primary_artist'].str.lower().str.strip()\n",
    "\n",
    "# Boolean Series for matches (not added to df)\n",
    "matches = name_clean == primary_clean\n",
    "\n",
    "# Show mismatched rows\n",
    "mismatches = df[~matches][['name', 'primary_artist']]\n",
    "\n",
    "print(f\"Total mismatches: {len(mismatches)}\")\n",
    "display(mismatches)"
   ],
   "id": "fb4cea877b50999a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outliers Detection\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9075adfbb6c5431"
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical Feature Definition\n",
    "\n",
    "# List of key numerical columns to analyze\n",
    "skewed_features = [\n",
    "    'tokens_per_sent', 'avg_token_per_clause', 'duration_ms', 'stats_pageviews', 'swear_EN', 'char_per_tok', 'swear_IT', 'bpm', 'n_sentences', 'n_tokens', 'rolloff', 'zcr', 'lexical_density', 'flatness'\n",
    "]\n",
    "\n",
    "simetric_features =[\n",
    "    'pitch', 'centroid', 'spectral_complexity', 'loudness', 'flux', 'rms'\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60d7e0107d95079",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "I removed the following variables from the list, the statistical analysis of outliers (IQR/Z-Score) is semantically wrong for them: disc_number, track_number, Month, Day"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4cd1a60ddf1081e"
  },
  {
   "cell_type": "code",
   "source": [
    "n_cols = 3\n",
    "n_rows = -(-len(simetric_features) // n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(simetric_features):\n",
    "    sns.boxplot(x=df[col], ax=axes[i], orient='h', color=\"skyblue\")\n",
    "    axes[i].set_title(col, fontsize=14)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Box Plot Analysis for Outlier Detection\", fontsize=20, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d34d7c4a4887869",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All six plots show distributions that appear relatively symmetric. The median line within each box is positioned near the center of the box (the Interquartile Range, or IQR), indicating that the 50th percentile is roughly equidistant from the 25th (Q1) and 75th (Q3) percentiles.\n",
    "\n",
    "Every feature have the presence of candidate outliers.\n",
    "\n",
    "Outlier Distribution:\n",
    "\n",
    "pitch, centroid, flux, and rms all show outliers on both the lower (left) and upper (right) ends of their distributions.\n",
    "\n",
    "spectral_complexity and loudness appear to have outliers almost exclusively on the high end (right side).\n",
    "\n",
    "This visual inspection suggests that while the central tendency of these features is symmetrically distributed, a small number of records possess extremely high and/or low values that fall outside the main data cluster."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2203b729e65be34"
  },
  {
   "cell_type": "code",
   "source": [
    "n_cols = 3\n",
    "n_rows = -(-len(skewed_features) // n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(skewed_features):\n",
    "    sns.boxplot(x=df[col], ax=axes[i], orient='h', color=\"skyblue\")\n",
    "    axes[i].set_title(col, fontsize=14)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Box Plot Analysis for Outlier Detection\", fontsize=20, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c55f9ff91e0fae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **Extreme Skewness:** For many features, particularly `stats_pageviews`, `swear_EN`, `swear_IT`, `tokens_per_sent`, and `avg_token_per_clause`, the main \"box\" is extremely compressed and pushed to the far left. This visually confirms a strong positive skew (right-tailed distribution).\n",
    "\n",
    "* **Vast Number of Outliers:** The most prominent characteristic is the dense cloud of outliers extending far to the right for these positively skewed features. This indicates that a large number of records have values significantly higher than the main cluster of data.\n",
    "\n",
    "* **Specific Cases:**\n",
    "    * **Positively Skewed:** `stats_pageviews` and `swear_EN` are the most extreme examples, where the box is barely visible, and the plot is dominated by a long stream of high-end outliers.\n",
    "    * **Negatively Skewed:** `flatness` is the clear exception, showing the opposite pattern. Its box is compressed to the far right, with a long tail of outliers on the low end (left side), confirming its negative skew.\n",
    "    * **Mixed Outliers:** Features like `duration_ms`, `n_sentences`, and `lexical_density` show outliers on *both* sides, though the high-end outliers are generally more numerous or extreme.\n",
    "\n",
    "The visual analysis shows that these features are heavily skewed and contain a large quantity of extreme values. Simply removing all these outliers would lead to massive data loss."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32dc30cde04600b4"
  },
  {
   "cell_type": "code",
   "source": [
    "# Analisi Statistica: Metodo IQR (per variabili asimmetriche)\n",
    "outlier_data = []\n",
    "\n",
    "for col in skewed_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    low_outliers = df[df[col] < lower_bound]\n",
    "    high_outliers = df[df[col] > upper_bound]\n",
    "\n",
    "    outlier_data.append({\n",
    "        'variable': col,\n",
    "        'High Outliers (Above)': len(high_outliers),\n",
    "        'Low Outliers (Below)': len(low_outliers)\n",
    "    })\n",
    "\n",
    "    print(f\"Variable '{col}':\")\n",
    "    print(f\"  IQR Limits: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  Found {len(low_outliers)} outliers below the limit.\")\n",
    "    print(f\"  Found {len(high_outliers)} outliers above the limit.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea63cd91263808bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if outlier_data:\n",
    "    df_outliers = pd.DataFrame(outlier_data)\n",
    "\n",
    "    # Calculate the total outliers for sorting\n",
    "    df_outliers['total_outliers'] = df_outliers['High Outliers (Above)'] + df_outliers['Low Outliers (Below)']\n",
    "\n",
    "    df_outliers = df_outliers.sort_values(by='total_outliers', ascending=False)\n",
    "\n",
    "    sort_order = df_outliers['variable'].tolist()\n",
    "\n",
    "    df_melted = df_outliers.melt(\n",
    "        id_vars=['variable', 'total_outliers'],\n",
    "        value_vars=['High Outliers (Above)', 'Low Outliers (Below)'],\n",
    "        var_name='Outlier Type',\n",
    "        value_name='Number of Outliers'\n",
    "    )\n",
    "\n",
    "    domain_ = [\"High Outliers (Above)\", \"Low Outliers (Below)\"]\n",
    "    range_ = [\"#d36ba8\", \"#8cbcd9\"]\n",
    "\n",
    "    base = alt.Chart(df_melted).mark_bar().encode(\n",
    "        y=alt.Y('Number of Outliers', title='Number of Outliers Detected'),\n",
    "\n",
    "        color=alt.Color('Outlier Type',\n",
    "                        legend=alt.Legend(title='Outlier Type'),\n",
    "                        scale=alt.Scale(domain=domain_, range=range_)),\n",
    "\n",
    "        tooltip=['variable', 'Outlier Type', 'Number of Outliers']\n",
    "\n",
    "    ).properties(\n",
    "        title='Outlier Count (IQR Method) for Skewed Variables'\n",
    "    )\n",
    "\n",
    "    chart = base.encode(\n",
    "        x=alt.X('Outlier Type', axis=None),\n",
    "        column=alt.Column('variable',\n",
    "                          sort=sort_order,\n",
    "                          header=alt.Header(\n",
    "                              title='Variable',\n",
    "                              titleOrient=\"bottom\",\n",
    "                              labelOrient=\"bottom\",\n",
    "                              labelAngle=-45,\n",
    "                              labelAlign='right',\n",
    "                              labelBaseline='middle',\n",
    "                              labelPadding=5\n",
    "                          ))\n",
    "    ).interactive()\n",
    "\n",
    "    display(chart)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1072529c9b233dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Analisi Statistica: Metodo Z-Score (per variabili simmetriche)\n",
    "outlier_data_z = []\n",
    "z_threshold = 3\n",
    "\n",
    "for col in simetric_features:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "\n",
    "    lower_bound_z = mean - (z_threshold * std)\n",
    "    upper_bound_z = mean + (z_threshold * std)\n",
    "\n",
    "    low_outliers_z = df[df[col] < lower_bound_z]\n",
    "    high_outliers_z = df[df[col] > upper_bound_z]\n",
    "\n",
    "    outlier_data_z.append({\n",
    "        'variable': col,\n",
    "        'High Outliers (Above)': len(high_outliers_z),\n",
    "        'Low Outliers (Below)': len(low_outliers_z)\n",
    "    })\n",
    "\n",
    "    print(f\"Variable '{col}':\")\n",
    "    print(f\"  Z-Score Limits (threshold={z_threshold}): [{lower_bound_z:.2f}, {upper_bound_z:.2f}]\")\n",
    "    print(f\"  Found {len(low_outliers_z)} outliers below the limit.\")\n",
    "    print(f\"  Found {len(high_outliers_z)} outliers above the limit.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f73d72c2996cb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The IQR statistical analysis confirms that **all asymmetric features contain a significant number of outliers**, as clearly visualized in the summary bar chart.\n",
    "\n",
    "* **Massive Outlier Counts:** The most striking case is `swear_EN`, with 2,740 high-end outliers. This is a statistical artifact: its IQR is `[0.00, 0.00]`, meaning any track with even one English swear word is flagged. `swear_IT` (746), `avg_token_per_clause` (605), and `flatness` (507) also show a very high volume of outliers, making simple removal impossible.\n",
    "\n",
    "* **One-Sided Distributions:**\n",
    "    * `stats_pageviews`, `swear_EN`, and `swear_IT` only have **high-end outliers**. This perfectly matches their positive skew (right-tail) and identifies \"hit songs\" or lyrically extreme tracks.\n",
    "    * `flatness` is the only feature with exclusively **low-end outliers**, confirming its negative skew (left-tail).\n",
    "\n",
    "* **Two-Sided Distributions:** Most features, including `duration_ms`, `n_sentences`, `lexical_density`, and `char_per_tok`, show a significant number of outliers on **both sides**. This implies the presence of errors or extreme values at both ends (e.g., for `duration_ms`, this likely includes both \"skit\" tracks and very long songs).\n",
    "\n",
    "* **Isolated Outliers:** `bpm` is a special case, showing **only one high-end outlier**. This is almost certainly an data-entry error (e.g., `bpm = 900`) and will be simple to inspect and correct."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de7d7eac6e050b6d"
  },
  {
   "cell_type": "code",
   "source": [
    "# Converti la lista in un DataFrame\n",
    "if outlier_data_z:\n",
    "    df_outliers_z = pd.DataFrame(outlier_data_z)\n",
    "\n",
    "    df_outliers_z['total_outliers'] = df_outliers_z['High Outliers (Above)'] + df_outliers_z['Low Outliers (Below)']\n",
    "\n",
    "    df_outliers_z = df_outliers_z.sort_values(by='total_outliers', ascending=False)\n",
    "\n",
    "    sort_order_z = df_outliers_z['variable'].tolist()\n",
    "\n",
    "    df_melted_z = df_outliers_z.melt(\n",
    "        id_vars=['variable', 'total_outliers'],\n",
    "        value_vars=['High Outliers (Above)', 'Low Outliers (Below)'],\n",
    "        var_name='Outlier Type',\n",
    "        value_name='Number of Outliers'\n",
    "    )\n",
    "\n",
    "    domain_z = [\"High Outliers (Above)\", \"Low Outliers (Below)\"]\n",
    "    range_z = [\"#d36ba8\", \"#8cbcd9\"]\n",
    "\n",
    "    base_z = alt.Chart(df_melted_z).mark_bar().encode(\n",
    "        y=alt.Y('Number of Outliers', title='Number of Outliers Detected'),\n",
    "        color=alt.Color('Outlier Type',\n",
    "                        legend=alt.Legend(title='Outlier Type'),\n",
    "                        scale=alt.Scale(domain=domain_z, range=range_z)),\n",
    "        tooltip=['variable', 'Outlier Type', 'Number of Outliers']\n",
    "    ).properties(\n",
    "        title='Outlier Count (Z-Score Method) for Symmetric Variables'\n",
    "    )\n",
    "\n",
    "    chart_z = base_z.encode(\n",
    "        x=alt.X('Outlier Type', axis=None),\n",
    "        column=alt.Column('variable',\n",
    "                          sort=sort_order_z,\n",
    "                          header=alt.Header(\n",
    "                              title='Variable',\n",
    "                              titleOrient=\"bottom\",\n",
    "                              labelOrient=\"bottom\",\n",
    "                              labelAngle=-45,\n",
    "                              labelAlign='right',\n",
    "                              labelBaseline='middle',\n",
    "                              labelPadding=5\n",
    "                          ))\n",
    "    ).interactive()\n",
    "\n",
    "    display(chart_z)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "673a4a866d3c001f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **Quantification of Outliers:** The analysis provides precise counts for these extreme values. `pitch` emerges as the feature with the most outliers (68 total), while `spectral_complexity` has the fewest (22). `centroid` (57) and `flux` (49) also show a notable number of outliers.\n",
    "\n",
    "* **Distribution of Outliers:**\n",
    "    * For most features (`pitch`, `centroid`, `spectral_complexity`, `flux`, and `rms`), the outliers are **distributed on both the high and low ends**. This indicates that there are tracks with values that are exceptionally high *and* exceptionally low for these audio characteristics.\n",
    "    * The most significant exception is `loudness`, which has 27 identified outliers, all of which are **exclusively on the high side** (above the 50.17 limit). This statistical result perfectly matches its box plot, which only showed an upper tail.\n",
    "\n",
    "This analysis confirms that while these features are symmetrically distributed, a small number of extreme values are present. These outliers are likely real but rare data points (e.g., unusually loud or high-pitched songs) that must be handled before clustering to prevent them from skewing the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f8e482fd92eccb"
  },
  {
   "cell_type": "markdown",
   "id": "719e2c00accc699a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Features Inspection\n",
    "\n",
    "In this phase, we inspect each feature to identify values that fall outside the expected domain, violate logical constraints, or appear inconsistent with the semantics of the variable. These anomalies may indicate data entry errors, corrupted records, incorrect parsing, or values that require transformation before further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b29a7d09a7219c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Artists Names\n",
    "This code groups the dataset by artist name to count how many songs each artist has, then displays the total number of unique artists and sorts them by song count. The analysis shows that there are 104 unique artists in the dataset. Among them, Mondo Marcio, Guè Pequeno, and Gemitaiz are the most prolific, each with over 300 songs. Other highly represented artists include Bassi Maestro, Fabri Fibra, and Vacca, each contributing more than 250 songs. On the other hand, a few artists such as O Zulù, Joey Funboy, and Hindaco have only a handful of tracks. Overall, the distribution highlights a few artists dominating the dataset while many others have significantly fewer songs."
   ]
  },
  {
   "cell_type": "code",
   "id": "3de46a44dfef028a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(df.shape)\n",
    "\n",
    "# 1. Conteggio brani per artista\n",
    "artist_song_counts = df.groupby('name').size()\n",
    "\n",
    "# 2. Series -> DataFrame\n",
    "artist_counts_df = (\n",
    "    artist_song_counts\n",
    "    .reset_index(name='song_count')\n",
    "    .sort_values(by='song_count', ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal number of unique artists: {len(artist_counts_df)}\")\n",
    "print(\"Unique Artists and Their Song Count:\")\n",
    "print(artist_counts_df)\n",
    "\n",
    "# 3. Bar chart orizzontale in Altair\n",
    "bar = alt.Chart(artist_counts_df).mark_bar().encode(\n",
    "    y=alt.Y('name:N',\n",
    "            sort='-x',\n",
    "            title='Artist'),\n",
    "    x=alt.X('song_count:Q',\n",
    "            title='Number of Songs'),\n",
    "    color=alt.Color(\n",
    "        'song_count:Q',\n",
    "        scale=alt.Scale(range=[\"#e1bee7\", \"#ce93d8\", \"#ba68c8\", \"#9c27b0\"]),\n",
    "        legend=None\n",
    "    ),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('name:N', title='Artist'),\n",
    "        alt.Tooltip('song_count:Q', title='Number of Songs')\n",
    "    ]\n",
    ").properties(\n",
    "    title='Number of Songs per Artist',\n",
    "    width=600,\n",
    "    height={'step': 16}  # altezza dinamica in base al numero di artisti\n",
    ")\n",
    "\n",
    "# 4. Etichette a destra di ogni barra\n",
    "text = bar.mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3\n",
    ").encode(\n",
    "    text='song_count:Q'\n",
    ")\n",
    "\n",
    "(bar + text)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Unique artist names in alphabetical order",
   "id": "59849a4b3440b46f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming your DataFrame is called df and the artist column is 'name'\n",
    "unique_artists = sorted(df['name'].dropna().unique())\n",
    "artists_unique=[]\n",
    "# Display the results\n",
    "for artist in unique_artists:\n",
    "    artists_unique.append(artist)\n",
    "dtale .show(artists_unique)"
   ],
   "id": "ad22f25889210ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking  whether name_artist and name are the same\n",
    "\n",
    "The values in the two columns generally match, referring to the same artist, but the name column provides a clearer and more standardized version of the artist’s name (e.g., “M¥SS KETA” instead of “miss keta”). Since both represent the same entity but name is formatted more accurately, we will retain the name column and drop name_artist for clarity and consistency."
   ],
   "id": "424f739a8f7024dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a new column that checks if they match (case-insensitive and stripped)\n",
    "df[\"same_artist_name\"] = (\n",
    "    df[\"name_artist\"].str.strip().str.lower() == df[\"name\"].str.strip().str.lower()\n",
    ")\n",
    "\n",
    "# See mismatches\n",
    "mismatched = df[~df[\"same_artist_name\"]]\n",
    "\n",
    "print(\"Mismatched rows:\")\n",
    "dtale.show(mismatched[[\"name_artist\", \"name\"]])\n"
   ],
   "id": "c293173ee12d02bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Artists' Gender",
   "id": "c1f980572f5c8ea3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Gender distribution ---\n",
    "gender_counts = df['gender'].value_counts(dropna=False)\n",
    "gender_percent = (gender_counts / gender_counts.sum()) * 100\n",
    "\n",
    "# --- Create bar chart ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(gender_counts.index.astype(str), gender_counts.values, color='skyblue')\n",
    "\n",
    "plt.title('Artist Gender Distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Artists')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# --- Add percentage labels on each bar ---\n",
    "for bar, pct in zip(bars, gender_percent):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,  # X position (center)\n",
    "        bar.get_height(),                   # Y position (top of bar)\n",
    "        f\"{pct:.1f}%\",                      # Label text\n",
    "        ha='center', va='bottom', fontsize=10, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Table of each artist with their gender ---\n",
    "artist_gender_table = df[['name', 'gender']].drop_duplicates().sort_values(by='name')\n",
    "dtale.show(artist_gender_table)\n",
    "\n",
    "print(df['gender'].value_counts())"
   ],
   "id": "4fd72b9082fdce19",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb4043d36e8b525b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Artists Description\n",
    "The folowing code counts how many times each unique description appears in the dataset. This helps identify which artist descriptions are the most common or repeated, showing patterns such as groups of artists sharing the same description or potential duplicates.\n",
    "\n",
    "The results show how the most frequent artist descriptions in the dataset. Most entries describe Italian rappers, producers, or singer-songwriters, reflecting that the dataset mainly focuses on Italian music artists.\n",
    "\n",
    "For instance, “gruppo musicale italiano” (Italian music group) appears 620 times, making it the most common description.\n",
    "\n",
    "Interestingly, there are also some non-musical or unrelated entries, like “dio indiano della distruzione e della trasformazione” (Indian god of destruction and transformation) or “tipo di barca a vela usata nel XVIII e XIX secolo” (type of sailing ship used in the 18th–19th century). These seem to be data errors.\n",
    "\n",
    "We also noticed an entry labeled “gruppo musicale canadese” (Canadian music group). Upon checking, this description is incorrectly assigned to the Italian rapper Priestess. Further research revealed a mix-up with a Canadian band that shares the same name. This confusion becomes evident when comparing the active_start year in the dataset, which matches that of the Canadian group rather than the Italian artist."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b662f080157746c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df['description'].value_counts()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb358f022a6c0416",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Identifying groups in the dataset\n",
    "\n",
    "This filter identifies all artists whose description includes the word \"gruppo\", which typically refers to musical groups or bands. The resulting list contains 7 well-known Italian music groups, such as 99 Posse, Articolo 31, Club Dogo, Colle Der Fomento, Cor Veleno, Dark Polo Gang, and Sottotono. There is an outlier group, it is canadian. "
   ]
  },
  {
   "cell_type": "code",
   "id": "29bb55f6b4d49d47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Filter rows where 'description' contains 'gruppo'\n",
    "artists_with_gruppo = (\n",
    "    df[df['description'].str.contains('gruppo', case=False, na=False)]\n",
    "[['name','description','birth_date','active_start']]\n",
    "    .drop_duplicates(subset=['name'])\n",
    "    .sort_values(by='name')\n",
    ")\n",
    "\n",
    "print(\"Artists with 'grupoo' in their description:\",artists_with_gruppo.shape )\n",
    "display(artists_with_gruppo)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fbe308caca5bc40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Artist's BirthDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30805de",
   "metadata": {},
   "source": [
    "##### Birthdate Column Type\n",
    "The column is object. We are going to  to see the values that cannot be converted to DateTime directly.\n",
    "Converting to DateTime is needed to inspect the values easier\n",
    "\n",
    "Based on the values that could not be converted to datetime, we found that the birth_date column contained several invalid entries, such as URLs (e.g., \"http://www.wikidata.org/.well-known/genid/...\") instead of actual dates. Since these values do not represent meaningful or recoverable information, there is nothing worth preserving. Therefore, we are going to apply the pd.to_datetime(errors='coerce') function directly, allowing all invalid entries to be converted to NaT."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b4807fe",
   "metadata": {},
   "source": [
    "date_cols = [ 'birth_date']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "# coverting to dateTime   \n",
    "date_cols = ['birth_date',  ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "\n",
    "df.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c914d1da5df2502",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Distribution of artist birth_date\n",
    "\n",
    "This code extracts each artist’s birth year and groups them by decade (e.g., 1960s, 1970s, 1980s, etc.) to analyze how artists are distributed over time. It calculates the percentage of artists born in each decade and visualizes it with a bar chart. The results show that most artists were born between the 1980s and 1990s, indicating that the majority belong to the Millennial generation, while fewer artists were born in the 1960s or after 2000.\n",
    "\n",
    "It also shows the histogram of the birth year"
   ]
  },
  {
   "cell_type": "code",
   "id": "801ba539d4ab5580",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Count missing values in 'birthdate' column\n",
    "missing_birthdates = df['birth_date'].isna().sum()\n",
    "\n",
    "print(f\"Number of missing values in 'birthdate' column: {missing_birthdates}\")\n",
    "\n",
    "functions.plot_birth_decades(df, \"Distribution of Artists' Birth Years  before Cleaning\",'Percentage of Unique Artists by Decade of Birth before cleaning')\n",
    "\n",
    "# Confirm df is unchanged\n",
    "print(df.columns)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "423258baf165a727",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Distribution of artist ages\n",
    "The results show the distribution of unique artists’ ages in the dataset, ranging from 22 to 58 years old. Most artists fall between their late 20s and mid-40s, with small peaks around ages 32, 36, and 46, each having between 6 and 7 artists. Younger artists under 25 and older ones above 50 are less represented. Overall, the majority of artists are in their thirties and early forties, reflecting the typical active and productive age range in the music industry."
   ]
  },
  {
   "cell_type": "code",
   "id": "467308b3ded546b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "functions.plot_artist_ages(df,\"Number of Unique Artists by Age (Before Cleaning)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b43daa9ef79712fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Artists with no birthdate\n",
    "This code shows the number of the artist that doesn't have a birthdate. They are 32."
   ]
  },
  {
   "cell_type": "code",
   "id": "e5da718f536352fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "missing_birth_artists = df[df['birth_date'].isna()]['name'].drop_duplicates()\n",
    "print(f\"Number of artists with missing birth date: {missing_birth_artists.shape[0]}\")\n",
    "\n",
    "print(\"Artists with missing birth date:\")\n",
    "print(missing_birth_artists.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b38229d1",
   "metadata": {},
   "source": [
    "##### Artists Names and their ages\n",
    "This code generates a table showing each unique artist and their corresponding age, calculated from their birth date. Artists are listed from oldest to youngest, highlighting ages from 23 to 58 in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "id": "fe230a39",
   "metadata": {},
   "source": [
    "\n",
    "# --- Calculate age without adding column to df ---\n",
    "import pandas as pd\n",
    "\n",
    "def display_artist_ages(df, title=\"Unique Artists and Their Age\"):\n",
    "    \"\"\"\n",
    "    Display a table of unique artists and their ages.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least 'name' and 'birth_date' columns.\n",
    "        title (str): Custom title for the output.\n",
    "    \"\"\"\n",
    "    # --- Compute current age in years ---\n",
    "    today = pd.Timestamp.today()\n",
    "    artist_age = (today - df['birth_date']).dt.days // 365\n",
    "\n",
    "    # --- Create temporary DataFrame with unique artists and their age ---\n",
    "    artist_age_df = pd.DataFrame({\n",
    "        'name': df['name'],\n",
    "        'age': artist_age\n",
    "    }).drop_duplicates().sort_values(by='age', ascending=False)\n",
    "\n",
    "    # --- Display the table ---\n",
    "    print(title + \":\")\n",
    "    display(artist_age_df.style.background_gradient(cmap='coolwarm'))\n",
    "\n",
    "    # --- Total number of unique artists ---\n",
    "    print(f\"\\nTotal number of unique artists: {artist_age_df['name'].nunique()}\")\n",
    "\n",
    "display_artist_ages(df, title=\"Artists Age Overview Before filling null Values and Cleaning\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cf49c20b8ce8327",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Active start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef3b53",
   "metadata": {},
   "source": [
    "##### Active_Start DataType Column\n",
    "The column is object. We are going to  to see the values that cannot be converted to DateTime directly.\n",
    "Converting to DateTime is needed to inspect the values easier\n",
    "\n",
    "We realized that all non-missing entries are  already in a valid date format, so they are going to be  successfully converted to datetime without any issues."
   ]
  },
  {
   "cell_type": "code",
   "id": "0060cba2",
   "metadata": {},
   "source": [
    "date_cols = [ 'active_start']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "#Converting to dateTime\n",
    "date_cols = [ 'active_start', ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "\n",
    "df.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71bc780d3dc52dd2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Percentage of Artists by Active Start Decade\n",
    "The dataset contains 4,601 missing values in the active_start column, meaning a significant number of artists have no recorded career start date. Considering only unique artists, the distribution across decades shows that the 1990s (32%), 2000s (30%), and 2010s (32%) were the most common periods for artists to begin their careers, indicating a fairly even spread among these decades. Earlier decades like the 1980s (4%) and the 2020s (2%) have much fewer entries, likely reflecting fewer documented artists or incomplete data for those periods."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbdada3bbae0d61b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(f\"Number of missing values in 'active_start': {df['active_start'].isna().sum()}\")\n",
    "functions.plot_active_start_decades(df,'Percentage of Unique Artists by Active Start Decade before Cleaning')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "736986c0018f0bd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Ages of artists when they started their career\n",
    "The distribution of ages of unique artists when they started their careers shows that most began between 17 and 22 years old, which is reasonable. However, there are outliers, such as one artist listed as starting at age 1 and another at age 10, which clearly do not make sense. These anomalous values indicate potential data errors, and we need to investigate these specific cases to determine the best way to clean or correct the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "3560cda07c5a88a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "functions.plot_age_at_career_start(df,'Age of unique Artists When They Started Their Career Before Cleaning')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5462f04cd7a2311",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Ckecking artist whose age was 10, 13, 27 when they started their career\n",
    "\n",
    "Among the unique artists, several had unusual ages at career start.  Nesli (age 10) had incorrect active_start date, while Salmo (age 13) and Mudimbi (age 27) were correct. "
   ]
  },
  {
   "cell_type": "code",
   "id": "771b0f5968efbad8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# lavoriamo su una copia per sicurezza\n",
    "tmp = df.copy()\n",
    "\n",
    "# assicuriamoci che siano datetime\n",
    "tmp['birth_date'] = pd.to_datetime(tmp['birth_date'], errors='coerce')\n",
    "tmp['active_start'] = pd.to_datetime(tmp['active_start'], errors='coerce')\n",
    "\n",
    "# calcola age at career start (in anni)\n",
    "ages = tmp['active_start'].dt.year - tmp['birth_date'].dt.year\n",
    "\n",
    "# filtra righe con età specifiche\n",
    "mask = ages.isin([ 10, 13, 27])\n",
    "outliers = tmp[mask].copy()\n",
    "\n",
    "# tieni solo artisti unici per nome\n",
    "unique_outliers = outliers.drop_duplicates(subset=['name']).copy()\n",
    "\n",
    "# ricalcola l'età solo su questi (così è allineata)\n",
    "unique_outliers['age_at_start'] = (\n",
    "    unique_outliers['active_start'].dt.year\n",
    "    - unique_outliers['birth_date'].dt.year\n",
    ")\n",
    "\n",
    "print(\"Unique artists with age  10, 13, 27 at career start:\")\n",
    "print(unique_outliers[['name', 'birth_date', 'active_start', 'age_at_start']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76d99a7d9d355b4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Artists with no active start date\n",
    "This code shows the number of the artist that doesn't have an active start date. They are 54 out 104."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9280568232360fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# --- Filter rows where 'active_start' is missing ---\n",
    "missing_active_start = df[df['active_start'].isna()]\n",
    "\n",
    "# --- Compute the earliest full release date (from year, month, day) per artist ---\n",
    "release_dates = pd.to_datetime(df[['year', 'month', 'day']], errors='coerce')\n",
    "album_dates = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    "# Group by artist and get earliest song date and earliest album release date\n",
    "earliest_dates = (\n",
    "    df.assign(_release_date=release_dates, _album_date=album_dates)\n",
    "      .groupby('name', as_index=False)\n",
    "      .agg({'_release_date': 'min', '_album_date': 'min'})\n",
    "      .rename(columns={'_release_date': 'earliest_song_date', '_album_date': 'earliest_album_date'})\n",
    ")\n",
    "\n",
    "# --- Merge with artists missing 'active_start' ---\n",
    "artists_missing_active = (\n",
    "    missing_active_start[['name', 'active_start', 'birth_date']]\n",
    "    .drop_duplicates()\n",
    "    .merge(earliest_dates, on='name', how='left')\n",
    "    .sort_values(by='name')\n",
    ")\n",
    "\n",
    "# --- Print the result ---\n",
    "print(\"Artists without 'active_start' information :\")\n",
    "print(artists_missing_active.to_string(index=False))\n",
    "\n",
    "# --- Optional count ---\n",
    "print(f\"\\nTotal number of unique artists missing 'active_start': {artists_missing_active['name'].nunique()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8cd67973a9552900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  Albums and Album Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919851b",
   "metadata": {},
   "source": [
    "##### Album release Date DataType Column\n",
    "\n",
    "The column is object. We are going to  to see the values that cannot be converted to DateTime directly.\n",
    "Converting to DateTime is needed to inspect the values easier\n",
    "\n",
    "We realized that all non-missing entries are  already in a valid date format, so they are going to be  successfully converted to datetime without any issues.\n",
    "\n",
    "Looking at the values in the album_release_date column that could not be converted to datetime, we noticed that many of them were just years (e.g., \"2004\"). If we used pd.to_datetime(errors='coerce') directly, these entries would have been turned into NaT. However, we wanted to keep this information by assigning a default month and day — the first day of the year.\n",
    "\n",
    "- Instead of converting the column directly, we applied a cleaning function that:\n",
    "\n",
    "- Detected values that were only a year (e.g., \"2004\") and changed them to a full date (\"2004-01-01\").\n",
    "\n",
    "- Kept valid full dates (e.g., \"2021-04-09\") unchanged.\n",
    "\n",
    "- Left missing values as they are.\n",
    "\n",
    "- Finally, converted everything into proper datetime format for consistency."
   ]
  },
  {
   "cell_type": "code",
   "id": "81741882",
   "metadata": {},
   "source": [
    "date_cols = ['album_release_date']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "# Converting to DateTime\n",
    "def fix_year_only_dates(val):\n",
    "    \"\"\"\n",
    "    If the value looks like a 4-digit year, convert it to 'YYYY-01-01'.\n",
    "    Otherwise, return the original value.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val_str = str(val).strip()\n",
    "    if re.fullmatch(r'\\d{4}', val_str):\n",
    "        return f\"{val_str}-01-01\"\n",
    "    return val_str\n",
    "\n",
    "# Apply to album_release_date\n",
    "df['album_release_date'] = df['album_release_date'].apply(fix_year_only_dates)\n",
    "\n",
    "# Convert to datetime\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4aff239d79e477f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Unique albums with their release dates"
   ]
  },
  {
   "cell_type": "code",
   "id": "36173704",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def show_unique_albums(df, album_col='album_name', artist_col='name', date_col='album_release_date'):\n",
    "    \"\"\"\n",
    "    Displays all unique albums with their artist and release date using D-Tale.\n",
    "    Does not modify the original dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - album_col: column containing album names\n",
    "    - artist_col: column containing artist names\n",
    "    - date_col: column containing album release dates\n",
    "    \n",
    "    Returns:\n",
    "    - unique_albums: DataFrame with unique albums and their details\n",
    "    \"\"\"\n",
    "    # --- Extract unique albums ---\n",
    "    unique_albums = df.drop_duplicates(subset=[album_col])[['id_album',album_col, artist_col, date_col,'album_type','album_image','disc_number']]\n",
    "    \n",
    "    # --- Print summary ---\n",
    "    print(f\"Number of unique albums: {len(unique_albums)}\")\n",
    "    \n",
    "    # --- Show in D-Tale ---\n",
    "    return dtale.show(unique_albums)\n",
    "\n",
    "show_unique_albums(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01a4ead4",
   "metadata": {},
   "source": [
    "##### Count Unique albums per artist"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3c9390d",
   "metadata": {},
   "source": [
    "\n",
    "def plot_albums_per_artist(df, top_n=200, artist_col='name', album_col='album_name'):\n",
    "    \"\"\"\n",
    "    Displays and plots how many unique albums each artist has.\n",
    "    Does not modify the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - top_n: number of top artists to display in the plot\n",
    "    - artist_col: column containing artist names\n",
    "    - album_col: column containing album names\n",
    "    \"\"\"\n",
    "    # --- Count unique albums per artist ---\n",
    "    artist_album_counts = (\n",
    "        df.drop_duplicates(subset=[artist_col, album_col])\n",
    "          .groupby(artist_col)[album_col]\n",
    "          .count()\n",
    "          .reset_index(name='album_count')\n",
    "          .sort_values(by='album_count', ascending=False)\n",
    "    )\n",
    "\n",
    "    # --- Display summary ---\n",
    "    print(f\"Total unique artists: {artist_album_counts.shape[0]}\")\n",
    "    print(\"Artists by number of albums:\")\n",
    "    print(artist_album_counts.head(top_n))\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    top_artists = artist_album_counts.head(top_n)\n",
    "    sns.barplot(\n",
    "        data=top_artists,\n",
    "        y=artist_col,\n",
    "        x='album_count',\n",
    "    \n",
    "    )\n",
    "\n",
    "    plt.title(\"Artists by Number of Unique Albums\", fontsize=16, pad=15)\n",
    "    plt.xlabel(\"Number of Albums\", fontsize=12)\n",
    "    plt.ylabel(\"Artist\", fontsize=12)\n",
    "\n",
    "    # --- Add labels ---\n",
    "    for i, val in enumerate(top_artists['album_count']):\n",
    "        plt.text(val + 0.1, i, str(val), va='center', fontsize=9)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return artist_album_counts\n",
    "\n",
    "artist_album_counts = plot_albums_per_artist(df, )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "867b4ae7",
   "metadata": {},
   "source": [
    "##### Album release decade distribution (unique albums only)"
   ]
  },
  {
   "cell_type": "code",
   "id": "40b2a5df",
   "metadata": {},
   "source": [
    "def plot_unique_album_release_distribution(df, album_col='album_name', date_col='album_release_date',\n",
    "                                           title=\"Percentage of Unique Albums by Release Decade\"):\n",
    "    \"\"\"\n",
    "    Plots the percentage of unique albums by their release decade (bar plot)\n",
    "    and a histogram of unique album release years.\n",
    "    Does not modify the original dataset.\n",
    "    \"\"\"\n",
    "    # --- Keep only unique albums ---\n",
    "    unique_albums = df.drop_duplicates(subset=[album_col]).copy()\n",
    "\n",
    "    # --- Extract album release years ---\n",
    "    album_years = pd.to_datetime(unique_albums[date_col], errors='coerce').dt.year.dropna()\n",
    "\n",
    "    if album_years.empty:\n",
    "        print(f\"No valid album release years found in column '{date_col}'.\")\n",
    "        return\n",
    "\n",
    "    # --- Define decade bins ---\n",
    "    start = int(album_years.min() // 10 * 10)\n",
    "    end = int(album_years.max() // 10 * 10 + 10)\n",
    "    bins = list(range(start, end + 10, 10))\n",
    "    labels = [f\"{b}s\" for b in bins[:-1]]\n",
    "\n",
    "    # --- Group by decade ---\n",
    "    decade_groups = pd.cut(album_years, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # --- Calculate percentages ---\n",
    "    group_percent = decade_groups.value_counts(normalize=True).sort_index() * 100\n",
    "    group_df = pd.DataFrame({'decade': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "    print(title)\n",
    "    print(group_df)\n",
    "\n",
    "    # --- Plot bar chart and histogram side by side ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- Bar plot ---\n",
    "    sns.barplot(data=group_df, x='decade', y='percent', hue='decade',\n",
    "                palette='mako', legend=False, ax=axes[0])\n",
    "    for i, val in enumerate(group_df['percent']):\n",
    "        axes[0].text(i, val + 0.5, f\"{val:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "    axes[0].set_title(title, fontsize=16, pad=15)\n",
    "    axes[0].set_xlabel(\"Album Release Decade\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "    sns.despine(ax=axes[0])\n",
    "\n",
    "    # --- Histogram of release years ---\n",
    "    sns.histplot(album_years, bins=20, kde=True, color='skyblue', ax=axes[1])\n",
    "    axes[1].set_title(\"Distribution of Unique Album Release Years\", fontsize=16, pad=15)\n",
    "    axes[1].set_xlabel(\"Release Year\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Count\", fontsize=12)\n",
    "    sns.despine(ax=axes[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_unique_album_release_distribution(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9073cbd7",
   "metadata": {},
   "source": [
    "##### Album release date Describtion"
   ]
  },
  {
   "cell_type": "code",
   "id": "7df6db3b",
   "metadata": {},
   "source": [
    "print(df['album_release_date'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48176cc5",
   "metadata": {},
   "source": [
    "##### Number of Tracks per Album"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd07d7dd",
   "metadata": {},
   "source": [
    "\n",
    "def plot_tracks_per_album(df, album_col='album_name', title=\"Tracks per top 20 Unique Album \"):\n",
    "    \"\"\"\n",
    "    Counts how many tracks exist for each unique album and plots:\n",
    "    \n",
    "    Bar chart of track counts per album\n",
    "    \"\"\"\n",
    "    # --- Drop missing album names ---\n",
    "    df_valid = df.dropna(subset=[album_col]).copy()\n",
    "\n",
    "    # --- Count tracks per album ---\n",
    "    track_counts = df_valid[album_col].value_counts().sort_values(ascending=False)\n",
    "    track_df = track_counts.reset_index()\n",
    "    track_df.columns = [album_col, 'track_count']\n",
    "\n",
    "    # --- Print summary ---\n",
    "    print(f\"Total unique albums: {len(track_df)}\")\n",
    "    print(\"\\nAlbums by Number of Tracks:\")\n",
    "    print(track_df)\n",
    "\n",
    "\n",
    "    # ---  Bar chart (each album with its track count) ---\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        data=track_df.head(20),\n",
    "        x='track_count',\n",
    "        y=album_col,\n",
    "    )\n",
    "    plt.title(title, fontsize=16, pad=15)\n",
    "    plt.xlabel(\"Number of Tracks\", fontsize=12)\n",
    "    plt.ylabel(\"Album Name\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return track_df\n",
    "\n",
    "album_track_counts = plot_tracks_per_album(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Fetch Albums from 50s till 80s\n",
    "\n",
    "This code identify and analyze albums released between the 1950s and 1980s, summarizing their release years, album types, artist names, and song titles. The goal was to check the accuracy of these albums because they represent the least values in the distribution, so there is a high chance there are error.\n",
    "\n",
    "There are 24 albums from the 50s till the 80s in the dataset.\n",
    "\n",
    "Upon inspection, we found that the album release years were mostly accurate — 21 out of 24 albums matched their verified release dates from official sources such as Discogs and Wikipedia. Only a few albums, like Amico È (Inno dell’amicizia) and Dentro e Fuori, had minor differences of about one or two years.\n",
    "\n",
    "When comparing the albums with the listed artists, most entries turned out to be incorrectly matched. Many classic or international albums, such as Wish You Were Here (Pink Floyd), Rattle and Hum (U2), Babylon By Bus (Bob Marley & The Wailers), and Come Dancing with the Kinks (The Kinks), were mistakenly paired with modern Italian artists like Lazza, Tedua, Baby K, and Gemitaiz.\n",
    "\n",
    "Most Importantly, when analyzing the albums and their corresponding songs (full_title), none of the listed songs actually belonged to the albums they were associated with. The tracks were all modern works by contemporary Italian artists, whereas the albums were classic releases from entirely different time periods and genres.\n",
    "\n"
   ],
   "id": "af76a8efcc51eef8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_albums_by_decades(\n",
    "    df,\n",
    "    start_decade=1950,\n",
    "    end_decade=1980,\n",
    "    date_col='album_release_date',\n",
    "    album_col='album_name',\n",
    "    track_col='full_title',\n",
    "    birth_col='birth_date'\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns albums released between given decades (inclusive),\n",
    "    along with their album type, artist name, birth date,\n",
    "    track titles, and a flag if the artist was born after the album release.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Make a safe copy and ensure proper types ---\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df[birth_col] = pd.to_datetime(df[birth_col], errors='coerce')\n",
    "\n",
    "    # --- Extract release year ---\n",
    "    df['release_year'] = df[date_col].dt.year\n",
    "\n",
    "    # --- Filter albums released in the given range (make a copy to avoid warnings) ---\n",
    "    filtered = df[\n",
    "        (df['release_year'] >= start_decade) &\n",
    "        (df['release_year'] < end_decade + 10)\n",
    "    ].copy()  # ✅ prevents SettingWithCopyWarning\n",
    "\n",
    "    # --- Determine if any artist was born after the album release ---\n",
    "    filtered['artist_born_after_album'] = filtered.apply(\n",
    "        lambda row: (\n",
    "            pd.notna(row[birth_col]) and\n",
    "            pd.notna(row['release_year']) and\n",
    "            row[birth_col].year > row['release_year']\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # --- Group by album and collect information ---\n",
    "    grouped = (\n",
    "        filtered.groupby(album_col, dropna=False)\n",
    "        .agg({\n",
    "            'release_year': 'first',\n",
    "            'album_type': lambda x: list(pd.unique(x)),\n",
    "            'name': lambda x: list(pd.unique(x)),\n",
    "            birth_col: lambda x: list(pd.unique(x)),\n",
    "            track_col: lambda x: list(pd.unique(x)),\n",
    "            'artist_born_after_album': 'any'  ,\n",
    "            'disc_number':lambda x: list(pd.unique(x)),\n",
    "            'track_number':lambda x: list(pd.unique(x)),\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Display and return ---\n",
    "    print(f\"Albums released between {start_decade}s and {end_decade}s: {grouped[album_col].nunique()} unique albums\")\n",
    "    display(grouped[[album_col, 'release_year', 'album_type', 'name', birth_col, track_col, 'artist_born_after_album','disc_number','track_number']])\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "albums_50s_80s = get_albums_by_decades(df, 1950, 1980)\n",
    "dtale.show(albums_50s_80s)\n"
   ],
   "id": "87d18a3194a69c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Number of Tracks per Album",
   "id": "acbfc2d9f1227924"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_tracks_per_album(df, album_col='album_name', title=\"Tracks per top 20 Unique Album \"):\n",
    "    \"\"\"\n",
    "    Counts how many tracks exist for each unique album and plots:\n",
    "\n",
    "    Bar chart of track counts per album\n",
    "    \"\"\"\n",
    "    # --- Drop missing album names ---\n",
    "    df_valid = df.dropna(subset=[album_col]).copy()\n",
    "\n",
    "    # --- Count tracks per album ---\n",
    "    track_counts = df_valid[album_col].value_counts().sort_values(ascending=False)\n",
    "    track_df = track_counts.reset_index()\n",
    "    track_df.columns = [album_col, 'track_count']\n",
    "\n",
    "    # --- Print summary ---\n",
    "    print(f\"Total unique albums: {len(track_df)}\")\n",
    "    print(\"\\nAlbums by Number of Tracks:\")\n",
    "    print(track_df)\n",
    "\n",
    "\n",
    "    # ---  Bar chart (each album with its track count) ---\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        data=track_df.head(20),\n",
    "        x='track_count',\n",
    "        y=album_col,\n",
    "    )\n",
    "    plt.title(title, fontsize=16, pad=15)\n",
    "    plt.xlabel(\"Number of Tracks\", fontsize=12)\n",
    "    plt.ylabel(\"Album Name\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return track_df\n",
    "\n",
    "album_track_counts = plot_tracks_per_album(df)"
   ],
   "id": "a1f38bafde7013f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bcc0c62d9a48c303",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ebb2a8b",
   "metadata": {},
   "source": [
    "##### Distribution of Unique Albums by Type"
   ]
  },
  {
   "cell_type": "code",
   "id": "5268f32d",
   "metadata": {},
   "source": [
    "def plot_unique_album_types(df, album_col='album_name', type_col='album_type',\n",
    "                            title=\"Distribution of Unique Albums by Type\"):\n",
    "    \"\"\"\n",
    "    Shows the count and percentage of each album type,\n",
    "    considering only unique albums (by album name).\n",
    "    \"\"\"\n",
    "    # --- Keep only unique albums ---\n",
    "    unique_albums = df.drop_duplicates(subset=[album_col]).copy()\n",
    "\n",
    "    # --- Drop missing album types ---\n",
    "    unique_albums = unique_albums.dropna(subset=[type_col])\n",
    "\n",
    "    # --- Count occurrences ---\n",
    "    type_counts = unique_albums[type_col].value_counts().sort_values(ascending=False)\n",
    "    type_percent = (type_counts / type_counts.sum() * 100).round(2)\n",
    "\n",
    "    # --- Combine results into a DataFrame ---\n",
    "    summary_df = pd.DataFrame({\n",
    "        'count': type_counts,\n",
    "        'percent': type_percent\n",
    "    })\n",
    "\n",
    "    print(\"Unique Album Types Summary:\\n\")\n",
    "    print(summary_df)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=summary_df.index, y=summary_df['count'], )\n",
    "\n",
    "    # Add count and percentage labels\n",
    "    for i, (count, percent) in enumerate(zip(summary_df['count'], summary_df['percent'])):\n",
    "        plt.text(i, count + 0.5, f\"{count} ({percent}%)\", ha='center', fontsize=10)\n",
    "\n",
    "    plt.title(title, fontsize=16, pad=15)\n",
    "    plt.xlabel(\"Album Type\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Unique Albums\", fontsize=12)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return summary_df\n",
    "plot_unique_album_types(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6de8b8c4",
   "metadata": {},
   "source": [
    "##### Albums with Names but without a release date"
   ]
  },
  {
   "cell_type": "code",
   "id": "72002d4d",
   "metadata": {},
   "source": [
    "def find_albums_missing_release_date(df, album_col='album_name', artist_col='name', date_col='album_release_date'):\n",
    "    \"\"\"\n",
    "    Finds and lists  albums that have a name but no release date.\n",
    "    \"\"\"\n",
    "    missing = df[df[album_col].notna() & df[date_col].isna()][[album_col, artist_col]]\n",
    "    print(f\"Number of  albums with a name but without release date: {len(missing)}\")\n",
    "    display(missing)\n",
    "    return missing\n",
    "\n",
    "missing_albums = find_albums_missing_release_date(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "02baded1",
   "metadata": {},
   "source": [
    "##### Tracks without albums"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf920992",
   "metadata": {},
   "source": [
    "def find_tracks_without_album(df, track_col='full_title', album_col='album_name', artist_col='name'):\n",
    "    \"\"\"\n",
    "    Lists tracks that do not have an album assigned.\n",
    "    \"\"\"\n",
    "    missing_tracks = df[df[album_col].isna()][[track_col, artist_col,album_col,'album_release_date','album_type']].drop_duplicates()\n",
    "    print(f\"Number of tracks without an album: {len(missing_tracks)}\")\n",
    "    display(missing_tracks['full_title'])\n",
    "    return missing_tracks\n",
    "\n",
    "# Usage\n",
    "tracks_without_album = find_tracks_without_album(df)\n",
    "dtale.show(tracks_without_album)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69d626f6",
   "metadata": {},
   "source": [
    "#####  Albums that appear with multiple different artist names"
   ]
  },
  {
   "cell_type": "code",
   "id": "37ad6fb0",
   "metadata": {},
   "source": [
    "def find_albums_with_multiple_artists(\n",
    "    df, \n",
    "    album_col='album_name', \n",
    "    artist_col='name', \n",
    "    feature_col='name_artist'\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds albums that appear with multiple different artist names.\n",
    "    Also lists the different values of another feature (e.g., 'name_artist') for each album.\n",
    "    \"\"\"\n",
    "    # Exclude rows with missing album names\n",
    "    df_filtered = df[df[album_col].notna()]\n",
    "\n",
    "    # Group by album and collect unique artist names and feature values\n",
    "    grouped = (\n",
    "        df_filtered\n",
    "        .groupby(album_col)\n",
    "        .agg({\n",
    "            artist_col: lambda x: list(pd.unique(x)),\n",
    "            feature_col: lambda x: list(pd.unique(x)),\n",
    "            'featured_artists' : lambda x: list(pd.unique(x)),\n",
    "            'album_type':lambda x: list(pd.unique(x)),\n",
    "            'primary_artist':lambda x: list(pd.unique(x)),\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only albums linked to more than one unique artist\n",
    "    inconsistent_albums = grouped[grouped[artist_col].apply(len) > 1]\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    inconsistent_albums = inconsistent_albums.rename(columns={\n",
    "        artist_col: 'Artist Name from Artists Dataset',\n",
    "        feature_col: f'Artist Name from Tracks Dataset',\n",
    "        'featured_artists': 'featured_artists',\n",
    "         'primary_artist':'primary_artist'\n",
    "    })\n",
    "\n",
    "    print(f\"Number of albums with multiple artist names: {len(inconsistent_albums)}\")\n",
    "    display(inconsistent_albums.head(20))\n",
    "\n",
    "    return inconsistent_albums\n",
    "\n",
    "# Usage\n",
    "albums_with_multiple_artists = find_albums_with_multiple_artists(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99045434",
   "metadata": {},
   "source": [
    "##### Albums that have multiple different release dates \n",
    "The analysis identified several albums with multiple recorded release dates. There are 43 albums with different release date. We are going to pick the newest release date in Fixing Errors part."
   ]
  },
  {
   "cell_type": "code",
   "id": "dc063693",
   "metadata": {},
   "source": [
    "def find_albums_with_multiple_release_dates(\n",
    "    df, \n",
    "    album_col='album_name', \n",
    "    date_col='album_release_date',\n",
    "    album_type_col='album_type'\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds albums with multiple different release dates and lists the dates,\n",
    "    also showing the corresponding album type.\n",
    "    \"\"\"\n",
    "    # Exclude null album names\n",
    "    df_filtered = df[df[album_col].notna()]\n",
    "\n",
    "    # Group by album and aggregate unique release dates and album types\n",
    "    grouped = (\n",
    "        df_filtered\n",
    "        .groupby(album_col)\n",
    "        .agg({\n",
    "            date_col: lambda x: list(pd.unique(x)),\n",
    "            album_type_col: lambda x: list(pd.unique(x)),\n",
    "            'name':lambda x: list(pd.unique(x)),\n",
    "            'language':lambda x: list(pd.unique(x))\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only albums with more than one unique release date\n",
    "    inconsistent_albums = grouped[grouped[date_col].apply(len) > 1].copy()\n",
    "\n",
    "    # Rename for clarity\n",
    "    inconsistent_albums = inconsistent_albums.rename(columns={\n",
    "        date_col: 'release_dates',\n",
    "        album_type_col: 'album_types',\n",
    "        'name':'name',\n",
    "        'language':'language'\n",
    "    })\n",
    "\n",
    "    print(f\"Number of albums with multiple release dates: {len(inconsistent_albums)}\")\n",
    "    display(inconsistent_albums)\n",
    "\n",
    "    return inconsistent_albums\n",
    "\n",
    "# Usage\n",
    "albums_with_inconsistent_dates = find_albums_with_multiple_release_dates(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Checking for Albums released before artist's birth\n",
    "\n",
    "There are 23 albums release before the artist has born. After verifying these 23 albums, the albums themselves were wrongly assigned to unrelated artists and songs."
   ],
   "id": "d3161b98b244fa78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_albums_before_birth(df):\n",
    "    \"\"\"\n",
    "    Find albums released before the artist's birth date.\n",
    "    \"\"\"\n",
    "    album_before_birth = df[df['album_release_date'] < df['birth_date']]\n",
    "\n",
    "    print(f\"Albums released before artist's birth: {len(album_before_birth)}\")\n",
    "    return(album_before_birth[['full_title','album_name', 'album_release_date', 'birth_date', 'name','disc_number','track_number']])\n",
    "\n",
    "albums_before_birth=find_albums_before_birth(df)\n",
    "dtale.show(albums_before_birth)"
   ],
   "id": "b911fffe37cbc93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Checking for album released before career start",
   "id": "d979bdb2b88c9cb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_albums_before_active_start(df):\n",
    "    \"\"\"\n",
    "    Find albums released before the artist's active date.\n",
    "    \"\"\"\n",
    "    album_before_active_start = df[df['album_release_date'] < df['active_start']]\n",
    "\n",
    "    print(f\"Albums released before artist's active start date: {len(album_before_active_start)}\")\n",
    "    return(album_before_active_start[['full_title','album_name', 'album_release_date','active_start', 'birth_date', 'name']])\n",
    "\n",
    "album_before_active_start=find_albums_before_active_start(df)\n",
    "display(album_before_active_start)"
   ],
   "id": "df5a1a8e16af74ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4fe4043f",
   "metadata": {},
   "source": [
    "#### Disk Number and Track Number"
   ]
  },
  {
   "cell_type": "code",
   "id": "059a628d",
   "metadata": {},
   "source": [
    "disc_counts = df['disc_number'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "disc_counts.plot(kind='bar')\n",
    "\n",
    "plt.title('Distribution of Disc Numbers')\n",
    "plt.xlabel('Disc Number')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba758cfc",
   "metadata": {},
   "source": [
    "##### Check for missing disc_number or track_number\n",
    "All entries with null values in either the disc_number or track_number columns were found to belong to albums where these fields are consistently missing across all tracks"
   ]
  },
  {
   "cell_type": "code",
   "id": "c53dec74",
   "metadata": {},
   "source": [
    "missing_disc_track = df.loc[\n",
    "    df['disc_number'].isna() | df['track_number'].isna(),\n",
    "    ['album_name', 'album_type', 'full_title']\n",
    "]\n",
    "\n",
    "print(f\"Missing disc_number or track_number: {len(missing_disc_track)} rows\")\n",
    "dtale.show(missing_disc_track)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1318190e",
   "metadata": {},
   "source": [
    "##### Checking for Non numeric values "
   ]
  },
  {
   "cell_type": "code",
   "id": "14e97671",
   "metadata": {},
   "source": [
    "# Check for non-numeric or invalid values in disc_number and track_number (ignoring nulls)\n",
    "invalid_disc = df[\n",
    "    df['disc_number'].notna() & ~df['disc_number'].apply(lambda x: float(x).is_integer())\n",
    "]\n",
    "\n",
    "invalid_track = df[\n",
    "    df['track_number'].notna() & ~df['track_number'].apply(lambda x: float(x).is_integer())\n",
    "]\n",
    "\n",
    "print(f\"Invalid disc_number entries (excluding nulls): {len(invalid_disc)}\")\n",
    "print(invalid_disc[['album_name', 'album_type', 'full_title', 'disc_number', 'track_number']])\n",
    "\n",
    "print(f\"Invalid track_number entries (excluding nulls): {len(invalid_track)}\")\n",
    "print(invalid_track[['album_name', 'album_type', 'full_title', 'disc_number', 'track_number']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0af7c46b",
   "metadata": {},
   "source": [
    "##### Tracks Numbered Outside Expected Ranges"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb2c9a35",
   "metadata": {},
   "source": [
    "# Check for track numbers outside expected range (e.g., <=0 or unusually high)\n",
    "invalid_track_range = df[(df['track_number'] <= 0) | (df['track_number'] > 99)]\n",
    "print(f\"Tracks with invalid numbering: {len(invalid_track_range)}\")\n",
    "print(invalid_track_range)\n",
    "print('---------------------------------------------------')\n",
    "print(df['track_number'].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Albums and their disk number and track number",
   "id": "12f3a2fdf495ac83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group albums but keep duplicates\n",
    "albums_info = (\n",
    "    df.groupby('album_name')[['disc_number', 'track_number', 'full_title', 'name']]\n",
    "    .apply(lambda x: x.sort_values(['disc_number', 'track_number']))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for album in albums_info['album_name'].unique():\n",
    "    subset = albums_info[albums_info['album_name'] == album]\n",
    "    print(f\"\\n🎵 Album: {album}\")\n",
    "    for _, row in subset.iterrows():\n",
    "        print(f\"  Disc {int(row['disc_number'])}, Track {int(row['track_number'])}: \"\n",
    "              f\"{row['full_title']} — {row['name']}\")\n"
   ],
   "id": "9099ed615db6c8a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e767bf9",
   "metadata": {},
   "source": [
    "##### Track Number Sequence Validation\n",
    "A total of 2,600 discs were found to have track sequence issues, indicating widespread inconsistencies in the ordering or numbering of songs within albums. These issues include missing or duplicate track numbers, sequences that don’t start at 1, or tracks that are out of order.\n",
    " \n",
    "The following issues indicate potential problems in the dataset, as they suggest that some albums may have incomplete or inconsistent track listings:\n",
    "\n",
    "Track numbers skip values, implying that some tracks might be missing.\n",
    "\n",
    "The track sequence does not start at 1, suggesting incomplete metadata or incorrect ordering.\n",
    "\n",
    "For this analysis, we will focus only on the duplicate track numbers, since these represent clear inconsistencies that can be directly identified and corrected."
   ]
  },
  {
   "cell_type": "code",
   "id": "355e2b84",
   "metadata": {},
   "source": [
    "def find_track_sequence_issues(df, album_col='album_name', disc_col='disc_number', track_col='track_number'):\n",
    "    \"\"\"\n",
    "       Finds albums/discs with track numbering issues:\n",
    "    - Missing numbers (gaps)\n",
    "    - Duplicate track numbers\n",
    "    - Track numbers not starting at 1\n",
    "    - Non-integer or invalid values\n",
    "    \"\"\"\n",
    "    \n",
    "    issues = []\n",
    "    numberOfDuplicates =0\n",
    "    # Group by album and disc\n",
    "    for (album, disc), group in df.groupby([album_col, disc_col]):\n",
    "        tracks = group[track_col].dropna().tolist()\n",
    "        if not tracks:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        \n",
    "           # Ensure numeric comparison\n",
    "        try:\n",
    "            tracks = sorted([int(x) for x in tracks])\n",
    "        except ValueError:\n",
    "            issues.append({\n",
    "                'album_name': album,\n",
    "                'disc_number': disc,\n",
    "                'issue': 'Non-numeric track numbers',\n",
    "                'track_numbers': group[track_col].unique().tolist()\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Detect duplicates\n",
    "        if len(tracks) != len(set(tracks)):\n",
    "            issues.append({\n",
    "                'album_name': album,\n",
    "                'disc_number': disc,\n",
    "                'issue': 'Duplicate track numbers',\n",
    "                'track_numbers': tracks\n",
    "            })\n",
    "            numberOfDuplicates =numberOfDuplicates+1\n",
    "\n",
    "        # Detect missing numbers\n",
    "        expected = list(range(1, max(tracks) + 1))\n",
    "        missing = sorted(set(expected) - set(tracks))\n",
    "        if missing:\n",
    "            issues.append({\n",
    "                'album_name': album,\n",
    "                'disc_number': disc,\n",
    "                'issue': f'Missing track numbers: {missing}',\n",
    "                'track_numbers': tracks\n",
    "            })\n",
    "\n",
    "        # Detect wrong start\n",
    "        if tracks[0] != 1:\n",
    "            issues.append({\n",
    "                'album_name': album,\n",
    "                'disc_number': disc,\n",
    "                'issue': f'Track sequence does not start at 1 (starts at {tracks[0]})',\n",
    "                'track_numbers': tracks\n",
    "            })\n",
    "\n",
    "\n",
    "    issues_df = pd.DataFrame(issues)\n",
    "    print(f\"Number of discs with sequence issues: {len(issues_df)}\")\n",
    "    print('Number of Disks that have Duplicates number of tracks ',numberOfDuplicates)\n",
    "    return issues_df\n",
    "track_issues = find_track_sequence_issues(df)\n",
    "dtale.show(track_issues)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30be177e",
   "metadata": {},
   "source": [
    "# identifying tracks number duplicates\n",
    "duplicates = df[df.duplicated(subset=['album_name', 'disc_number', 'track_number'], keep=False)]\n",
    "print(f\"🎵 Found {len(duplicates)} duplicate track entries.\")\n",
    "dtale.show(duplicates[['album_name', 'disc_number', 'track_number', 'full_title', 'name']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Duration",
   "id": "7e2ba38825ef6a4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('missing values',df['duration_ms'].isna().sum())\n",
    "plt.hist(df['duration_ms'].dropna(), bins=50)\n",
    "plt.xlabel(\"Duration (ms)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Track Duration (ms)\")\n",
    "plt.show()\n"
   ],
   "id": "8f82582d6cf9181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create duration in minutes\n",
    "df['duration_minutes'] = df['duration_ms'] / 60000\n",
    "\n",
    "# Select and sort\n",
    "song_durations = (\n",
    "    df[['full_title', 'title', 'duration_ms', 'duration_minutes']]\n",
    "    .sort_values(by='duration_minutes', ascending=False)\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "song_durations"
   ],
   "id": "c6ae359ef82248bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a360dbc0",
   "metadata": {},
   "source": [
    "#### Track Year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84395c58",
   "metadata": {},
   "source": [
    "##### Track Year DataType Column \n",
    "The column is Object.  We are going to  to see the values that cannot be converted to Numbers directly.\n",
    "Converting to Number is needed to inspect the values easier.\n",
    "\n",
    "Inspecting the values in the year column, we observed that while most entries were numerical, some contained unexpected or non-numeric characters. To handle this, we converted the column directly to a numeric type using pd.to_numeric() with the errors='coerce' parameter, which automatically transforms any invalid or non-numeric values into NaN."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4c06599",
   "metadata": {},
   "source": [
    "numeric_cols = ['year']\n",
    "\n",
    "# --- Check numeric columns ---\n",
    "for col in numeric_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_numeric(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to numeric:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to numeric.\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "#Converting to numbers    \n",
    "    \n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce') \n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9ebc7d6",
   "metadata": {},
   "source": [
    "##### Track Year Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f1c29",
   "metadata": {},
   "source": [
    "Looking at the distribution of values in the track year in the previous section, we notice some entries before 1950 and after 2025, which don’t make much sense. Therefore, we will investigate these cases further to understand the cause and decide how to correct them.\n",
    "\n",
    "\n",
    "Result:\n",
    "From the plots, we can see that more than half of the songs and albums were released between 2000 and 2020, indicating that most of the data comes from the recent two decades"
   ]
  },
  {
   "cell_type": "code",
   "id": "2669c126",
   "metadata": {},
   "source": [
    "def plot_year_distribution_by_decade(df, title, year_col='year'):\n",
    "    \"\"\"\n",
    "    Plot the percentage distribution of entries by decade based on a given year column.\n",
    "    Does not modify the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Convert 'year' to numeric safely ---\n",
    "    years = pd.to_numeric(df[year_col], errors='coerce').dropna()\n",
    "\n",
    "    # --- Define decade bins (e.g., 1960s, 1970s, ..., 2020s) ---\n",
    "    start = int((years.min() // 10) * 10)\n",
    "    end = int((years.max() // 10) * 10 + 10)\n",
    "    bins = list(range(start, end + 10, 10))\n",
    "    labels = [f\"{b}s\" for b in bins[:-1]]\n",
    "\n",
    "    # --- Categorize years into decades ---\n",
    "    decade_groups = pd.cut(years, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # --- Calculate percentage per decade ---\n",
    "    group_percent = decade_groups.value_counts(normalize=True).sort_index() * 100\n",
    "    group_df = pd.DataFrame({'decade': group_percent.index, 'percent': group_percent.values})\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=group_df, x='decade', y='percent', hue='decade', palette='viridis', legend=False)\n",
    "\n",
    "    # --- Add percentage labels ---\n",
    "    for i, val in enumerate(group_df['percent']):\n",
    "        plt.text(i, val + 0.5, f\"{val:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "    plt.title(title, fontsize=18, pad=15)\n",
    "    plt.xlabel(\"Decade\", fontsize=12)\n",
    "    plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_year_distribution_by_decade(df, \"Percentage of Songs by Decade  before cleaning\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a54e035",
   "metadata": {},
   "source": [
    "Descriptive Statistics\n",
    "The summary statistics show that the song release years range from 1900 to 2100, with an average around 2013, indicating some unrealistic future values."
   ]
  },
  {
   "cell_type": "code",
   "id": "ccb1a924",
   "metadata": {},
   "source": [
    "# For the 'year' column\n",
    "print(df['year'].describe())  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04489f1e",
   "metadata": {},
   "source": [
    "##### Number of Songs before 1950 and after 2025"
   ]
  },
  {
   "cell_type": "code",
   "id": "53330998",
   "metadata": {},
   "source": [
    "def check_songs_before_1950(tracks):\n",
    "    \"\"\"\n",
    "    Identify and display tracks released before 1950.\n",
    "    \"\"\"\n",
    "    tracks['year'] = pd.to_numeric(tracks['year'], errors='coerce')\n",
    "    songs_before_1950 = tracks[tracks['year'] < 1950].shape[0]\n",
    "\n",
    "    print(f\"Number of songs before 1950: {songs_before_1950}\")\n",
    "\n",
    "    return tracks[tracks['year'] < 1950][['full_title', 'album_name', 'album_release_date', 'album_type', 'year', 'month', 'day']]\n",
    "     \n",
    "   \n",
    "old_songs =check_songs_before_1950(tracks)\n",
    "dtale.show(old_songs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a57678f5",
   "metadata": {},
   "source": [
    "def check_songs_after_2025(tracks):\n",
    "    \"\"\"\n",
    "    Identify and display tracks released after 2025.\n",
    "    \"\"\"\n",
    "    tracks['year'] = pd.to_numeric(tracks['year'], errors='coerce')\n",
    "    songs_after_2025 = tracks[tracks['year'] > 2025].shape[0]\n",
    "\n",
    "    print('-----------------------------------------------')\n",
    "    print(f\"Number of songs after 2025: {songs_after_2025}\")\n",
    "\n",
    "    return tracks[tracks['year'] > 2025][['full_title', 'album_name', 'album_release_date', 'album_type', 'year', 'month', 'day']]\n",
    " \n",
    "future_songs =check_songs_after_2025(tracks)\n",
    "dtale.show(future_songs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e11d5f8e",
   "metadata": {},
   "source": [
    "#### Inconsistency with years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108327d",
   "metadata": {},
   "source": [
    "##### Checking if the active_start date is earlier than the artist’s birth_date"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a1ffd4d",
   "metadata": {},
   "source": [
    "def find_invalid_active_start(df):\n",
    "    \"\"\"\n",
    "    Find artists whose active_start date is earlier than their birth date.\n",
    "    \"\"\"\n",
    "    df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "    df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "    invalid_dates = df[df['active_start'] < df['birth_date']]\n",
    "\n",
    "    print(f\"Found {len(invalid_dates)} artists with 'active_start' earlier than 'birth_date'.\")\n",
    "    display(invalid_dates[['id_artist', 'name_artist', 'birth_date', 'active_start']])\n",
    "find_invalid_active_start(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "767f10e6",
   "metadata": {},
   "source": [
    "##### Checking for tracks released (year) before the artist’s career started (active start)"
   ]
  },
  {
   "cell_type": "code",
   "id": "29b2c5a5",
   "metadata": {},
   "source": [
    "def find_tracks_before_career_start(df):\n",
    "    \"\"\"\n",
    "    Find tracks released before the artist's career start.\n",
    "    \"\"\"\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "    df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "    inconsistency = df[\n",
    "        (df['year'].notna()) &\n",
    "        (df['active_start'].notna()) &\n",
    "        (df['year'] < df['active_start'].dt.year)\n",
    "    ]\n",
    "\n",
    "    print(f\"⚠️ Number of records where a song was released before the artist's career start: {len(inconsistency)}\")\n",
    "\n",
    "    return inconsistency[['name', 'full_title', 'year', 'active_start', 'album_release_date']].sort_values(by='name')\n",
    "    \n",
    "tracks_beforer_career=find_tracks_before_career_start(df)\n",
    "dtale.show(tracks_beforer_career)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e02285fa",
   "metadata": {},
   "source": [
    "##### Checking for tracks released before the artist’s birth"
   ]
  },
  {
   "cell_type": "code",
   "id": "18e23e98",
   "metadata": {},
   "source": [
    "def find_tracks_before_birth(df):\n",
    "    \"\"\"\n",
    "    Find tracks released before the artist's birth date.\n",
    "    \"\"\"\n",
    "    tracks_before_birth = df[df['year'] < df['birth_date'].dt.year]\n",
    "\n",
    "    print(f\"Number of tracks released before artist's birth: {len(tracks_before_birth)}\")\n",
    "    return (tracks_before_birth[['full_title', 'year', 'birth_date', 'album_release_date', 'name_artist']])\n",
    "tracks_before_birth =find_tracks_before_birth(df)\n",
    "dtale.show(tracks_before_birth)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2759c4ec",
   "metadata": {},
   "source": [
    "##### Checking for album released before career start"
   ]
  },
  {
   "cell_type": "code",
   "id": "65754b67",
   "metadata": {},
   "source": [
    "def find_albums_before_career(df):\n",
    "    \"\"\"\n",
    "    Find albums released before the artist's career start date.\n",
    "    \"\"\"\n",
    "    album_before_career = df[df['album_release_date'] < df['active_start']]\n",
    "\n",
    "    print(f\"Albums released before artist's career start: {len(album_before_career)}\")\n",
    "    return (album_before_career[['full_title', 'album_release_date', 'active_start', 'name_artist']])\n",
    "albums_before_career =find_albums_before_career(df)\n",
    "dtale.show(albums_before_career)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22f0936c",
   "metadata": {},
   "source": [
    "##### Checking for Albums released before artist's birth"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b0cafa2",
   "metadata": {},
   "source": [
    "def find_albums_before_birth(df):\n",
    "    \"\"\"\n",
    "    Find albums released before the artist's birth date.\n",
    "    \"\"\"\n",
    "    album_before_birth = df[df['album_release_date'] < df['birth_date']]\n",
    "\n",
    "    print(f\"Albums released before artist's birth: {len(album_before_birth)}\")\n",
    "    return(album_before_birth[['full_title', 'album_release_date', 'birth_date', 'name_artist']])\n",
    "albums_before_birth=find_albums_before_birth(df)\n",
    "dtale.show(albums_before_birth)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f21f1921",
   "metadata": {},
   "source": [
    "##### Checking for Tracks released before album release excluding singles"
   ]
  },
  {
   "cell_type": "code",
   "id": "b90b6ab2",
   "metadata": {},
   "source": [
    "def find_tracks_before_album(df):\n",
    "    \"\"\"\n",
    "    Find tracks released before their album release (excluding singles).\n",
    "    \"\"\"\n",
    "    tracks_before_album = df[\n",
    "        (df['year'] < df['album_release_date'].dt.year) &\n",
    "        (df['album_type'] != 'single')\n",
    "    ]\n",
    "\n",
    "    print(f\"Tracks released before the album (excluding singles): {len(tracks_before_album)}\")\n",
    "    return (tracks_before_album[['full_title', 'year', 'album_release_date', 'album_type', 'name_artist']])\n",
    "tracks_before_album =find_tracks_before_album(df)\n",
    "dtale.show(tracks_before_album)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "627da10c",
   "metadata": {},
   "source": [
    "####  Artists Location Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89500a87",
   "metadata": {},
   "source": [
    "##### Inspecting Coordinates and Birthplace\n",
    "\n",
    "Upon inspecting the geographic data, we found that the vast majority of coordinates correctly referred to each artist’s birth place. However, three rows were inconsistent: their birth places were listed as Almería, Buenos Aires, and Singapore, but the the coordinates either null or refer to province of the artist\n",
    "\n",
    "To maintain consistency across the dataset, we are going to replace the coordinates in these rows with the correct latitude and longitude of the corresponding birth places."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_copy=df.copy()\n",
    "# Drop rows with no birth_place\n",
    "df_copy = df_copy.dropna(subset=['birth_place'])\n",
    "\n",
    "# Keep only the columns we care about\n",
    "cols = ['birth_place', 'latitude', 'longitude','province','region']\n",
    "\n",
    "# Drop duplicates so each place appears once (keeping the first lat/lon found)\n",
    "unique_places = df_copy[cols].drop_duplicates(subset=['birth_place'])\n",
    "\n",
    "# Sort alphabetically by birth_place (optional)\n",
    "unique_places = unique_places.sort_values(by='birth_place').reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "dtale.show(unique_places)\n"
   ],
   "id": "6c7db0c4970ec7b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Getting null rows",
   "id": "cf22cd8e5dac60a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define columns to check\n",
    "cols_to_check = [\n",
    "    'birth_place',\n",
    "    'nationality',\n",
    "    'province',\n",
    "    'region',\n",
    "    'country',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# Filter rows where any of these columns are null\n",
    "missing_rows = df[df[cols_to_check].isnull().any(axis=1)]\n",
    "\n",
    "# Select only artist name + the relevant columns\n",
    "columns_to_show = ['name'] + cols_to_check\n",
    "missing_subset = missing_rows[columns_to_show]\n",
    "\n",
    "# Keep only unique artist names (first occurrence)\n",
    "unique_missing_subset = missing_subset.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Show the result in D-Tale\n",
    "print(unique_missing_subset.shape)\n",
    "display(unique_missing_subset)\n"
   ],
   "id": "95103a0dbb82763a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Checking Coordinates",
   "id": "ff577a98f4239ca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_outliers = df[(df['latitude'] < 35.5) | (df['latitude'] > 47.1) |\n",
    "                  (df['longitude'] < 6.6) | (df['longitude'] > 18.5)]\n",
    "print(f\"Number of Geographic coordinates outside Italy range: {len(geo_outliers)} records\")\n",
    "display(geo_outliers[['name_artist', 'latitude', 'longitude', 'birth_place']].head(10))"
   ],
   "id": "6dd93a9f2a133166",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dae8375f",
   "metadata": {},
   "source": [
    "##### Artists' Country Values\n",
    "\n",
    "All the countries have the value of Italia"
   ]
  },
  {
   "cell_type": "code",
   "id": "c52c8ded",
   "metadata": {},
   "source": [
    "# Count the occurrences of each country\n",
    "country_counts = df['country'].value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "country_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Distribution of Artists by Country', fontsize=14, pad=12)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Artists', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e5d72d7",
   "metadata": {},
   "source": [
    "##### Checking if there is an artist his/her country not Italy but his/her coordinates are in Italy"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a810541",
   "metadata": {},
   "source": [
    "# Filter rows where country is not Italy and coordinates are present\n",
    "non_italy_with_coords = df[\n",
    "    (df['country'].notna()) & \n",
    "    (df['country'] != \"Italia\") & \n",
    "    (df['latitude'].notna()) & \n",
    "    (df['longitude'].notna())\n",
    "]\n",
    "\n",
    "# Count the number of such records\n",
    "num_records = len(non_italy_with_coords)\n",
    "print(f\"Number of non-Italy records with coordinates: {num_records}\")\n",
    "\n",
    "# Show the records\n",
    "print(non_italy_with_coords[['country', 'latitude', 'longitude']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9a5d10c",
   "metadata": {},
   "source": [
    "##### Artists Nationality Distribution\n",
    "\n",
    "Almost all artists are Italian (99.5%), with a small minority from Argentina (0.5%)."
   ]
  },
  {
   "cell_type": "code",
   "id": "671bb61f",
   "metadata": {},
   "source": [
    "def plot_nationality_distribution(df, column='nationality', top_n=20):\n",
    "    \"\"\"\n",
    "    Plots the percentage distribution of artist nationalities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing nationality information.\n",
    "    column : str, optional\n",
    "        The column name containing nationality data (default = 'nationality').\n",
    "    top_n : int, optional\n",
    "        Number of top nationalities to display in the chart (default = 20).\n",
    "    \"\"\"\n",
    "\n",
    "    # Count and calculate percentages\n",
    "    nat_counts = df[column].value_counts(dropna=True)\n",
    "    nat_percent = (nat_counts / nat_counts.sum()) * 100\n",
    "    nat_df = nat_percent.reset_index()\n",
    "    nat_df.columns = [column, 'percent']\n",
    "\n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(\n",
    "        data=nat_df.head(top_n),\n",
    "        x='percent',\n",
    "        y=column,\n",
    "        hue=column,\n",
    "        palette='crest',\n",
    "        dodge=False\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Percentage of Artists by {column.capitalize()}\", fontsize=18, pad=15)\n",
    "    plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "    plt.ylabel(column.capitalize(), fontsize=12)\n",
    "\n",
    "    # Add percentage labels\n",
    "    for index, value in enumerate(nat_df.head(top_n)['percent']):\n",
    "        plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "    plt.xlim(0, nat_df['percent'].max() + 5)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the computed DataFrame for reference\n",
    "    return nat_df\n",
    "# Example usage\n",
    "nat_df = plot_nationality_distribution(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9068c310",
   "metadata": {},
   "source": [
    "##### Checking if there are artists with Non-Italian Nationality and Italian Coordinates\n",
    "There is one instance repeated 40 times where the artist has a nationality other than Italian (all Argentinian) but also have italian geographic coordinates."
   ]
  },
  {
   "cell_type": "code",
   "id": "60ecbffc",
   "metadata": {},
   "source": [
    "def get_non_italy_with_coords(df, nationality_col='nationality',\n",
    "                              lat_col='latitude', lon_col='longitude'):\n",
    "    \"\"\"\n",
    "    Filters rows where nationality is not 'Italia' and coordinates are present.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing nationality and coordinate columns.\n",
    "    nationality_col : str, optional\n",
    "        Column name for nationality (default = 'nationality').\n",
    "    lat_col : str, optional\n",
    "        Column name for latitude (default = 'latitude').\n",
    "    lon_col : str, optional\n",
    "        Column name for longitude (default = 'longitude').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Filtered DataFrame of non-Italy records with valid coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the data\n",
    "    non_italy = df[\n",
    "        (df[nationality_col].notna()) &\n",
    "        (df[nationality_col].str.lower() != \"italia\") &\n",
    "        (df[lat_col].notna()) &\n",
    "        (df[lon_col].notna())\n",
    "        ]\n",
    "\n",
    "    # Count and print summary\n",
    "    num_records = len(non_italy)\n",
    "    print(f\"Number of non-Italy nationality records with coordinates: {num_records}\\n\")\n",
    "\n",
    "    # Show preview of relevant columns\n",
    "    print(non_italy[[nationality_col, lat_col, lon_col]].head(10))\n",
    "\n",
    "    return non_italy\n",
    "\n",
    "get_non_italy_with_coords(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b61c59c0",
   "metadata": {},
   "source": [
    "##### Distribution of Artist's Birth Places\n",
    "\n",
    "The majority of artists were born in major Italian cities, with Milano (1,843) and Roma (1,048) being the most frequent birthplaces, indicating a strong concentration of artists from these cultural and economic centers.\n",
    "\n",
    "Smaller Italian towns such as Senigallia (443), Torino (397), and Avellino (329) also show notable representation, suggesting a widespread national distribution beyond just the biggest cities.\n",
    "\n",
    "Only a few artists were born outside Italy — such as Buenos Aires (40) and Almería (26) — representing less than 1% of the total, which confirms that the dataset is predominantly composed of Italian-born artists."
   ]
  },
  {
   "cell_type": "code",
   "id": "3469be61",
   "metadata": {},
   "source": [
    "def plot_birthplace_distribution(df, column='birth_place', top_n=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a categorical column (default: 'birth_place'),\n",
    "    showing both counts and percentages above each bar.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing the column to analyze.\n",
    "    column : str, optional\n",
    "        The column name to plot (default = 'birth_place').\n",
    "    top_n : int or None, optional\n",
    "        Show only the top N most frequent categories (default = all).\n",
    "    \"\"\"\n",
    "\n",
    "    # Count and percentage\n",
    "    counts = df[column].value_counts()\n",
    "    if top_n:\n",
    "        counts = counts.head(top_n)\n",
    "    percents = (counts / len(df)) * 100\n",
    "\n",
    "    # Print counts for inspection\n",
    "    print(counts)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.bar(counts.index, counts.values, color='skyblue')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title(f'Distribution of {column.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    plt.xlabel(column.replace(\"_\", \" \").title())\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Add count and percent labels above bars\n",
    "    for i, (count, percent) in enumerate(zip(counts.values, percents.values)):\n",
    "        plt.text(i, count + (count * 0.01),\n",
    "                 f\"{count:,}\\n({percent:.1f}%)\",\n",
    "                 ha='center', va='bottom', fontsize=6, color='black')\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Plot all birthplaces\n",
    "plot_birthplace_distribution(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "052af884",
   "metadata": {},
   "source": [
    "##### Checking Birth Place vs Country\n",
    "\n",
    "This section verifies whether each artist’s birth place matches their country.  We have one instance in which the artist was born in Almería but his country is Italy."
   ]
  },
  {
   "cell_type": "code",
   "id": "52d8799d",
   "metadata": {},
   "source": [
    "def check_birthplace_country_mismatch(\n",
    "        df,\n",
    "        italian_cities=None,\n",
    "        foreign_cities_to_country=None,\n",
    "        birth_col='birth_place',\n",
    "        country_col='country'\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect mismatches between birth_place and country columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing artist information.\n",
    "    italian_cities : list, optional\n",
    "        List of Italian city names to validate (default = predefined list).\n",
    "    foreign_cities_to_country : dict, optional\n",
    "        Mapping of known foreign cities to their expected country names.\n",
    "    birth_col : str, optional\n",
    "        Column name for birthplace (default = 'birth_place').\n",
    "    country_col : str, optional\n",
    "        Column name for country (default = 'country').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with a new boolean column 'birth_place_country_mismatch'\n",
    "        and prints mismatch summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default Italian cities list\n",
    "    if italian_cities is None:\n",
    "        italian_cities = [\n",
    "            \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "            \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "            \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "            \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "            \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "            \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "            \"Lodi\"\n",
    "        ]\n",
    "\n",
    "    # Default foreign city mappings\n",
    "    if foreign_cities_to_country is None:\n",
    "        foreign_cities_to_country = {\n",
    "            \"Singapore\": \"Singapore\",\n",
    "            \"Buenos Aires\": \"Argentina\",\n",
    "            \"Almería\": \"Spagna\",\n",
    "        }\n",
    "\n",
    "    # Inner function for row-level logic\n",
    "    def _check(row):\n",
    "        birth = row.get(birth_col)\n",
    "        country = row.get(country_col)\n",
    "        if pd.notna(birth) and pd.notna(country):\n",
    "            if birth in italian_cities and country != \"Italia\":\n",
    "                return True  # mismatch\n",
    "            elif birth in foreign_cities_to_country:\n",
    "                if country != foreign_cities_to_country[birth]:\n",
    "                    return True  # mismatch\n",
    "        return False  # coherent or missing data\n",
    "\n",
    "    # Apply to dataframe\n",
    "    df = df.copy()\n",
    "    df['birth_place_country_mismatch'] = df.apply(_check, axis=1)\n",
    "\n",
    "    # Summary\n",
    "    num_mismatches = df['birth_place_country_mismatch'].sum()\n",
    "    print(f\"Number of birth_place-country mismatches: {num_mismatches}\")\n",
    "\n",
    "    # Show mismatched records\n",
    "    mismatched = df[df['birth_place_country_mismatch']]\n",
    "    if not mismatched.empty:\n",
    "        print(\"\\nMismatched records:\")\n",
    "        display(mismatched[[birth_col, country_col]])\n",
    "    else:\n",
    "        print(\"\\n✅ No mismatches found.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_checked = check_birthplace_country_mismatch(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd84ae6d",
   "metadata": {},
   "source": [
    "##### Birth Place vs Nationality\n",
    "\n",
    "We have two instance in which the artist was born outside Italy but the nationality is Italian. Chadia rodriguez  was born in Almería and baby k was born in  Singapore."
   ]
  },
  {
   "cell_type": "code",
   "id": "21b250d0",
   "metadata": {},
   "source": [
    "def check_birthplace_nationality_mismatch(\n",
    "        df,\n",
    "        italian_cities=None,\n",
    "        foreign_cities_to_nationality=None,\n",
    "        birth_col='birth_place',\n",
    "        nationality_col='nationality',\n",
    "        name_col='name'\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect mismatches between birthplace and nationality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset containing birthplace and nationality columns.\n",
    "    italian_cities : list, optional\n",
    "        List of Italian city names (default = predefined Italian cities).\n",
    "    foreign_cities_to_nationality : dict, optional\n",
    "        Mapping of known foreign cities to their expected nationality names.\n",
    "    birth_col : str, optional\n",
    "        Column name for birthplace (default = 'birth_place').\n",
    "    nationality_col : str, optional\n",
    "        Column name for nationality (default = 'nationality').\n",
    "    name_col : str, optional\n",
    "        Column name for artist name (default = 'name').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Original DataFrame with an added boolean column:\n",
    "        'birth_place_nationality_mismatch'\n",
    "    \"\"\"\n",
    "\n",
    "    # Default list of Italian cities\n",
    "    if italian_cities is None:\n",
    "        italian_cities = [\n",
    "            \"Milano\", \"Roma\", \"Senigallia\", \"Torino\", \"Avellino\", \"Cagliari\", \"Salerno\",\n",
    "            \"Olbia\", \"Napoli\", \"Vimercate\", \"Vicenza\", \"Verona\", \"Scampia\", \"Nicosia\",\n",
    "            \"Sternatia\", \"Padova\", \"Grottaglie\", \"La Spezia\", \"Scafati\", \"Nocera Inferiore\",\n",
    "            \"Sesto San Giovanni\", \"Genova\", \"Alpignano\", \"Fiumicino\", \"Treviso\", \"Bologna\",\n",
    "            \"San Siro\", \"Rho\", \"Brescia\", \"Grugliasco\", \"Reggio Calabria\", \"Gallarate\",\n",
    "            \"Desenzano del Garda\", \"Pieve Emanuele\", \"San Benedetto del Tronto\", \"Firenze\",\n",
    "            \"Lodi\"\n",
    "        ]\n",
    "\n",
    "    # Default mapping of foreign cities\n",
    "    if foreign_cities_to_nationality is None:\n",
    "        foreign_cities_to_nationality = {\n",
    "            \"Singapore\": \"Singapore\",\n",
    "            \"Buenos Aires\": \"Argentina\",\n",
    "            \"Almería\": \"Spagna\",\n",
    "        }\n",
    "\n",
    "    # Define the mismatch check logic\n",
    "    def _check(row):\n",
    "        birth = row.get(birth_col)\n",
    "        nationality = row.get(nationality_col)\n",
    "        if pd.notna(birth) and pd.notna(nationality):\n",
    "            if birth in italian_cities and nationality != \"Italia\":\n",
    "                return True  # mismatch\n",
    "            elif birth in foreign_cities_to_nationality:\n",
    "                if nationality != foreign_cities_to_nationality[birth]:\n",
    "                    return True  # mismatch\n",
    "        return False\n",
    "\n",
    "    # Apply to DataFrame\n",
    "    df = df.copy()\n",
    "    df['birth_place_nationality_mismatch'] = df.apply(_check, axis=1)\n",
    "\n",
    "    # Report\n",
    "    num_mismatches = df['birth_place_nationality_mismatch'].sum()\n",
    "    print(f\"Number of birth_place-nationality mismatches: {num_mismatches}\")\n",
    "\n",
    "    # Display mismatched records (if any)\n",
    "    mismatched_records = df[df['birth_place_nationality_mismatch']]\n",
    "    if not mismatched_records.empty:\n",
    "        print(\"\\nMismatched records:\")\n",
    "        print(mismatched_records[[name_col, birth_col, nationality_col]].drop_duplicates())\n",
    "    else:\n",
    "        print(\"\\n✅ No mismatches found.\")\n",
    "\n",
    "    return df\n",
    "# Run the function on your DataFrame\n",
    "df_checked = check_birthplace_nationality_mismatch(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "94d951dc",
   "metadata": {},
   "source": [
    "##### Distribution of Songs by Province and Region\n",
    "\n",
    "This code calculates and visualizes the percentage distribution of songs by province and region. It counts occurrences, converts them to percentages, and displays bar charts with labeled values to show which areas have the highest song representation"
   ]
  },
  {
   "cell_type": "code",
   "id": "97cc29af",
   "metadata": {},
   "source": [
    "def plot_location_distributions(\n",
    "        df,\n",
    "        province_col='province',\n",
    "        region_col='region',\n",
    "        top_n=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots percentage distributions for province and region columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset containing province and region columns.\n",
    "    province_col : str, optional\n",
    "        Column name for province (default = 'province').\n",
    "    region_col : str, optional\n",
    "        Column name for region (default = 'region').\n",
    "    top_n : int, optional\n",
    "        Number of top entries to display (default = 20).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Province Plot ----------\n",
    "    province_counts = df[province_col].value_counts()\n",
    "    province_percent = (province_counts / province_counts.sum()) * 100\n",
    "    province_df = province_percent.reset_index()\n",
    "    province_df.columns = [province_col, 'percent']\n",
    "\n",
    "    print(\"Provinces:\\n\", province_counts, \"\\n\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(\n",
    "        data=province_df.head(top_n),\n",
    "        x='percent',\n",
    "        y=province_col,\n",
    "        hue=province_col,\n",
    "        palette='viridis',\n",
    "        dodge=False\n",
    "    )\n",
    "\n",
    "    plt.title(\"Percentage of Songs by Province\", fontsize=20, pad=15, color=\"#000000\")\n",
    "    plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "    plt.ylabel(\"Province\", fontsize=12)\n",
    "\n",
    "    # Add labels\n",
    "    for index, value in enumerate(province_df.head(top_n)['percent']):\n",
    "        plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "    plt.xlim(0, province_df['percent'].max() + 5)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- Region Plot ----------\n",
    "    region_counts = df[region_col].value_counts()\n",
    "    region_percent = (region_counts / region_counts.sum()) * 100\n",
    "    region_df = region_percent.reset_index()\n",
    "    region_df.columns = [region_col, 'percent']\n",
    "\n",
    "    print(\"Regions:\\n\", region_counts, \"\\n\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(\n",
    "        data=region_df,\n",
    "        x='percent',\n",
    "        y=region_col,\n",
    "        hue=region_col,\n",
    "        palette='coolwarm',\n",
    "        dodge=False\n",
    "    )\n",
    "\n",
    "    plt.title(\"Percentage of Songs by Region\", fontsize=20, pad=15, color=\"#000000\")\n",
    "    plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "    plt.ylabel(\"Region\", fontsize=12)\n",
    "\n",
    "    for index, value in enumerate(region_df['percent']):\n",
    "        plt.text(value + 0.5, index, f\"{value:.1f}%\", va='center', fontsize=9, color='#000000')\n",
    "\n",
    "    plt.xlim(0, region_df['percent'].max() + 5)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return summary DataFrames\n",
    "    return province_df, region_df\n",
    "# Example usage\n",
    "province_df, region_df = plot_location_distributions(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "847032a7",
   "metadata": {},
   "source": "##### Province/Region vs Country\n"
  },
  {
   "cell_type": "code",
   "id": "fa563015",
   "metadata": {},
   "source": [
    "# Example mapping of Italian regions to their provinces (from your data)\n",
    "region_provinces = {\n",
    "    \"Lombardia\": [\"Milano\", \"Monza e della Brianza\", \"Brescia\", \"Varese\", \"Lodi\"],\n",
    "    \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\"],\n",
    "    \"Lazio\": [\"Roma\"],\n",
    "    \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "    \"Piemonte\": [\"Torino\"],\n",
    "    \"Sardegna\": [\"Cagliari\", \"Gallura\"],\n",
    "    \"Puglia\": [\"Lecce\", \"Taranto\"],\n",
    "    \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "    \"Sicilia\": [\"Enna\"],\n",
    "    \"Emilia-Romagna\": [\"Bologna\"],\n",
    "    \"Calabria\": [\"Reggio Calabria\"],\n",
    "    \"Marche\": [\"Ancona\", \"Ascoli Piceno\"],\n",
    "    \"Toscana\": [\"Firenze\"]\n",
    "}\n",
    "\n",
    "# Flatten all Italian provinces for quick lookup\n",
    "all_italian_provinces = [prov for provs in region_provinces.values() for prov in provs]\n",
    "\n",
    "# Function to check province/region ↔ country\n",
    "def check_province_region_country(row):\n",
    "    if pd.notna(row['country']):\n",
    "        if pd.notna(row['province']) and row['province'] in all_italian_provinces:\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "        elif pd.notna(row['region']) and row['region'] in region_provinces.keys():\n",
    "            if row['country'] != \"Italia\":\n",
    "                return True  # mismatch\n",
    "    return False  # coherent or missing data\n",
    "\n",
    "# Apply the function\n",
    "df['province_region_country_mismatch'] = df.apply(check_province_region_country, axis=1)\n",
    "\n",
    "# Count mismatches\n",
    "num_mismatches = df['province_region_country_mismatch'].sum()\n",
    "print(f\"Number of province/region-country mismatches: {num_mismatches}\")\n",
    "\n",
    "# Show records with mismatch\n",
    "mismatched_records = df[df['province_region_country_mismatch']]\n",
    "print(mismatched_records[['province', 'region', 'country']])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "470d0871",
   "metadata": {},
   "source": [
    "##### Province/Region vs Birth Place\n",
    "\n",
    "This check compares each artist’s birth_place with the corresponding province and region. Mismatches occur when the province or region does not align with the birth_place. There are 19 uniques cases in which the province/region doesn't match the birth_place."
   ]
  },
  {
   "cell_type": "code",
   "id": "96982939",
   "metadata": {},
   "source": [
    "def check_birthplace_province_region_mismatch(\n",
    "        df,\n",
    "        region_provinces=None,\n",
    "        birth_col='birth_place',\n",
    "        province_col='province',\n",
    "        region_col='region'\n",
    "):\n",
    "    \"\"\"\n",
    "    Checks for mismatches between a person's birthplace, province, and region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing birthplace, province, and region columns.\n",
    "    region_provinces : dict, optional\n",
    "        Mapping of regions → list of provinces/birthplaces.\n",
    "        (If None, uses the predefined mapping from your dataset.)\n",
    "    birth_col : str, optional\n",
    "        Column name for birthplace (default = 'birth_place').\n",
    "    province_col : str, optional\n",
    "        Column name for province (default = 'province').\n",
    "    region_col : str, optional\n",
    "        Column name for region (default = 'region').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Original DataFrame with an added boolean column:\n",
    "        'birth_place_province_region_mismatch'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default mapping (your provided one)\n",
    "    if region_provinces is None:\n",
    "        region_provinces = {\n",
    "            \"Lombardia\": [\"Milano\", \"Vimercate\", \"Sesto San Giovanni\", \"Alpignano\", \"Fiumicino\",\n",
    "                          \"Brescia\", \"Grugliasco\", \"Rho\", \"Gallarate\", \"Desenzano del Garda\", \"Lodi\", \"San Siro\"],\n",
    "            \"Lazio\": [\"Roma\"],\n",
    "            \"Piemonte\": [\"Torino\"],\n",
    "            \"Campania\": [\"Salerno\", \"Napoli\", \"Avellino\", \"Scafati\", \"Nocera Inferiore\"],\n",
    "            \"Veneto\": [\"Vicenza\", \"Verona\", \"Padova\", \"Treviso\"],\n",
    "            \"Sardegna\": [\"Cagliari\", \"Olbia\", \"Gallura\"],\n",
    "            \"Puglia\": [\"Lecce\", \"Taranto\", \"Grottaglie\", \"Sternatia\", \"San Benedetto del Tronto\"],\n",
    "            \"Liguria\": [\"Genova\", \"La Spezia\"],\n",
    "            \"Sicilia\": [\"Enna\", \"Nicosia\"],\n",
    "            \"Emilia-Romagna\": [\"Bologna\"],\n",
    "            \"Calabria\": [\"Reggio Calabria\"],\n",
    "            \"Marche\": [\"Ancona\", \"Senigallia\", \"Ascoli Piceno\"],\n",
    "            \"Toscana\": [\"Firenze\", \"Scampia\", \"Padova\"]\n",
    "        }\n",
    "\n",
    "    # Flatten mapping to get province → region\n",
    "    province_to_region = {\n",
    "        prov: reg for reg, provs in region_provinces.items() for prov in provs\n",
    "    }\n",
    "\n",
    "    # Row-level check\n",
    "    def _check(row):\n",
    "        birth = row.get(birth_col)\n",
    "        prov = row.get(province_col)\n",
    "        reg = row.get(region_col)\n",
    "\n",
    "        if pd.notna(birth) and birth in province_to_region:\n",
    "            expected_region = province_to_region[birth]\n",
    "\n",
    "            # Check for mismatched province or region\n",
    "            if (pd.notna(prov) and prov != birth) or (pd.notna(reg) and reg != expected_region):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Apply check to DataFrame\n",
    "    df = df.copy()\n",
    "    df['birth_place_province_region_mismatch'] = df.apply(_check, axis=1)\n",
    "\n",
    "    # Report\n",
    "    num_mismatches = df['birth_place_province_region_mismatch'].sum()\n",
    "    print(f\"Number of birth_place–province/region mismatches: {num_mismatches}\")\n",
    "\n",
    "    mismatched = df[df['birth_place_province_region_mismatch']].drop_duplicates(\n",
    "        subset=[birth_col, province_col, region_col]\n",
    "    )\n",
    "    if not mismatched.empty:\n",
    "        print(\"\\nMismatched records:\")\n",
    "        print(mismatched[[birth_col, province_col, region_col]])\n",
    "    else:\n",
    "        print(\"\\n✅ No mismatches found.\")\n",
    "\n",
    "    return df\n",
    "# Run the function on your dataset\n",
    "df_checked = check_birthplace_province_region_mismatch(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92fe8ac6",
   "metadata": {},
   "source": [
    "##### Geographic Distribution of Artists by Province and Region\n",
    "\n",
    "This analysis aggregates the number of artists by their latitude, longitude, province, and region. The resulting table shows the locations with the highest concentration of artists at the top. For example, Milano (Lombardia) has the most artists with 1,843, followed by Roma (Lazio) with 1,048, and Torino (Piemonte) with 397. The code also generates a map where the size and color of the points reflect the number of artists per location, providing a clear visual of artist density across Italy."
   ]
  },
  {
   "cell_type": "code",
   "id": "db1bfe7b",
   "metadata": {},
   "source": [
    "# Aggregate by latitude and longitude to count number of artists\n",
    "location_counts = df.groupby(['latitude', 'longitude', 'region', 'province']).size().reset_index(name='num_artists')\n",
    "\n",
    "# Sort by number of artists descending\n",
    "location_counts = location_counts.sort_values(by='num_artists', ascending=False)\n",
    "\n",
    "# Print the sorted table\n",
    "print(location_counts)\n",
    "\n",
    "# Define a color scale\n",
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "\n",
    "# Create the scatter map\n",
    "fig = px.scatter_mapbox(\n",
    "    location_counts,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    hover_data=[\"region\", \"province\", \"num_artists\"],  # show count on hover\n",
    "    size=\"num_artists\",  # size of marker represents number of artists\n",
    "    color=\"num_artists\",  # color also shows density\n",
    "    color_continuous_scale=color_scale,\n",
    "    zoom=5,\n",
    "    height=800,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0fb08601c4534c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Popularity\n",
    "\n",
    "The popularity feature is a key indicator representing the estimated popularity of a track, typically defined in a normalized range between 0 and 100."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea378da5e6e7cec9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check type of popularity\n",
    "\n",
    "mask_simboli = df['popularity'].astype(str).str.contains(r'%|[MKmk]|views|[A-Za-z]', na=False)\n",
    "\n",
    "valori_strani2 = (\n",
    "    df.loc[mask_simboli, 'popularity']\n",
    "      .dropna()\n",
    "      .drop_duplicates()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "print(valori_strani2.to_list())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fea571b6d650ea8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "Looking at the values of the  `popularity` column, we noticed that some entries contained **non-numeric characters**, percent signs (`%`), abbreviations like `K` (thousands) or `M` (millions), and words such as `\"views\"` appended to the numbers.  \n",
    "\n",
    "Instead of converting the column directly to numeric using pd.to_numeric(errors='coerce'), which would have turned all invalid entries into NaN, we applied a cleaning function to preserve and correctly interpret useful numeric information before conversion. The function:\n",
    "\n",
    "- Removed non-numeric characters and words like `\"views\"` and `%`.\n",
    "- Converted abbreviations (`K → 1,000`, `M → 1,000,000`) to numeric values.\n",
    "- Extracted the first numeric part if extra text was present.\n",
    "- Converted the cleaned values to floats, marking any remaining invalid entries as `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f3124e1ab1b11b3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_popularity(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value_str = str(value).strip().lower()  # normalize\n",
    "    \n",
    "    # Remove common words like 'views'\n",
    "    value_str = value_str.replace('views','').replace('%','').strip()\n",
    "    value_str = value_str.lower()  \n",
    "    # Handle K and M\n",
    "    multiplier = 1\n",
    "    if value_str.endswith('k'):\n",
    "        multiplier = 1_000\n",
    "        value_str = value_str[:-1]\n",
    "    elif value_str.endswith('m'):\n",
    "        multiplier = 1_000_000\n",
    "        value_str = value_str[:-1]\n",
    "    \n",
    "    # Take only first token if words remain\n",
    "    value_str = value_str.split()[0]\n",
    "    \n",
    "    # Try converting to float\n",
    "    try:\n",
    "        return (float(value_str) * multiplier)\n",
    "    except:\n",
    "        return None  # invalid entries become None/NaN\n",
    "    \n",
    "df['popularity'].apply(clean_popularity)\n",
    "\n",
    "def clean_popularity(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    value_str = str(value).strip().lower()\n",
    "\n",
    "    # togli parole/simboli comuni\n",
    "    value_str = value_str.replace('views', '').replace('%', '').strip()\n",
    "\n",
    "    # gestisci suffissi K / M\n",
    "    multiplier = 1\n",
    "    if value_str.endswith('k'):\n",
    "        multiplier = 1_000\n",
    "        value_str = value_str[:-1]\n",
    "    elif value_str.endswith('m'):\n",
    "        multiplier = 1_000_000\n",
    "        value_str = value_str[:-1]\n",
    "\n",
    "    # prendi solo il primo token se rimane testo\n",
    "    value_str = value_str.split()[0]\n",
    "\n",
    "    try:\n",
    "        return float(value_str) * multiplier\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# applica la pulizia\n",
    "df['popularity'] = df['popularity'].apply(clean_popularity)\n",
    "\n",
    "\n",
    "df['popularity'] = df['popularity'].astype('float')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa12aab9424dda61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Missing values count and percentage\n",
    "missing_pop = df['popularity'].isna().sum()\n",
    "missing_perc = (missing_pop / len(df)) * 100\n",
    "\n",
    "print(\"Missing popularity values:\", missing_pop)\n",
    "print(f\"Missing popularity percentage: {missing_perc:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20a1877ae47c9435",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conteggi\n",
    "low = df[df['popularity'] < 0].shape[0]\n",
    "high = df[df['popularity'] > 100].shape[0]\n",
    "total = low + high\n",
    "\n",
    "# DataFrame per il plot\n",
    "plot_df = pd.DataFrame({\n",
    "    \"condition\": [\"popularity < 0\", \"popularity > 100\", \"out of range total\"],\n",
    "    \"count\": [low, high, total]\n",
    "})\n",
    "\n",
    "print(\"Values < 0:\", low) \n",
    "print(\"Values > 100:\", high) \n",
    "print(\"Total out of range:\", low + high)\n",
    "\n",
    "# Grafico Altair\n",
    "chart = alt.Chart(plot_df).mark_bar().encode(\n",
    "    x=alt.X(\"condition:N\", title=\"Condition\"),\n",
    "    y=alt.Y(\"count:Q\", title=\"Number of tracks\"),\n",
    "    color=alt.Color(\n",
    "        \"condition:N\",\n",
    "        scale=alt.Scale(range=[\"#ba68c8\", \"#e1bee7\", \"#9c27b0\"])  # primi 3 della tua palette\n",
    "    ),\n",
    "    tooltip=[\"condition\", \"count\"]\n",
    ").properties(\n",
    "    title=\"Popularity Out of Range Check\",\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "chart"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aea31f3e31230479",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary statistics\n",
    "df['popularity'].describe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f95fa10c89f512c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zero popularity analysis\n",
    "zero_count = (df['popularity'] == 0).sum()\n",
    "zero_perc = (zero_count / len(df)) * 100\n",
    "\n",
    "print(\"Popularity == 0:\", zero_count)\n",
    "print(f\"Percentage of zeros: {zero_perc:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba8d8265512d9da7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Popularity vs Modified Popularity\n",
    "\n",
    "Goal: to verify whether songs with `modified_popularity = 1` (adjusted popularity scores)  \n",
    "show a different distribution of `popularity` than unmodified songs.  \n",
    "\n",
    "We analyze:\n",
    "- Distribution through boxplots and violin plots  \n",
    "- Summary statistics  \n",
    "- Non-parametric test (Mann–Whitney U)  \n",
    "- Effect size (Cohen’s d)"
   ]
  },
  {
   "cell_type": "code",
   "id": "182a25f2895293b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ensure numeric conversion\n",
    "df[\"popularity\"] = pd.to_numeric(df[\"popularity\"], errors='coerce')\n",
    "df[\"stats_pageviews\"] = pd.to_numeric(df.get(\"stats_pageviews\", np.nan), errors='coerce')\n",
    "\n",
    "# Clean modified_popularity column (it may contain strings like 'True', 'Yes', etc.)\n",
    "if \"modified_popularity\" in df.columns:\n",
    "    df[\"modified_popularity\"] = (\n",
    "        df['modified_popularity']\n",
    "        .astype(str).str.strip().str.lower()\n",
    "        .map({'1':1,'true':1,'yes':1,'y':1,'t':1,'0':0,'false':0,'no':0,'n':0,'f':0})\n",
    "        .fillna(df['modified_popularity'].apply(lambda x: 1 if x in [1, True] else 0))\n",
    "        .astype('int64')\n",
    "    )\n",
    "else:\n",
    "    df['modified_popularity'] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39f6bc94c38ca863",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's check if all the out of range popularity have modified_popularity equal 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbf744ab63af96a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Identify rows where popularity is out of the expected range (0–100)\n",
    "out_of_range = df[(df['popularity'] < 0) | (df['popularity'] > 100)]\n",
    "\n",
    "print(f\"Found {len(out_of_range)} rows with invalid popularity values:\\n\")\n",
    "display(out_of_range[['popularity', 'modified_popularity']].head(50))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "110d268581f3f37f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "colors = [\"#e1bee7\", \"#9c27b0\"]\n",
    "\n",
    "# Violin plot (KDE workaround)\n",
    "violin = (\n",
    "    alt.Chart(df)\n",
    "    .transform_density(\n",
    "        \"popularity\",\n",
    "        as_=[\"popularity\", \"density\"],\n",
    "        groupby=[\"modified_popularity\"]\n",
    "    )\n",
    "    .mark_area(orient=\"horizontal\")\n",
    "    .encode(\n",
    "        y=alt.Y(\"popularity:Q\", title=\"Popularity\"),\n",
    "        x=alt.X(\"density:Q\", stack=\"center\", title=None),\n",
    "        color=alt.Color(\"modified_popularity:N\", scale=alt.Scale(range=colors))\n",
    "    )\n",
    "    .properties(width=300, title=\"Violin plot — Popularity by Modified\")\n",
    ")\n",
    "\n",
    "# Boxplot\n",
    "box = (\n",
    "    alt.Chart(df)\n",
    "    .mark_boxplot(size=70)\n",
    "    .encode(\n",
    "        x=alt.X(\"modified_popularity:N\", title=\"modified_popularity\"),\n",
    "        y=alt.Y(\"popularity:Q\", title=\"Popularity\"),\n",
    "        color=alt.Color(\"modified_popularity:N\", scale=alt.Scale(range=colors))\n",
    "    )\n",
    "    .properties(width=200, title=\"Boxplot — Popularity by Modified\")\n",
    ")\n",
    "\n",
    "alt.hconcat(violin, box).resolve_scale(color=\"shared\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7abe2f59d20ea38a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tracks that show inconsistent popularity across duplicates tend to be more popular and exhibit greater variability in popularity values than tracks without inconsistencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b532a1b415e80ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Language\n",
    "\n",
    "We analyzed the language attribute to understand how tracks are distributed across different languages and to check whether language labeling is consistent within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "7a78ac408cef5b46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check type of popularity\n",
    "print(\"\\nLanguage dtype:\")\n",
    "print(df['language'].dtype)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c4b5455382b500d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Valori unici presenti nella colonna\n",
    "df['language'].unique()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6da59738870d47e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conteggio lingue includendo i NaN\n",
    "lang_counts = df['language'].value_counts(dropna=False).reset_index()\n",
    "lang_counts.columns = ['language', 'count']\n",
    "\n",
    "# Plot\n",
    "alt.Chart(lang_counts).mark_bar().encode(\n",
    "    x=alt.X('count:Q', title='Number of Tracks'),\n",
    "    y=alt.Y('language:N', sort='-x', title='Language'),\n",
    "    color=alt.Color(\n",
    "        'language:N',\n",
    "        scale=alt.Scale(range=[\"#f3e5f5\", \"#e1bee7\", \"#ce93d8\", \"#ba68c8\", \"#9c27b0\"])\n",
    "    ),\n",
    "    tooltip=['language', 'count']\n",
    ").properties(\n",
    "    title=\"Tracks per Language (Including NaN)\",\n",
    "    width=600,\n",
    "    height=300\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c24cf94daf24f68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The language attribute initially contained many invalid or inconsistent values (noise, non-ISO codes, and missing entries). After filtering the dataset and keeping only valid language codes, we observed that the majority of tracks are in Italian, with English representing a small secondary portion. This confirms that the dataset is strongly focused on the Italian music market.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "75af8d511b9379e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "total = len(df)\n",
    "it_count = len(df[df['language'] == 'it'])\n",
    "nan_count = len(df[df['language'].isna()])\n",
    "non_it_count = total - it_count - nan_count\n",
    "\n",
    "print(f\"Total songs: {total}\")\n",
    "print(f\"IT songs: {it_count}\")\n",
    "print(f\"NaN (unknown language): {nan_count}\")\n",
    "print(f\"Non-IT & Non-NaN songs: {non_it_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "714e7640950a7430",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "display(df[['language', 'lyrics', 'swear_EN', 'swear_IT']].head(50))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f6ae05701c8c863",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Swear Words"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c8b58585add70af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "display(df[['swear_IT', 'swear_IT_words']].head(50))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e9cb98d98fce641",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "display(df[['swear_EN', 'swear_EN_words']].head(50))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e71c1b0c88e53d9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Check the counts of swear word IT/EN\n",
    "\n",
    "To check if there are some errors I am going to use a model to check if the number of swear words is the same of the one reported."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5a32eaf3be7c1d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "IT_COUNT_COL = \"swear_IT\"\n",
    "EN_COUNT_COL = \"swear_EN\"\n",
    "IT_WORDS_COL = \"swear_IT_words\"\n",
    "EN_WORDS_COL = \"swear_EN_words\"\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "\n",
    "def normalize_token(tok: str) -> str:\n",
    "    tok = str(tok).lower()\n",
    "    # compress crazy repetitions: \"fuuuuuck\" -> \"fuuck\"\n",
    "    tok = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", tok)\n",
    "    return tok\n",
    "\n",
    "def parse_swear_list(value):\n",
    "    \"\"\"\n",
    "    Robust parser for swear_*_words.\n",
    "    Supports:\n",
    "      - real Python lists: ['cazzo', 'figa']\n",
    "      - strings that LOOK like lists: \"['cazzo', 'figa']\"\n",
    "      - simple strings: \"cazzo, figa\", \"cazzo|figa\", \"cazzo figa\"\n",
    "    \"\"\"\n",
    "    # Case 1: already a list/tuple/set\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return [normalize_token(v) for v in value if str(v).strip()]\n",
    "\n",
    "    # Case 2: not a string -> nothing\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return []\n",
    "\n",
    "    s = value.strip()\n",
    "\n",
    "    # Case 3: looks like \"['a', 'b']\" or similar\n",
    "    if (s[0] in \"[(\" and s[-1] in \"])\") or (s.startswith(\"{\") and s.endswith(\"}\")):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, (list, tuple, set)):\n",
    "                return [normalize_token(v) for v in parsed if str(v).strip()]\n",
    "        except Exception:\n",
    "            # fall back to regex split if literal_eval fails\n",
    "            pass\n",
    "\n",
    "    # Case 4: generic split on separators\n",
    "    parts = re.split(r\"[,\\|;/\\s]+\", s)\n",
    "    return [normalize_token(p) for p in parts if p]\n",
    "\n",
    "def safe_int(x):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, str) and x.strip() == \"\":\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def count_swears_in_lyrics(lyrics, swear_list):\n",
    "    \"\"\"\n",
    "    Count how many times words in swear_list appear in lyrics.\n",
    "    Returns (total_count, {word: count})\n",
    "    \"\"\"\n",
    "    if not isinstance(lyrics, str) or not swear_list:\n",
    "        return 0, {}\n",
    "\n",
    "    tokens = TOKEN_PATTERN.findall(lyrics.lower())\n",
    "    norm_tokens = [normalize_token(t) for t in tokens]\n",
    "\n",
    "    target_set = set(swear_list)\n",
    "    detail = {}\n",
    "    for t in norm_tokens:\n",
    "        if t in target_set:\n",
    "            detail[t] = detail.get(t, 0) + 1\n",
    "\n",
    "    total = sum(detail.values())\n",
    "    return total, detail\n",
    "\n",
    "\n",
    "it_auto_counts = []\n",
    "it_auto_details = []\n",
    "it_matches = []\n",
    "\n",
    "en_auto_counts = []\n",
    "en_auto_details = []\n",
    "en_matches = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    lyrics = row.get(\"lyrics\", \"\")\n",
    "\n",
    "    # ----- ITALIAN SWEARS -----\n",
    "    it_targets = parse_swear_list(row.get(IT_WORDS_COL, \"\"))\n",
    "    it_declared = safe_int(row.get(IT_COUNT_COL))\n",
    "    it_total, it_detail = count_swears_in_lyrics(lyrics, it_targets)\n",
    "\n",
    "    if it_declared is not None and it_targets:\n",
    "        it_ok = (it_declared == it_total)\n",
    "    else:\n",
    "        it_ok = None\n",
    "\n",
    "    # ----- ENGLISH SWEARS -----\n",
    "    en_targets = parse_swear_list(row.get(EN_WORDS_COL, \"\"))\n",
    "    en_declared = safe_int(row.get(EN_COUNT_COL))\n",
    "    en_total, en_detail = count_swears_in_lyrics(lyrics, en_targets)\n",
    "\n",
    "    if en_declared is not None and en_targets:\n",
    "        en_ok = (en_declared == en_total)\n",
    "    else:\n",
    "        en_ok = None\n",
    "\n",
    "    # Debug print: only if there is something to check\n",
    "    if idx % 500 == 0 and (it_targets or en_targets or it_declared not in (None, 0) or en_declared not in (None, 0)):\n",
    "        print(f\"[{idx}] IT targets={it_targets} declared={it_declared} found={it_total} match={it_ok}\")\n",
    "        if it_detail:\n",
    "            print(f\"       IT detail={it_detail}\")\n",
    "        print(f\"[{idx}] EN targets={en_targets} declared={en_declared} found={en_total} match={en_ok}\")\n",
    "        if en_detail:\n",
    "            print(f\"       EN detail={en_detail}\")\n",
    "\n",
    "    it_auto_counts.append(it_total)\n",
    "    it_auto_details.append(it_detail)\n",
    "    it_matches.append(it_ok)\n",
    "\n",
    "    en_auto_counts.append(en_total)\n",
    "    en_auto_details.append(en_detail)\n",
    "    en_matches.append(en_ok)\n",
    "\n",
    "df[\"swear_IT_auto_count\"] = it_auto_counts\n",
    "df[\"swear_IT_auto_detail\"] = it_auto_details\n",
    "df[\"swear_IT_match\"] = it_matches\n",
    "\n",
    "df[\"swear_EN_auto_count\"] = en_auto_counts\n",
    "df[\"swear_EN_auto_detail\"] = en_auto_details\n",
    "df[\"swear_EN_match\"] = en_matches\n",
    "\n",
    "# ---- STATS: only rows where we have both declared count AND target words ----\n",
    "\n",
    "it_valid = df.apply(\n",
    "    lambda r: (safe_int(r.get(IT_COUNT_COL)) is not None) and bool(parse_swear_list(r.get(IT_WORDS_COL, \"\"))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "en_valid = df.apply(\n",
    "    lambda r: (safe_int(r.get(EN_COUNT_COL)) is not None) and bool(parse_swear_list(r.get(EN_WORDS_COL, \"\"))),\n",
    "    axis=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6926875531f73ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"\\n=== ITALIAN SWEAR COUNT CONSISTENCY ===\")\n",
    "print(f\"Rows with valid Italian annotations: {it_valid.sum()}\")\n",
    "print(f\"Rows with correct Italian counts: {(df.loc[it_valid, 'swear_IT_match'] == True).sum()}\")\n",
    "print(f\"Rows with WRONG Italian counts: {(df.loc[it_valid, 'swear_IT_match'] == False).sum()}\")\n",
    "\n",
    "print(\"\\n=== ENGLISH SWEAR COUNT CONSISTENCY ===\")\n",
    "print(f\"Rows with valid English annotations: {en_valid.sum()}\")\n",
    "print(f\"Rows with correct English counts: {(df.loc[en_valid, 'swear_EN_match'] == True).sum()}\")\n",
    "print(f\"Rows with WRONG English counts: {(df.loc[en_valid, 'swear_EN_match'] == False).sum()}\")\n",
    "\n",
    "\n",
    "display(df[['swear_IT', 'swear_EN', 'swear_IT_auto_count', 'swear_EN_auto_count']].head(50))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a036e13df7d70688",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are no errors. We can remove all the coloums."
   ]
  },
  {
   "cell_type": "code",
   "id": "5229deebb632a3e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df.drop(columns=['swear_IT_auto_count'], inplace=True)\n",
    "df.drop(columns=['swear_IT_auto_detail'], inplace=True)\n",
    "df.drop(columns=['swear_IT_match'], inplace=True)\n",
    "\n",
    "df.drop(columns=['swear_EN_auto_count'], inplace=True)\n",
    "df.drop(columns=['swear_EN_auto_detail'], inplace=True)\n",
    "df.drop(columns=['swear_EN_match'], inplace=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1658bcdf997a13eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbde77af448850f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"Analysis of 'Strange' Short Lyrics \")\n",
    "\n",
    "# Inspect the 73 'Empty' Lyrics\n",
    "# These are rows where lyrics is NOT missing, but n_tokens IS missing.\n",
    "# This happens when lyrics is an empty string \"\" or similar.\n",
    "empty_lyrics = df[df['lyrics'].notna() & df['n_tokens'].isna()]\n",
    "\n",
    "print(f\"\\nFound {empty_lyrics.shape[0]} rows with 'empty' lyrics (non-NaN lyrics but NaN n_tokens).\")\n",
    "if not empty_lyrics.empty:\n",
    "    print(\"These are 'strange values' that need to be cleaned.\")\n",
    "\n",
    "    display(empty_lyrics[['id', 'full_title', 'lyrics', 'n_tokens']].head(20))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Inspect 'Very Short' (but not empty) Lyrics\n",
    "short_lyrics = df[(df['n_tokens'] > 0) & (df['n_tokens'] < 20)]\n",
    "\n",
    "print(f\"\\nFound {short_lyrics.shape[0]} songs with very short lyrics.\")\n",
    "if not short_lyrics.empty:\n",
    "    print(\"Inspecting these 'strange' short lyrics:\")\n",
    "    display(short_lyrics[['id', 'full_title', 'lyrics', 'n_tokens', 'lexical_density']].sort_values('n_tokens'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34fae8f95d77b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for index, row in short_lyrics.iterrows():\n",
    "\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "\n",
    "    # Print the title for context, if present\n",
    "    if 'full_title' in row:\n",
    "        print(f\"TITLE: {row['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in row:\n",
    "        print(f\"Tokens: {row['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    full_text = row['lyrics']\n",
    "    print(full_text)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"End. Displayed {len(short_lyrics)} lyrics.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7412383193f53d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Analysis of 'Strange' Long Lyrics (Outliers)\")\n",
    "\n",
    "# Get the 99th percentile to define \"very long\"\n",
    "q99 = df['n_tokens'].quantile(0.99)\n",
    "print(f\"The 99th percentile for n_tokens is: {q99:.0f} tokens.\")\n",
    "\n",
    "# Inspect songs above this threshold\n",
    "long_lyrics = df[df['n_tokens'] > q99]\n",
    "\n",
    "print(f\"\\nFound {long_lyrics.shape[0]} songs with more than {q99:.0f} tokens.\")\n",
    "if not long_lyrics.empty:\n",
    "    print(\"These are the statistical outliers (long lyrics):\")\n",
    "    display(long_lyrics[['id', 'full_title', 'n_tokens', 'lexical_density', 'swear_IT']].sort_values('n_tokens', ascending=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1e044678b37d6d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for index, row in long_lyrics.iterrows():\n",
    "\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "\n",
    "    # Print the title for context, if present\n",
    "    if 'full_title' in row:\n",
    "        print(f\"TITLE: {row['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in row:\n",
    "        print(f\"Tokens: {row['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    full_text = row['lyrics']\n",
    "    print(full_text)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"End. Displayed {len(long_lyrics)} lyrics.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1315935545ef1f4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Analysis of 'Strange' Content (Placeholders) \")\n",
    "\n",
    "# Define common placeholder patterns\n",
    "placeholder_patterns = r\"Contributors|Contributor|Lyrics|COMING SOON|instrumental\"\n",
    "\n",
    "# Search for lyrics containing these patterns\n",
    "placeholder_lyrics = df[df['lyrics'].str.contains(placeholder_patterns, case=False, na=False, regex=True)]\n",
    "\n",
    "print(f\"\\nFound {placeholder_lyrics.shape[0]} songs containing placeholder text.\")\n",
    "\n",
    "patterns_to_check = {\n",
    "    \"Contributor(s)\": r\"Contributor\"\n",
    "}\n",
    "\n",
    "print(\"\\nIndividual placeholder counts (a song can be in multiple categories):\")\n",
    "for label, pattern in patterns_to_check.items():\n",
    "    # Count how many lyrics contain the specific pattern\n",
    "    keyword_count = df[df['lyrics'].str.contains(pattern, case=False, na=False, regex=True)].shape[0]\n",
    "    print(f\"  - Songs containing '{label}': {keyword_count}\")\n",
    "\n",
    "if not placeholder_lyrics.empty:\n",
    "    print(\"Inspecting these 'strange' placeholder lyrics:\")\n",
    "    display(placeholder_lyrics[['id', 'full_title', 'lyrics', 'n_tokens']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8c25900c8ee934",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for index, row in placeholder_lyrics.iterrows():\n",
    "\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "\n",
    "    # Print the title for context, if present\n",
    "    if 'full_title' in row:\n",
    "        print(f\"TITLE: {row['full_title']}\")\n",
    "\n",
    "    if 'n_tokens' in row:\n",
    "        print(f\"Tokens: {row['n_tokens']}\")\n",
    "\n",
    "    print(f\"----------------------------------------------\")\n",
    "\n",
    "    full_text = row['lyrics']\n",
    "    print(full_text)\n",
    "\n",
    "print(f\"\\n==============================================\")\n",
    "print(f\"End. Displayed {len(placeholder_lyrics)} lyrics.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "352199513e23b16",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Intro"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b880d80ad853ac36"
  },
  {
   "cell_type": "code",
   "source": [
    "# Specific search for keywords\n",
    "intro_pattern = 'Intro|Interlude|Outro|Prequel|Introduzione'\n",
    "keywords_list = intro_pattern.split('|') # Create a list of the keywords\n",
    "\n",
    "# Find all rows that match *any* of the keywords\n",
    "intro_songs = df[df['full_title'].str.contains(intro_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\n--- Analysis of Intro/Outro/Interlude Keywords ---\")\n",
    "print(f\"Total rows matching at least one keyword: {len(intro_songs)}\")\n",
    "\n",
    "# --- New section for individual counts ---\n",
    "print(\"\\nIndividual keyword counts (a song can be in multiple categories):\")\n",
    "for keyword in keywords_list:\n",
    "    # Count how many titles contain the specific keyword\n",
    "    keyword_count = df[df['full_title'].str.contains(keyword, case=False, na=False)].shape[0]\n",
    "    print(f\"  - Titles containing '{keyword}': {keyword_count}\")\n",
    "# --- End of new section ---\n",
    "\n",
    "# Display the first 20 matches, as before\n",
    "if not intro_songs.empty:\n",
    "    print(\"\\nDisplaying first 20 matches:\")\n",
    "    display(\n",
    "        intro_songs[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'album_type']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "703cbb500441cf10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "intro_lyrics = intro_songs.sort_values(by='n_tokens', ascending=True)\n",
    "\n",
    "print(\"Inspecting the intro songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in intro_lyrics.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bd41dcf82afbf60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of Sentence and Number of Tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b10c1154bb83edf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Number of tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e92a52cc1049c85"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 7.50\n",
    "print(f\"Using threshold from your IQR analysis: n_tokens < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['n_tokens'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} tokens) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics (skits, interludes, or real short songs):\")\n",
    "    # We display the same columns you requested\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3db747640e1e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "# We use the 979.50 threshold from your IQR analysis for 'n_tokens'\n",
    "word_threshold_upper = 979.50\n",
    "print(f\"Using upper threshold from your IQR analysis: n_tokens > {word_threshold_upper}\")\n",
    "\n",
    "long_lyrics_mask = df['n_tokens'] > word_threshold_upper\n",
    "\n",
    "long_lyrics_df = df[long_lyrics_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} tokens):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 157:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis (157 outliers).\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis (157).\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers (sorted by n_tokens):\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms']]\n",
    "        .sort_values('n_tokens', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a33e80b7b2c409ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort the very short songs by n_tokens (from lowest to highest)\n",
    "long_songs = long_lyrics_df.sort_values(by='n_tokens', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "# Print the full text of the first 5 (Note: .iterrows() will iterate all, not just 5)\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac23e75d47d3f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Number of Sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f26534fb403f68df"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 5.50\n",
    "print(f\"Using threshold from your IQR analysis: n_tokens < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['n_sentences'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} sentences) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics:\")\n",
    "\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "373fe5140647ec03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "short_songs = short_lyrics_clean.sort_values(by=['n_sentences', 'n_tokens'], ascending=True)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b61b3d915b08b81",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "word_threshold_upper = 113.50\n",
    "print(f\"Using upper threshold from your IQR analysis: n_sentences > {word_threshold_upper}\")\n",
    "\n",
    "long_lyrics_mask = df['n_sentences'] > word_threshold_upper\n",
    "\n",
    "long_lyrics_df = df[long_lyrics_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} sentences):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 205:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis.\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis.\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers :\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'n_sentences']]\n",
    "        .sort_values('n_sentences', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3effd4dc032067",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "long_songs = long_lyrics_df.sort_values(by='n_sentences', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afb38a5db3cd787e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Coerence Check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87bddbc3c9990d18"
  },
  {
   "cell_type": "code",
   "source": [
    "# Auto counts\n",
    "n_sent_auto = []\n",
    "n_tok_auto = []\n",
    "\n",
    "for idx, lyrics in enumerate(df[\"lyrics\"]):\n",
    "    ns = functions.count_sentences(lyrics)\n",
    "    nt = functions.count_tokens(lyrics)\n",
    "\n",
    "    # Debug every 500 rows\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"[{idx}] auto_sentences={ns} auto_tokens={nt}\")\n",
    "\n",
    "    n_sent_auto.append(ns)\n",
    "    n_tok_auto.append(nt)\n",
    "\n",
    "df[\"n_sentences_auto\"] = n_sent_auto\n",
    "df[\"n_tokens_auto\"] = n_tok_auto\n",
    "\n",
    "# Match flags (only where ground truth is present)\n",
    "if \"n_sentences\" in df.columns:\n",
    "    df[\"n_sentences_match\"] = df.apply(\n",
    "        lambda r: (\n",
    "            safe_int(r[\"n_sentences\"]) == r[\"n_sentences_auto\"]\n",
    "            if safe_int(r[\"n_sentences\"]) is not None\n",
    "            else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    print(\"WARNING: column 'n_sentences' not found in dataframe.\")\n",
    "\n",
    "if \"n_tokens\" in df.columns:\n",
    "    df[\"n_tokens_match\"] = df.apply(\n",
    "        lambda r: (\n",
    "            safe_int(r[\"n_tokens\"]) == r[\"n_tokens_auto\"]\n",
    "            if safe_int(r[\"n_tokens\"]) is not None\n",
    "            else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    print(\"WARNING: column 'n_tokens' not found in dataframe.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdcfcef11c727a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if \"n_sentences\" in df.columns:\n",
    "    sent_valid_mask = df[\"n_sentences\"].apply(lambda x: functions.safe_int(x) is not None)\n",
    "    print(\"\\n=== SENTENCE COUNT CONSISTENCY ===\")\n",
    "    print(f\"Rows with declared n_sentences: {sent_valid_mask.sum()}\")\n",
    "    print(f\"Rows with correct n_sentences: {(df.loc[sent_valid_mask, 'n_sentences_match'] == True).sum()}\")\n",
    "    print(f\"Rows with WRONG n_sentences: {(df.loc[sent_valid_mask, 'n_sentences_match'] == False).sum()}\")\n",
    "\n",
    "if \"n_tokens\" in df.columns:\n",
    "    tok_valid_mask = df[\"n_tokens\"].apply(lambda x: functions.safe_int(x) is not None)\n",
    "    print(\"\\n=== TOKEN COUNT CONSISTENCY ===\")\n",
    "    print(f\"Rows with declared n_tokens: {tok_valid_mask.sum()}\")\n",
    "    print(f\"Rows with correct n_tokens: {(df.loc[tok_valid_mask, 'n_tokens_match'] == True).sum()}\")\n",
    "    print(f\"Rows with WRONG n_tokens: {(df.loc[tok_valid_mask, 'n_tokens_match'] == False).sum()}\")\n",
    "\n",
    "\n",
    "display(df[['n_sentences', 'n_tokens', 'n_sentences_auto', 'n_tokens_auto']].head(50))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac98c4e1cc7bbdcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"We will use 'n_sentences_auto' as it is our validated column.\")\n",
    "\n",
    "# Analysis of 'Strange' Low Sentence Counts\n",
    "print(\"Analysis of 'Strange' Low Sentence Counts\")\n",
    "\n",
    "broken_mask = df['n_tokens'].isna()\n",
    "low_sentences_df = df[(df['n_sentences_auto'] <= 1) & ~broken_mask]\n",
    "\n",
    "print(f\"\\nFound {low_sentences_df.shape[0]} songs with 0 or 1 sentence (excluding 'broken' rows).\")\n",
    "print(\"Hypothesis: These are the 'junk' placeholder lyrics (e.g., '1 Contributor Lyrics').\")\n",
    "\n",
    "if not low_sentences_df.empty:\n",
    "    print(\"Inspecting a sample of these 'low sentence' lyrics:\")\n",
    "    display(low_sentences_df[['id', 'full_title', 'lyrics', 'n_sentences_auto', 'n_tokens_auto']].head(10))\n",
    "else:\n",
    "    print(\"No 'low sentence' outliers found (excluding broken rows).\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f351a30a4bdb5322",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "print(\"Cross-Analysis: Low Sentences vs. Placeholders\")\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Define Group A: Placeholders\n",
    "placeholder_patterns = r\"Contributors|Contributor|Lyrics|COMING SOON|instrumental\"\n",
    "placeholder_mask = df['lyrics'].str.contains(placeholder_patterns, case=False, na=False, regex=True)\n",
    "total_placeholders = placeholder_mask.sum()\n",
    "\n",
    "# Define Group B: Low Sentences\n",
    "broken_mask = df['n_tokens'].isna()\n",
    "low_sent_mask = (df['n_sentences_auto'] <= 1) & ~broken_mask\n",
    "total_low_sentences = low_sent_mask.sum()\n",
    "\n",
    "# Calculate Intersection (Venn Diagram)\n",
    "overlap_mask = placeholder_mask & low_sent_mask\n",
    "overlap_count = overlap_mask.sum()\n",
    "\n",
    "low_sent_only_mask = low_sent_mask & ~placeholder_mask\n",
    "low_sent_only_count = low_sent_only_mask.sum()\n",
    "\n",
    "placeholder_only_mask = placeholder_mask & ~low_sent_mask\n",
    "placeholder_only_count = placeholder_only_mask.sum()\n",
    "\n",
    "# Print Text Results\n",
    "print(f\"Total songs with placeholders (Group A): {total_placeholders}\")\n",
    "print(f\"Total songs with 0 or 1 sentence (Group B): {total_low_sentences}\")\n",
    "print(\"\\n--- Venn Diagram Results ---\")\n",
    "print(f\"Songs IN BOTH Group A AND Group B (Overlap - 'Junk'): {overlap_count}\")\n",
    "print(f\"Songs with low sentences BUT NO placeholder (Group B only - 'Strange'): {low_sent_only_count}\")\n",
    "print(f\"Songs with placeholders BUT > 1 sentence (Group A only - 'Contaminated'): {placeholder_only_count}\")\n",
    "\n",
    "#  Visualization\n",
    "print(\"Visualizing the Overlap\")\n",
    "\n",
    "plot_data = pd.DataFrame([\n",
    "    {'category': f'Placeholders Only (Contaminated)',\n",
    "     'description': '\"Contaminated\" lyrics (> 1 sentence)',\n",
    "     'count': placeholder_only_count},\n",
    "    {'category': f'Low Sentences Only (Strange)',\n",
    "     'description': '\"Strange\" lyrics but not placeholders (<= 1 sentence)',\n",
    "     'count': low_sent_only_count},\n",
    "    {'category': f'Overlap (Junk)',\n",
    "     'description': '\"Junk\" lyrics (placeholder AND <= 1 sentence)',\n",
    "     'count': overlap_count}\n",
    "])\n",
    "\n",
    "# Create the Altair bar chart\n",
    "chart = alt.Chart(plot_data).mark_bar().encode(\n",
    "    x=alt.X('category:N', title='Category (Venn Analysis)', sort='-y'), # Sort by count\n",
    "    y=alt.Y('count:Q', title='Number of Songs'),\n",
    "    color=alt.Color('category:N', title='Category'),\n",
    "    tooltip=['category', 'description', 'count']\n",
    ").properties(\n",
    "    title='Cross-Analysis: Placeholders vs. Short Sentences'\n",
    ")\n",
    "\n",
    "# Add numeric labels above the bars\n",
    "text = chart.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=-5  # Move text 5 pixels above the bar\n",
    ").encode(\n",
    "    text='count:Q'\n",
    ")\n",
    "\n",
    "final_chart = (chart + text).interactive()\n",
    "\n",
    "\n",
    "final_chart"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ec9ce27c04f8a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cross-analysis confirms that our two \"low-quality\" data groups are highly related but distinct.\n",
    "\n",
    "The 114-song overlap (Overlap (Junk)) confirms that the vast majority of tracks with n_sentences_auto <= 1 are \"junk\" data (e.g., \"1 Contributor Lyrics\"). These will be removed.\n",
    "\n",
    "The 41 Placeholders Only songs are \"contaminated\" tracks. As noted, these contain valid lyrics mixed with placeholder text and will be cleaned (not removed).\n",
    "\n",
    "The 7 Low Sentences Only songs are \"strange\" but valid outliers (e.g., short interludes) that will also be removed as they lack sufficient content for analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97621aca0c969d4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number Token per  Sentence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691d15854c35aadf"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 4.19\n",
    "print(f\"Using threshold from your IQR analysis: tokens_per_sent < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['tokens_per_sent'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} token per sentence) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics:\")\n",
    "    # We display the same columns you requested\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences', 'tokens_per_sent']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98dcd091eadc9a37",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "short_songs = short_lyrics_clean.sort_values(by='tokens_per_sent', ascending=True)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Token per sentence: {row['tokens_per_sent']}\")\n",
    "    print(f\"Duration (ms): {row['duration_ms']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6ec493dcdc35abb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "word_threshold_upper = 12.58\n",
    "print(f\"Using upper threshold from your IQR analysis: tokens_per_sent > {word_threshold_upper}\")\n",
    "\n",
    "long_lyrics_mask = df['tokens_per_sent'] > word_threshold_upper\n",
    "\n",
    "long_lyrics_df = df[long_lyrics_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} token per sentence):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 283:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis.\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis.\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers :\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'n_sentences', 'tokens_per_sent']]\n",
    "        .sort_values('tokens_per_sent', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce1e4c7dbaab27d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "long_songs = long_lyrics_df.sort_values(by='tokens_per_sent', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Token per sent: {row['tokens_per_sent']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da8920e0b99dc40a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Coerence Check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c891dd291b47e35a"
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Recalculating 'tokens_per_sent_auto' using n_tokens_auto / n_sentences_auto...\")\n",
    "\n",
    "df['tokens_per_sent_auto'] = np.where(\n",
    "    df['n_sentences_auto'] == 0,\n",
    "    0,  # Imposta a 0 se n_sentences_auto è 0\n",
    "    df['n_tokens_auto'] / df['n_sentences_auto']\n",
    ")\n",
    "\n",
    "print(\"Comparing new 'tokens_per_sent_auto' with original 'tokens_per_sent'...\")\n",
    "\n",
    "if \"tokens_per_sent\" in df.columns:\n",
    "\n",
    "    valid_mask = df['tokens_per_sent'].notna()\n",
    "\n",
    "    match_series = np.isclose(\n",
    "        df.loc[valid_mask, 'tokens_per_sent'],\n",
    "        df.loc[valid_mask, 'tokens_per_sent_auto'],\n",
    "        atol=0.01\n",
    "    )\n",
    "\n",
    "    df['tokens_per_sent_match'] = pd.NA\n",
    "    df.loc[valid_mask, 'tokens_per_sent_match'] = match_series\n",
    "\n",
    "    print(\"\\n=== tokens_per_sent CONSISTENCY ===\")\n",
    "    print(f\"Rows with declared tokens_per_sent: {valid_mask.sum()}\")\n",
    "    print(f\"Rows with correct tokens_per_sent (within 0.01 tolerance): {(df['tokens_per_sent_match'] == True).sum()}\")\n",
    "    print(f\"Rows with WRONG tokens_per_sent: {(df['tokens_per_sent_match'] == False).sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"WARNING: column 'tokens_per_sent' not found in dataframe.\")\n",
    "\n",
    "print(\"We compare the original 'tokens_per_sent' with our 'tokens_per_sent_auto'.\")\n",
    "display(df[[\n",
    "    'tokens_per_sent', 'tokens_per_sent_auto', 'tokens_per_sent_match',\n",
    "    'n_sentences', 'n_sentences_auto',\n",
    "    'n_tokens', 'n_tokens_auto'\n",
    "]].head(20))\n",
    "\n",
    "\n",
    "print(\"Verifying that 'tokens_per_sent_auto' for 'junk' data.\")\n",
    "#\n",
    "display(df[df['lyrics'].str.contains(r\"Contributors|COMING SOON\", na=False)][[\n",
    "    'lyrics',\n",
    "    'n_sentences_auto',\n",
    "    'n_tokens_auto',\n",
    "    'tokens_per_sent',\n",
    "    'tokens_per_sent_auto'\n",
    "]].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "757a030dbd3c5cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of char per token\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f62d89509924b9c4"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 3.41\n",
    "print(f\"Using threshold from your IQR analysis: char_per_tok < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['char_per_tok'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} char per token) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics:\")\n",
    "    # We display the same columns you requested\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences', 'tokens_per_sent', 'char_per_tok']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8c806dd54570d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "short_songs = short_lyrics_clean.sort_values(by='char_per_tok', ascending=True)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd7a0f05b3a72e5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "word_threshold_upper = 4.62\n",
    "print(f\"Using upper threshold from your IQR analysis: char_per_tok > {word_threshold_upper}\")\n",
    "\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "long_lyrics_mask = df['char_per_tok'] > word_threshold_upper\n",
    "\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = long_lyrics_mask & not_junk_mask\n",
    "\n",
    "long_lyrics_df = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} char per token):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 330:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis.\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis.\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers :\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'n_sentences', 'tokens_per_sent', 'char_per_tok' ]]\n",
    "        .sort_values('char_per_tok', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f00b80e5bd3fcdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "long_songs = long_lyrics_df.sort_values(by='char_per_tok', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Token per sent: {row['tokens_per_sent']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29fc1ef5ad0cd3b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Coerence Check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867b7c78238620ed"
  },
  {
   "cell_type": "code",
   "source": [
    "# Recalculate 'char_per_tok'\n",
    "print(\"Calculating 'total_chars_auto' (sum of all token lengths)...\")\n",
    "df['total_chars_auto'] = df['lyrics'].apply(functions.count_total_token_chars)\n",
    "\n",
    "print(\"Recalculating 'char_per_tok_auto' using total_chars_auto / n_tokens_auto...\")\n",
    "\n",
    "df['char_per_tok_auto'] = np.where(\n",
    "    df['n_tokens_auto'] == 0,\n",
    "    0,\n",
    "    df['total_chars_auto'] / df['n_tokens_auto']\n",
    ")\n",
    "\n",
    "# Compare 'char_per_tok_auto' with the original 'char_per_tok'\n",
    "print(\"Comparing new 'char_per_tok_auto' with original 'char_per_tok'...\")\n",
    "\n",
    "if \"char_per_tok\" in df.columns:\n",
    "    valid_mask = df['char_per_tok'].notna()\n",
    "\n",
    "    match_series = np.isclose(\n",
    "        df.loc[valid_mask, 'char_per_tok'],\n",
    "        df.loc[valid_mask, 'char_per_tok_auto'],\n",
    "        atol=0.01\n",
    "    )\n",
    "\n",
    "    df['char_per_tok_match'] = pd.NA\n",
    "    df.loc[valid_mask, 'char_per_tok_match'] = match_series\n",
    "\n",
    "    print(\"\\n=== char_per_tok CONSISTENCY ===\")\n",
    "    print(f\"Rows with declared char_per_tok: {valid_mask.sum()}\")\n",
    "    print(f\"Rows with correct char_per_tok (within 0.01 tolerance): {(df['char_per_tok_match'] == True).sum()}\")\n",
    "    print(f\"Rows with WRONG char_per_tok: {(df['char_per_tok_match'] == False).sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"WARNING: column 'char_per_tok' not found in dataframe.\")\n",
    "\n",
    "print(\"We compare the original 'char_per_tok' with our 'char_per_tok_auto'.\")\n",
    "display(df[[\n",
    "    'char_per_tok', 'char_per_tok_auto', 'char_per_tok_match',\n",
    "    'n_tokens', 'n_tokens_auto',\n",
    "    'total_chars_auto'\n",
    "]].head(20))\n",
    "\n",
    "print(\"Verifying 'char_per_tok_auto' for 'junk' data.\")\n",
    "display(df[df['lyrics'].str.contains(r\"Contributors|COMING SOON\", na=False)][[\n",
    "    'lyrics',\n",
    "    'n_tokens_auto',\n",
    "    'total_chars_auto',\n",
    "    'char_per_tok',\n",
    "    'char_per_tok_auto'\n",
    "]].head(5))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0174b5c0971a636",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lexical density"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289388bbe5f330f2"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 0.39\n",
    "print(f\"Using threshold from your IQR analysis: lexical_density < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['lexical_density'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} lexical density) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics:\")\n",
    "    # We display the same columns you requested\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences', 'tokens_per_sent', 'char_per_tok', 'lexical_density']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c389a1ce979c655e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "short_songs = short_lyrics_clean.sort_values(by='lexical_density', ascending=True)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(f\"Lexical density: {row['lexical_density']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d31a309535e3ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "word_threshold_upper = 0.63\n",
    "print(f\"Using upper threshold from your IQR analysis: char_per_tok > {word_threshold_upper}\")\n",
    "\n",
    "long_lyrics_mask = df['char_per_tok'] > word_threshold_upper\n",
    "\n",
    "long_lyrics_df = df[long_lyrics_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} char per token):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 318:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis.\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis.\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers :\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'n_sentences', 'tokens_per_sent', 'char_per_tok', 'lexical_density' ]]\n",
    "        .sort_values('lexical_density', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8352779fb613551d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "long_songs = long_lyrics_df.sort_values(by='lexical_density', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Token per sent: {row['tokens_per_sent']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(f\"Lexical density: {row['lexical_density']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7620c26d4505f654",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Coerence Check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ecbdb0dab0c598f"
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Calculating 'n_unique_words_auto'...\")\n",
    "df['n_unique_words_auto'] = df['lyrics'].apply(functions.count_unique_tokens)\n",
    "\n",
    "print(\"Recalculating 'lexical_density_auto' using n_unique_words_auto / n_tokens_auto...\")\n",
    "\n",
    "df['lexical_density_auto'] = np.where(\n",
    "    df['n_tokens_auto'] == 0,\n",
    "    0,\n",
    "    df['n_unique_words_auto'] / df['n_tokens_auto']\n",
    ")\n",
    "\n",
    "print(\"Comparing new 'lexical_density_auto' with original 'lexical_density'...\")\n",
    "\n",
    "if \"lexical_density\" in df.columns:\n",
    "    valid_mask = df['lexical_density'].notna()\n",
    "\n",
    "    match_series = np.isclose(\n",
    "        df.loc[valid_mask, 'lexical_density'],\n",
    "        df.loc[valid_mask, 'lexical_density_auto'],\n",
    "        atol=0.01\n",
    "    )\n",
    "\n",
    "    df['lexical_density_match'] = pd.NA\n",
    "    df.loc[valid_mask, 'lexical_density_match'] = match_series\n",
    "\n",
    "    print(\"\\n=== lexical_density CONSISTENCY ===\")\n",
    "    print(f\"Rows with declared lexical_density: {valid_mask.sum()}\")\n",
    "    print(f\"Rows with correct lexical_density (within 0.01 tolerance): {(df['lexical_density_match'] == True).sum()}\")\n",
    "    print(f\"Rows with WRONG lexical_density: {(df['lexical_density_match'] == False).sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"WARNING: column 'lexical_density' not found in dataframe.\")\n",
    "\n",
    "\n",
    "display(df[[\n",
    "    'lexical_density', 'lexical_density_auto', 'lexical_density_match',\n",
    "    'n_tokens', 'n_tokens_auto',\n",
    "    'n_unique_words_auto'\n",
    "]].head(10))\n",
    "\n",
    "print(\"Verifying 'lexical_density_auto' for 'junk' data.\")\n",
    "display(df[df['lyrics'].str.contains(r\"Contributors|COMING SOON\", na=False)][[\n",
    "    'lyrics',\n",
    "    'n_tokens_auto',\n",
    "    'n_unique_words_auto',\n",
    "    'lexical_density_auto'\n",
    "]].head(5))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "178c442999d0a7ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Average token per clause"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e69505a1c2c6560a"
  },
  {
   "cell_type": "code",
   "source": [
    "word_threshold = 2.51\n",
    "print(f\"Using threshold from your IQR analysis: avg_token_per_clause < {word_threshold}\")\n",
    "\n",
    "# The pattern of 'junk' text to exclude\n",
    "junk_pattern = 'Contributors|Contributor'\n",
    "\n",
    "short_lyrics_mask = df['avg_token_per_clause'] < word_threshold\n",
    "\n",
    "not_junk_mask = ~df['lyrics'].str.contains(junk_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "final_mask = short_lyrics_mask & not_junk_mask\n",
    "\n",
    "short_lyrics_clean = df[final_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'short lyrics' (< {word_threshold} average token per clause) AND are NOT placeholders:\")\n",
    "print(f\"Total number: {len(short_lyrics_clean)}\")\n",
    "\n",
    "if not short_lyrics_clean.empty:\n",
    "    print(\"\\nInspecting these 'clean' short lyrics:\")\n",
    "    # We display the same columns you requested\n",
    "    display(short_lyrics_clean[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'n_sentences', 'tokens_per_sent', 'char_per_tok', 'lexical_density', 'avg_token_per_clause']].head(20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f5c76d9b90e223",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "short_songs = short_lyrics_clean.sort_values(by='avg_token_per_clause', ascending=True)\n",
    "\n",
    "print(\"Inspecting the short songs\")\n",
    "\n",
    "for index, row in short_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(f\"Lexical density: {row['lexical_density']}\")\n",
    "    print(f\"Token per clause: {row['avg_token_per_clause']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2300e6b4e18db07b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"--- Inspecting 'Long Lyrics' Outliers (High-End) ---\")\n",
    "\n",
    "word_threshold_upper = 11.44\n",
    "print(f\"Using upper threshold from your IQR analysis: avg_token_per_clause > {word_threshold_upper}\")\n",
    "\n",
    "long_lyrics_mask = df['avg_token_per_clause'] > word_threshold_upper\n",
    "\n",
    "long_lyrics_df = df[long_lyrics_mask]\n",
    "\n",
    "print(f\"\\nTotal rows with 'long lyrics' (> {word_threshold_upper} average token per clause):\")\n",
    "print(f\"Total number found: {len(long_lyrics_df)}\")\n",
    "\n",
    "if len(long_lyrics_df) == 605:\n",
    "    print(\"SUCCESS: The count matches your IQR analysis.\")\n",
    "else:\n",
    "    print(f\"NOTE: The count ({len(long_lyrics_df)}) does not match your IQR analysis.\")\n",
    "    print(\"This is OK, it just means the 'df' has been cleaned since the IQR was calculated.\")\n",
    "\n",
    "if not long_lyrics_df.empty:\n",
    "    print(\"\\nInspecting the 20 longest outliers :\")\n",
    "\n",
    "    display(\n",
    "        long_lyrics_df[['id', 'name_artist', 'full_title', 'lyrics', 'n_tokens', 'duration_ms', 'n_sentences', 'tokens_per_sent', 'char_per_tok', 'lexical_density', 'avg_token_per_clause' ]]\n",
    "        .sort_values('avg_token_per_clause', ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo 'long lyrics' outliers found.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cf92bc9b611844c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "long_songs = long_lyrics_df.sort_values(by='avg_token_per_clause', ascending=False)\n",
    "\n",
    "print(\"Inspecting the long songs\")\n",
    "\n",
    "for index, row in long_songs.iterrows():\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"INDEX (original): {index}\")\n",
    "    print(f\"ARTIST: {row['name_artist']}\")\n",
    "    print(f\"TITLE: {row['full_title']}\")\n",
    "    print(f\"Tokens: {row['n_tokens']}\")\n",
    "    print(f\"Sentences: {row['n_sentences']}\")\n",
    "    print(f\"Token per sent: {row['tokens_per_sent']}\")\n",
    "    print(f\"Char per sent: {row['char_per_tok']}\")\n",
    "    print(f\"Lexical density: {row['lexical_density']}\")\n",
    "    print(f\"Token per clause: {row['avg_token_per_clause']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"LYRICS:\\n'{str(row['lyrics'])}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "748363b79ff25d31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DATA CORRELATION BEFORE CLEANING\n",
    "\n",
    " In this section we:\n",
    " - select numeric columns\n",
    " - compute Pearson and Spearman correlation matrices\n",
    " - detect highly correlated pairs (potentially redundant features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4500ca45bb82c2e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Select only numeric columns (optionally exclude IDs)\n",
    "id_like = {\"id\", \"id_track\", \"id_album\", \"id_album_clean\", \"id_artist\"}\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in id_like]\n",
    "\n",
    "print(\"Numeric columns used:\", num_cols)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e5f6dca6bd97e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pearson correlation (linear relationships)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "111694e398080fb8"
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Column filters for Pearson\n",
    "# =========================\n",
    "id_like = {\"id\", \"id_track\", \"id_album\", \"id_album_clean\", \"id_artist\"}\n",
    "\n",
    "# =========================\n",
    "# 1) Select numeric candidates (exclude id-like)\n",
    "# =========================\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in id_like]\n",
    "\n",
    "excluded = {}\n",
    "kept = []\n",
    "\n",
    "for c in num_cols:\n",
    "    s = df[c]\n",
    "    reasons = []\n",
    "    if pd.api.types.is_bool_dtype(s) or functions.is_binary_series(s):\n",
    "        reasons.append(\"binary/bool → use point-biserial (not raw Pearson)\")\n",
    "    if functions.is_low_cardinal_int(s, max_levels=5):\n",
    "        reasons.append(\"low-cardinality ordinal → prefer Spearman/Kendall\")\n",
    "    if functions.is_quasi_constant(s):\n",
    "        reasons.append(\"quasi-constant → variance ~ 0\")\n",
    "    if functions.is_extreme_zeroinflated_count(s):\n",
    "        reasons.append(\"extreme zero-inflated count\")\n",
    "    if reasons:\n",
    "        excluded[c] = \"; \".join(reasons)\n",
    "    else:\n",
    "        kept.append(c)\n",
    "\n",
    "print(\"Columns kept for Pearson:\", kept)\n",
    "if excluded:\n",
    "    print(\"\\nColumns excluded and reason:\")\n",
    "    for k, v in excluded.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Pearson correlation (only 'kept' columns)\n",
    "# =========================\n",
    "if len(kept) < 2:\n",
    "    raise ValueError(\"Too many columns excluded: fewer than 2 columns suitable for Pearson.\")\n",
    "\n",
    "pearson_corr = df[kept].corr(method=\"pearson\")\n",
    "print(\"\\nPearson correlation (filtered):\")\n",
    "print(pearson_corr)\n",
    "\n",
    "# =========================\n",
    "# 3) Strong correlations (pair list, NOT matrix)\n",
    "# =========================\n",
    "threshold = 0.9  # adjust as needed\n",
    "\n",
    "upper_mask = np.triu(np.ones(pearson_corr.shape), k=1).astype(bool)\n",
    "pairs = (\n",
    "    pearson_corr.where(upper_mask)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "pairs.columns = [\"feature_1\", \"feature_2\", \"pearson_r\"]\n",
    "\n",
    "strong_pearson = (\n",
    "    pairs[pairs[\"pearson_r\"].abs() >= threshold]\n",
    "    .sort_values(by=\"pearson_r\", key=lambda s: s.abs(), ascending=False)\n",
    ")\n",
    "print(f\"\\nStrong Pearson pairs (|r| >= {threshold}):\")\n",
    "print(strong_pearson if not strong_pearson.empty else \"None\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4129f61617ca3bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ===== Custom palette =====\n",
    "\n",
    "hex_colors = [\"#e1bee7\", \"#ce93d8\", \"#9c27b0\"]  # your colors\n",
    "cmap_custom = LinearSegmentedColormap.from_list(\"custom_purple\", hex_colors)\n",
    "\n",
    "# =========================\n",
    "# Heatmap (Pearson matrix) with custom colors\n",
    "# =========================\n",
    "# Uses the same column order as `kept`\n",
    "corr_mat = pearson_corr.loc[kept, kept]\n",
    "\n",
    "# Mask NaNs to show as empty cells\n",
    "data = np.ma.masked_invalid(corr_mat.to_numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "# TwoSlopeNorm centers the color at 0, with vmin=-1, vmax=1\n",
    "norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "im = plt.imshow(data, aspect=\"auto\", norm=norm, cmap=cmap_custom)\n",
    "plt.colorbar(im, label=\"Pearson r\")\n",
    "\n",
    "plt.xticks(ticks=np.arange(len(kept)), labels=kept, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(kept)), labels=kept)\n",
    "\n",
    "# Optional grid\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(-.5, len(kept), 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, len(kept), 1), minor=True)\n",
    "ax.grid(which=\"minor\", linestyle=\"-\", linewidth=0.5, alpha=0.25)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "plt.title(\"Pearson correlation matrix (filtered)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58dfc724f886691b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Spearman correlation (monotonic relationships, robust to outliers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f28a079652216e"
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Column filters for Spearman + Pearson vs Spearman comparison\n",
    "# =========================\n",
    "\n",
    "id_like = {\"id\", \"id_track\", \"id_album\", \"id_album_clean\", \"id_artist\"}\n",
    "\n",
    "# Variables typically circular (Spearman is not recommended for cyclic relationships)\n",
    "circular_names = {\"month\", \"day\", \"weekday\", \"dow\", \"hour\", \"minute\", \"second\"}\n",
    "\n",
    "# =========================\n",
    "# 1) Select numeric candidates for Spearman (keep ordinal/binary, exclude id-like)\n",
    "# =========================\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in id_like]\n",
    "\n",
    "excluded_s = {}\n",
    "kept_s = []\n",
    "spearman_soft_warn = {}\n",
    "\n",
    "for c in num_cols:\n",
    "    s = df[c]\n",
    "    reasons = []\n",
    "    # Hard exclusions for Spearman suitability\n",
    "    if functions.is_quasi_constant(s) or s.dropna().nunique() < 2:\n",
    "        reasons.append(\"constant/nearly-constant or <2 distinct non-null values\")\n",
    "    if c.lower() in circular_names:\n",
    "        reasons.append(\"circular variable (cyclic) — Spearman not suitable\")\n",
    "    if reasons:\n",
    "        excluded_s[c] = \"; \".join(reasons)\n",
    "        continue\n",
    "\n",
    "    # Keep the column\n",
    "    kept_s.append(c)\n",
    "\n",
    "    # Soft warnings (still kept, but flagged)\n",
    "    warns = []\n",
    "    if pd.api.types.is_integer_dtype(s) and (s >= 0).all() and (s == 0).mean() >= 0.95:\n",
    "        warns.append(\"extreme zero-inflation (many ties) — Kendall τ-b or transformations may be better\")\n",
    "    if warns:\n",
    "        spearman_soft_warn[c] = \"; \".join(warns)\n",
    "\n",
    "print(\"Columns kept for Spearman:\", kept_s)\n",
    "if excluded_s:\n",
    "    print(\"\\nColumns excluded for Spearman (hard) and reason:\")\n",
    "    for k, v in excluded_s.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "if spearman_soft_warn:\n",
    "    print(\"\\nColumns where Spearman is NOT recommended (soft warning):\")\n",
    "    for k, v in spearman_soft_warn.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Spearman correlation (only 'kept_s' columns)\n",
    "# =========================\n",
    "if len(kept_s) < 2:\n",
    "    raise ValueError(\"Too few columns for Spearman: need at least 2 usable columns.\")\n",
    "\n",
    "# Use min_periods to reduce all-NaN issues with sparse overlap\n",
    "spearman_corr = df[kept_s].corr(method=\"spearman\", min_periods=3)\n",
    "print(\"\\nSpearman correlation (filtered):\")\n",
    "print(spearman_corr)\n",
    "\n",
    "# =========================\n",
    "# 3) Strong correlations (pair list, NOT matrix)\n",
    "# =========================\n",
    "threshold = 0.9  # adjust as needed\n",
    "\n",
    "upper_mask_s = np.triu(np.ones(spearman_corr.shape), k=1).astype(bool)\n",
    "pairs_s = (\n",
    "    spearman_corr.where(upper_mask_s)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "pairs_s.columns = [\"feature_1\", \"feature_2\", \"spearman_rho\"]\n",
    "\n",
    "strong_spearman = (\n",
    "    pairs_s[pairs_s[\"spearman_rho\"].abs() >= threshold]\n",
    "    .sort_values(by=\"spearman_rho\", key=lambda s: s.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"\\nStrong Spearman pairs (|rho| >= {threshold}):\")\n",
    "print(strong_spearman if not strong_spearman.empty else \"None\")\n",
    "\n",
    "# =========================\n",
    "# 4) Pearson vs Spearman comparison on common pairs\n",
    "#    (requires you already computed `pearson_corr` and `strong_pearson`)\n",
    "# =========================\n",
    "def _pairs_from_corr(corr_df: pd.DataFrame, val_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Stack upper triangle of a correlation matrix into a tidy pair list.\"\"\"\n",
    "    mask = np.triu(np.ones(corr_df.shape), k=1).astype(bool)\n",
    "    out = (\n",
    "        corr_df.where(mask)\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "    )\n",
    "    out.columns = [\"feature_1\", \"feature_2\", val_name]\n",
    "    # Ensure consistent pair ordering (feature_1 < feature_2) to merge reliably\n",
    "    ordered = out.apply(\n",
    "        lambda r: pd.Series(sorted([r[\"feature_1\"], r[\"feature_2\"]]) + [r[val_name]]),\n",
    "        axis=1\n",
    "    )\n",
    "    ordered.columns = [\"f1\", \"f2\", val_name]\n",
    "    return ordered\n",
    "\n",
    "try:\n",
    "    # Intersect the variable sets, so we're comparing apples to apples\n",
    "    common_cols = sorted(set(pearson_corr.columns).intersection(set(spearman_corr.columns)))\n",
    "    if len(common_cols) >= 2:\n",
    "        p_pairs = _pairs_from_corr(pearson_corr.loc[common_cols, common_cols], \"pearson_r\")\n",
    "        s_pairs = _pairs_from_corr(spearman_corr.loc[common_cols, common_cols], \"spearman_rho\")\n",
    "        comp = p_pairs.merge(s_pairs, on=[\"f1\", \"f2\"], how=\"inner\")\n",
    "        comp[\"abs_delta\"] = (comp[\"pearson_r\"] - comp[\"spearman_rho\"]).abs()\n",
    "        comp[\"sign_disagreement\"] = np.sign(comp[\"pearson_r\"]) != np.sign(comp[\"spearman_rho\"])\n",
    "\n",
    "        # Sort by absolute delta descending\n",
    "        comp_sorted = comp.sort_values(\"abs_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        print(\"\\nPearson vs Spearman comparison (common pairs):\")\n",
    "        print(comp_sorted.head(30).to_string(index=False))\n",
    "\n",
    "        # Optional: also show only sign disagreements\n",
    "        comp_sign_flip = comp_sorted[comp_sorted[\"sign_disagreement\"]]\n",
    "        if not comp_sign_flip.empty:\n",
    "            print(\"\\nPairs with SIGN DISAGREEMENT between Pearson and Spearman:\")\n",
    "            print(comp_sign_flip.head(30).to_string(index=False))\n",
    "\n",
    "        # Expose for later use (e.g., plotting)\n",
    "        comparison_table = comp_sorted.copy()\n",
    "    else:\n",
    "        print(\"\\nNot enough common columns for comparison.\")\n",
    "except NameError:\n",
    "    print(\"\\n`pearson_corr` not found. Run the Pearson block first to enable the comparison.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffae70bcc9207ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom palette\n",
    "cmap_custom = LinearSegmentedColormap.from_list(\"custom_purple\", hex_colors)\n",
    "\n",
    "# Ensure same column order as kept_s\n",
    "corr_mat_s = spearman_corr.loc[kept_s, kept_s]\n",
    "\n",
    "# Mask NaNs so they appear as empty cells\n",
    "data_s = np.ma.masked_invalid(corr_mat_s.to_numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)  # center color at 0\n",
    "im = plt.imshow(data_s, aspect=\"auto\", norm=norm, cmap=cmap_custom)\n",
    "plt.colorbar(im, label=\"Spearman ρ\")\n",
    "\n",
    "plt.xticks(ticks=np.arange(len(kept_s)), labels=kept_s, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(kept_s)), labels=kept_s)\n",
    "\n",
    "# Optional grid to separate cells\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(-.5, len(kept_s), 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, len(kept_s), 1), minor=True)\n",
    "ax.grid(which=\"minor\", linestyle=\"-\", linewidth=0.5, alpha=0.25)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "plt.title(\"Spearman correlation matrix (filtered)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e4cf712992bcb0e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
