{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.311832Z",
     "start_time": "2025-11-14T14:25:21.629118Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Show full column contents (no truncation)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Don’t wrap long output lines\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect, DetectorFactory\n",
    "from itertools import cycle, islice\n",
    "import dtale\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import project_fuctions as functions\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73c464444effbc",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3a375f25e3be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.327178Z",
     "start_time": "2025-11-14T14:25:25.325108Z"
    }
   },
   "outputs": [],
   "source": [
    "artists_path = 'data\\\\artists.csv'\n",
    "tracks_path = 'data\\\\tracks.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec85dbee3b5b18",
   "metadata": {},
   "source": [
    "This code automatically detects the correct separator for two dataset files (tracks and artists) by checking which character — comma, semicolon, or tab — appears most in the first line. It then loads each file into a pandas DataFrame using the detected separator, prints their shapes, and displays the first few rows.\n",
    "\n",
    " The tracks dataset has 11,166 rows and 45 columns, while the artists dataset has 104 rows and 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f505bee69c781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.697809Z",
     "start_time": "2025-11-14T14:25:25.343486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funzione helper per capire il separatore corretto\n",
    "def detect_separator(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample = f.readline()\n",
    "    # Conta quanti separatori compaiono\n",
    "    seps = {',': sample.count(','), ';': sample.count(';'), '\\t': sample.count('\\t')}\n",
    "    best_sep = max(seps, key=seps.get)\n",
    "    print(f\"Detected separator for {filepath}: '{best_sep}'\")\n",
    "    return best_sep\n",
    "\n",
    "# Rileva automaticamente il separatore\n",
    "sep_tracks = detect_separator(tracks_path)\n",
    "sep_artists = detect_separator(artists_path)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "# Carica i dataset in base al separatore rilevato\n",
    "tracks = pd.read_csv(tracks_path, sep=sep_tracks, encoding='utf-8', engine='python')\n",
    "artists = pd.read_csv(artists_path, sep=sep_artists, encoding='utf-8', engine='python')\n",
    "\n",
    "df = tracks.merge(\n",
    "    artists,\n",
    "    left_on=\"id_artist\",\n",
    "    right_on=\"id_author\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_artist\")\n",
    ")\n",
    "\n",
    "# Mostra alcune info per verifica\n",
    "print(f\"Tracks shape: {tracks.shape}\")\n",
    "print(f\"Artists shape: {artists.shape}\")\n",
    "print(\"Shape df (merged):\", df.shape)\n",
    "print('------------------------------------')\n",
    "\n",
    "print('TRACKS')\n",
    "display(tracks.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('ARTISTS')\n",
    "display(artists.head(3))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('MERGERD')\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4d8e9f12d6eb",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5fb592115b28",
   "metadata": {},
   "source": [
    "### Fixing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be5cbfa81ded29",
   "metadata": {},
   "source": [
    "#### Fixing Duplicated Tracks Id\n",
    "After reviewing the songs associated with the duplicated IDs, we found that each duplicated ID corresponds to different songs, except for one case that will be treated later. Therefore, the most reasonable solution is to modify the duplicated IDs by appending the row number to each one. This approach ensures that all songs are preserved while maintaining unique identifiers for every track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97227a4d3a422a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.723304Z",
     "start_time": "2025-11-14T14:25:25.710814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify duplicated IDs\n",
    "duplicate_mask = tracks.duplicated(subset='id', keep=False)\n",
    "\n",
    "# Assign new unique IDs only to duplicated rows\n",
    "tracks.loc[duplicate_mask, 'id'] = (\n",
    "    tracks.loc[duplicate_mask]\n",
    "    .apply(lambda x: f\"{x['id']}_{x.name}\", axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Example of updated duplicates:\")\n",
    "display(tracks[duplicate_mask][['id', 'full_title']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e46153",
   "metadata": {},
   "source": [
    "##### Duplicated Tracks based on title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1e50e8e6e6258",
   "metadata": {},
   "source": [
    "#### Fixing duplicate coloums\n",
    "\n",
    "In this section, we remove all columns that store the same information or redundant representations of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a8d3c1cbf5eed",
   "metadata": {},
   "source": [
    "##### Primary Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82a5055b8756d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.743665Z",
     "start_time": "2025-11-14T14:25:25.735309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove primary_artist column from the dataset\n",
    "df = df.drop(columns=['primary_artist'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187dc1b7aa31dea",
   "metadata": {},
   "source": [
    "##### Full title\n",
    "We are going to take only title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402046d59ed28f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.755669Z",
     "start_time": "2025-11-14T14:25:25.747669Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['full_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f756af46b66f93",
   "metadata": {},
   "source": [
    "##### Name of the artists\n",
    "name_artist and name both represent the same entity but name is formatted more accurately, we will retain the name column and drop name_artist for clarity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30b60fecb79109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.766789Z",
     "start_time": "2025-11-14T14:25:25.759763Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['name_artist'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8bca0a024d415",
   "metadata": {},
   "source": [
    "##### Album\n",
    "Once we understand that album is more stable than album, we can remove both of these two coloums (album_name and id_album). We remove also album_image because it is useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7994dc4fddb45ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.784073Z",
     "start_time": "2025-11-14T14:25:25.769793Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['album_name'], inplace=True)\n",
    "df.drop(columns=['id_album'], inplace=True)\n",
    "df.drop(columns=['album_image'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31512a988ef1b85",
   "metadata": {},
   "source": [
    "#### Fixing duplicate tracks based on lyrics\n",
    "As previously identified, there are 9 duplicate rows based on identical lyrics. To maintain data integrity, we will remove these duplicates from the dataset. This step is crucial to prevent skewing any analysis or derived features that rely on the lyrics content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745fbe358529e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.869910Z",
     "start_time": "2025-11-14T14:25:25.788078Z"
    }
   },
   "outputs": [],
   "source": [
    "original_row_count = len(df)\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "\n",
    "non_nan_mask = df['lyrics'].notna()\n",
    "df_nan_lyrics = df[~non_nan_mask]\n",
    "df_non_nan_lyrics = df[non_nan_mask]\n",
    "\n",
    "print(f\"  > Found {len(df_nan_lyrics)} rows with NaN lyrics (keeping all for now).\")\n",
    "print(f\"  > Found {len(df_non_nan_lyrics)} rows with non-NaN lyrics (checking for duplicates).\")\n",
    "\n",
    "duplicate_mask = df_non_nan_lyrics.duplicated(subset=['lyrics', 'album_type'], keep='first')\n",
    "\n",
    "indices_to_drop = df_non_nan_lyrics[duplicate_mask].index\n",
    "\n",
    "print(f\"  > Found {len(indices_to_drop)} true logical duplicates (same lyrics AND album_type) to remove.\")\n",
    "\n",
    "df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "cleaned_row_count = len(df)\n",
    "rows_removed_total = original_row_count - cleaned_row_count\n",
    "\n",
    "print(f\"\\nDataFrame shape after dropping logical duplicates: {df.shape}\")\n",
    "print(f\"Total rows removed: {rows_removed_total}\")\n",
    "\n",
    "# We now expect a smaller number (e.g., 8 instead of 10) because\n",
    "# the 'Madame' tracks (and any others with different album_type) were kept.\n",
    "print(f\"SUCCESS: Removed {rows_removed_total} rows. Duplicates with different 'album_type' were kept.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e7d4e",
   "metadata": {},
   "source": [
    "### Fixing DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074befa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting birth_date to dateTime   \n",
    "date_cols = ['birth_date',  ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "#Converting active_start to dateTime\n",
    "date_cols = [ 'active_start', ]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # convert to datetime, invalid dates become NaT\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#converting album_release_date to DateTime\n",
    "date_cols = ['album_release_date']\n",
    "# --- Check date columns ---\n",
    "for col in date_cols:\n",
    "    original = df[col].copy()\n",
    "    converted = pd.to_datetime(original, errors='coerce')\n",
    "    non_convertible = original[original.notna() & converted.isna()]\n",
    "    \n",
    "    print(f\"\\nColumn '{col}'  entries that cannot be converted to datetime:\")\n",
    "    if not non_convertible.empty:\n",
    "        for idx, val in non_convertible.items():\n",
    "            print(f\"Row {idx}: {val}\")\n",
    "    else:\n",
    "        print(\"All non-missing entries can be converted to datetime.\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "# Converting to DateTime\n",
    "def fix_year_only_dates(val):\n",
    "    \"\"\"\n",
    "    If the value looks like a 4-digit year, convert it to 'YYYY-01-01'.\n",
    "    Otherwise, return the original value.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val_str = str(val).strip()\n",
    "    if re.fullmatch(r'\\d{4}', val_str):\n",
    "        return f\"{val_str}-01-01\"\n",
    "    return val_str\n",
    "\n",
    "# Apply to album_release_date\n",
    "df['album_release_date'] = df['album_release_date'].apply(fix_year_only_dates)\n",
    "\n",
    "# Convert album_release_date to datetime\n",
    "df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "\n",
    " # Convert year  to number\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce') \n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c435ec9a332cae",
   "metadata": {},
   "source": [
    "### Filling Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef5f9cb2e415f7",
   "metadata": {},
   "source": [
    "#### Active End\n",
    "\n",
    "It is a complete empy coloum so we can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c6b64832826da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.879239Z",
     "start_time": "2025-11-14T14:25:25.872914Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['active_end'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26288143869b754",
   "metadata": {},
   "source": [
    "#### Stats Pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6465456e488f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.889242Z",
     "start_time": "2025-11-14T14:25:25.882242Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['stats_pageviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2515a4efeac9d4",
   "metadata": {},
   "source": [
    "#### Features Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc639e38178905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.899152Z",
     "start_time": "2025-11-14T14:25:25.891749Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['featured_artists'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d3530fd73e5f",
   "metadata": {},
   "source": [
    "#### Popularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996f6a6372867a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.907052Z",
     "start_time": "2025-11-14T14:25:25.902156Z"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df.loc[\n",
    "        (df['popularity'].isna()),\n",
    "        ['popularity', 'title']\n",
    "    ].head(50)\n",
    ")\n",
    "print(f\"We have {df['popularity'].isna().sum()} null value in popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b5382512ff16",
   "metadata": {},
   "source": [
    "Given that the popularity score is a continuous metric with skewed distribution and that dropping rows would remove valuable tracks, we opted for median-based imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278427550a98c2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.915023Z",
     "start_time": "2025-11-14T14:25:25.910056Z"
    }
   },
   "outputs": [],
   "source": [
    "df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce')\n",
    "\n",
    "df['popularity_missing_flag'] = df['popularity'].isna().astype(int)\n",
    "median_pop = df['popularity'].median()\n",
    "df['popularity'] = df['popularity'].fillna(median_pop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946464fb9a1dad0",
   "metadata": {},
   "source": [
    "#### Filling Birth Dates\n",
    "This code manually fills missing birth dates for specific artists in the dataset. It first defines a dictionary mapping artist names to their known or estimated birth dates.\n",
    "\n",
    "9 entries couldn’t be filled so their birth dates are intentionally left blank in the dictionary. For Miss Keta, the birth date is unknown, so no accurate value can be provided. The others — Bushwaka, Sottotono, Dark Polo Gang, Cor Veleno, Colle Der Fomento, Club Dogo, Articolo 31, and 99 Posse — are all music groups or duos, not individual artists, meaning they don’t have a single birth date associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb9b4cf669fa0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:25.947145Z",
     "start_time": "2025-11-14T14:25:25.918028Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Define the Missing Dates as a Dictionary ---\n",
    "# Source of truth for the manual fill\n",
    "birth_dates_to_fill = {\n",
    "    'alfa': '2000-08-22',\n",
    "    'anna pepe': '2003-08-15',\n",
    "    'beba': '1994-10-10',\n",
    "    'bigmama': '2000-03-10',\n",
    "    'brusco': '1974-01-04',\n",
    "    'caneda': '1976-09-30',\n",
    "    'dargen d_amico': '1980-11-29',\n",
    "    'guè pequeno': '1980-12-25',\n",
    "    'johnny marsiglia': '1986-08-05',\n",
    "    'nerone': '1991-05-23',\n",
    "    'priestess': '1996-08-20',\n",
    "    'samuel heron': '1991-01-01',\n",
    "    'shiva': '1999-08-27',\n",
    "    'yeиdry': '1993-07-27',\n",
    "    'o zulù': '1970-11-15',\n",
    "    'skioffi':'1992-06-05',\n",
    "    'eva rea':'1993-01-01',\n",
    "    'hindaco':'1996-01-01',\n",
    "    'joey funboy':'1995-01-01',\n",
    "    'mistico':'1982-01-01',\n",
    "    'mike24':'1985-08-02',\n",
    "    'doll kill':'1996-01-01',\n",
    "    'miss simpatia':'1986-03-23',\n",
    "    'miss keta':'',#unknown\n",
    "    'bushwaka':'',#duo\n",
    "    'sottotono':'',#group\n",
    "    'dark polo gang':'',#group\n",
    "    'cor veleno':'',#group\n",
    "    'colle der fomento':'',#group\n",
    "    'club dogo':'',#group\n",
    "    'articolo 31':'',#group\n",
    "    '99 posse':''#gruppo\n",
    "    }\n",
    "\n",
    "# --- 2. Fill the Missing Data (Imputation) ---\n",
    "\n",
    "# Convert the dictionary to a Pandas Series for easy lookup and indexing\n",
    "birth_date_series = pd.Series(birth_dates_to_fill)\n",
    "\n",
    "# Iterate through the artists in your fill list and update the DataFrame\n",
    "for artist, bday in birth_date_series.items():\n",
    "    # Use .loc to find rows where 'artist_name' matches and update 'birth_date'\n",
    "    # The second part of the condition (artist_df['birth_date'].isna()) ensures\n",
    "    # we only overwrite if the date was previously missing (NaN).\n",
    "    df.loc[\n",
    "        (df['name'] == artist) & (df['birth_date'].isna()),\n",
    "        'birth_date'\n",
    "    ] = bday\n",
    "\n",
    "# --- 3. Final Conversion and Verification ---\n",
    "\n",
    "# Convert the 'birth_date' column to the proper datetime format again\n",
    "# (This is crucial for accurate age calculation)\n",
    "df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "\n",
    "# Optional: Print out the affected rows to verify the fix\n",
    "print(\"--- Verification of Filled Birth Dates  ---\")\n",
    "# Filter the DataFrame to show only the artists we just updated\n",
    "filled_artists = df[df['name'].isin(birth_dates_to_fill.keys())]\n",
    "\n",
    "# Show the unique artist names and their newly filled birth dates\n",
    "print(filled_artists[['name', 'birth_date']].drop_duplicates().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61085a819e98b55b",
   "metadata": {},
   "source": [
    "##### Checking distribution after filling Bithdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8583e11e3a2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:26.448802Z",
     "start_time": "2025-11-14T14:25:25.950149Z"
    }
   },
   "outputs": [],
   "source": [
    "functions.plot_birth_decades(df, \"Distribution of Artists' Birth Years  After Filling Nan\",'Percentage of Unique Artists by Decade of Birth After Filling Nan')\n",
    "functions.plot_artist_ages(df,'Number of Unique Artists by Age (After Filling NaN)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4081d8dfc9794",
   "metadata": {},
   "source": [
    "#### Filling Active Start Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1a2b19f88f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:26.682569Z",
     "start_time": "2025-11-14T14:25:26.460314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total number of unique artists missing 'active_start': 54\n",
    "active_starts_consolidated = {\n",
    "    'alfa': '01-15-2017',\n",
    "    'anna pepe': '01-01-2018',\n",
    "    'babaman': '01-01-1989',\n",
    "    'beba': '11-01-2015',\n",
    "    'brusco': '01-01-1991',\n",
    "    'capo plaza': '01-01-2013',\n",
    "    'chadia rodriguez': '01-01-2017',\n",
    "    'clementino': '04-29-2006',\n",
    "    'dargen d_amico': '01-01-1999',\n",
    "    'don joe': '01-01-1999',\n",
    "    'fred de palma': '01-01-2008',\n",
    "    'geolier': '01-01-2018',\n",
    "    'guè pequeno': '01-01-1997',\n",
    "    'miss keta': '01-01-2013',\n",
    "    'shiva': '01-01-2014',\n",
    "    'tedua': '01-01-2013',\n",
    "    'tony effe': '01-01-2014',\n",
    "    'sottotono': '01-01-1994',\n",
    "    'bushwaka': '01-01-2007',\n",
    "    'mike24': '01-01-2009',\n",
    "    'mistico': '01-01-2008',\n",
    "    'skioffi': '01-01-2014',\n",
    "    \"caneda\": \"01-01-1993\",\n",
    "    \"club dogo\": \"01-01-2002\",\n",
    "    \"colle der fomento\": \"01-01-1994\",\n",
    "    \"dani faiv\": \"01-01-2014\",\n",
    "    \"doll kill\": \"01-01-2012\",\n",
    "    \"drefgold\": \"01-01-2012\",\n",
    "    \"entics\": \"01-01-2004\",\n",
    "    \"eva rea\": \"12-18-2014\",\n",
    "    \"hell raton\": \"01-01-2010\",\n",
    "    \"hindaco\": \"02-21-2020\",\n",
    "    \"jack the smoker\": \"01-01-2000\",\n",
    "    \"joey funboy\": \"01-01-2016\",\n",
    "    \"johnny marsiglia\": \"01-01-2007\",\n",
    "    \"la pina\": \"01-01-1994\",\n",
    "    \"luchè\": \"01-01-1997\",\n",
    "    \"mambolosco\": \"02-10-2017\",\n",
    "    \"massimo pericolo\": \"01-01-2016\",\n",
    "    \"miss simpatia\": \"01-01-2007\",\n",
    "    \"mistaman\": \"01-01-1994\",\n",
    "    \"mondo marcio\": \"01-01-2003\",\n",
    "    \"nerone\": \"01-01-2008\",\n",
    "    \"niky savage\": \"01-01-2021\",\n",
    "    \"o zulù\": \"01-01-1991\",\n",
    "    \"papa v\": \"01-01-2020\",\n",
    "    \"rondodasosa\": \"01-01-2020\",\n",
    "    \"samuel heron\": \"01-01-2012\",\n",
    "    \"shablo\": \"01-01-1999\",\n",
    "    \"slait\": \"01-01-2010\",\n",
    "    \"tony boy\": \"01-01-2018\",\n",
    "    \"tormento\": \"01-01-1991\",\n",
    "    \"yeиdry\": \"01-01-2012\",\n",
    "    \"yung snapp\": \"01-01-2012\",\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the dictionary to a Pandas Series for efficient filling\n",
    "start_date_series = pd.Series(active_starts_consolidated)\n",
    "\n",
    "# Iterate and fill the missing data in the 'active_start' column\n",
    "for artist, start_date in start_date_series.items():\n",
    "    # Use .loc to find rows where 'artist_name' matches and update 'active_start'\n",
    "    df.loc[\n",
    "        df['name'] == artist,\n",
    "        'active_start'\n",
    "    ] = start_date\n",
    "\n",
    "# Ensure the 'active_start' column is a proper datetime object\n",
    "df['active_start'] = pd.to_datetime(df['active_start'], errors='coerce')\n",
    "\n",
    "print(\"Active start dates have been filled in the 'active_start' column.\")\n",
    "\n",
    "functions.plot_active_start_decades(df,'Percentage of Unique Artists by Active Start Decade After Filling Missing Values')\n",
    "functions.plot_age_at_career_start(df,'Age of unique Artists When They Started Their Career After Filling Missing Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20237069",
   "metadata": {},
   "source": [
    "#### Adding Album_release_date for albums that doesn't have date (todooo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62aceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##todoooo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5860b3651894a7",
   "metadata": {},
   "source": [
    "#### Filling missing albums for tracks (REDooo)\n",
    "We have 78 tracks without albums. We managed to find the albums for 16 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190920a5bb6e3f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:27.203748Z",
     "start_time": "2025-11-14T14:25:26.686575Z"
    }
   },
   "outputs": [],
   "source": [
    "# # --- 1. Define mapping for known tracks ---\n",
    "# # Keys = track title (partial or exact match)\n",
    "# # Values = album name, release date, and type\n",
    "# album_updates = {\n",
    "\n",
    "#     \"PTS (PoiTiSpiego/PostTraumaticStress)\": {\n",
    "#         \"album_name\": \"Gilmar / Embrionale\",\n",
    "#         \"album_release_date\": \"2012-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Quelli che benpensano\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Autodafè\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Fight da faida\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Giù le mani da Caino\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Pedala\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Accendimi\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Fili\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Rap Lamento\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Cali di tensione\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Cubetti tricolori\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Faccio la mia cosa\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Libri Di Sangue\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Potere Alla Parola\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Nuvole\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "#     \"Il beat come anestetico\": {\n",
    "#         \"album_name\": \"La morte dei miracoli\",\n",
    "#         \"album_release_date\": \"1997-01-01\",\n",
    "#         \"album_type\": \"album\"\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --- 2. Define a helper function to safely update missing info ---\n",
    "# def update_album_info(row):\n",
    "#     for track, info in album_updates.items():\n",
    "#         if track.lower() in str(row['title']).lower():  # partial match, case-insensitive\n",
    "#             if pd.isna(row['album_name']):\n",
    "#                 row['album_name'] = info['album_name']\n",
    "#             if pd.isna(row['album_release_date']):\n",
    "#                 row['album_release_date'] = pd.to_datetime(info['album_release_date'])\n",
    "#             if pd.isna(row['album_type']):\n",
    "#                 row['album_type'] = info['album_type']\n",
    "#             break\n",
    "#     return row\n",
    "\n",
    "# # --- 3. Apply to the dataset ---\n",
    "# df = df.apply(update_album_info, axis=1)\n",
    "\n",
    "# # --- 4. Verify the updates ---\n",
    "# updated = df[df['title'].str.contains('|'.join(album_updates.keys()), case=False, na=False)]\n",
    "# print(f\" Updated {len(updated)} tracks with album information.\")\n",
    "# display(updated[['title', 'album_name', 'album_release_date', 'album_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7691eca1ef2068",
   "metadata": {},
   "source": [
    "#### Artist Location Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449d1a43dc7a6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:29.618727Z",
     "start_time": "2025-11-14T14:25:27.208056Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_all_artist_data(df):\n",
    "    \"\"\"\n",
    "    Performs comprehensive imputation of all missing biographical fields,\n",
    "    including Latitude and Longitude coordinates, for the Italian artist dataset.\n",
    "\n",
    "    The function relies on a manually curated knowledge base (IMPUTATION_MAP).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Define the Master Imputation Map with Coordinates\n",
    "    # Value: (Birth Place, Nationality, Country, Province, Region, Latitude, Longitude)\n",
    "    IMPUTATION_MAP = {\n",
    "        'articolo 31': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'bushwaka': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'club dogo': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'dargen d_amico': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'doll kill': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'guè pequeno': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'miss keta': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'nerone': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'shiva': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "        'niky savage': ('Milano', 'Italia', 'Italia', 'Milano', 'Lombardia', 45.4642, 9.1900),\n",
    "\n",
    "        # --- ROMA (Lazio) ---\n",
    "        'brusco': ('Roma', 'Italia', 'Italia', 'Roma', 'Lazio', 41.8933, 12.4829),\n",
    "        'colle der fomento': ('Roma', 'Italia', 'Italia', 'Roma', 'Lazio', 41.8933, 12.4829),\n",
    "        'cor veleno': ('Roma', 'Italia', 'Italia', 'Roma', 'Lazio', 41.8933, 12.4829),\n",
    "        'mistico': ('Roma', 'Italia', 'Italia', 'Roma', 'Lazio', 41.8933, 12.4829),\n",
    "        'dark polo gang':('Roma', 'Italia', 'Italia', 'Roma', 'Lazio', 41.8933, 12.4829),\n",
    "\n",
    "        # --- NAPOLI (Campania) ---\n",
    "        '99 posse': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'o zulù': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'bigmama': ('Avellino', 'Italia', 'Italia', 'Avellino', 'Campania', 40.9167, 14.7833),\n",
    "        'eva rea': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'joey funboy': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'mike24': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'miss simpatia': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "        'samuel heron': ('Napoli', 'Italia', 'Italia', 'Napoli', 'Campania', 40.8518, 14.2681),\n",
    "\n",
    "        # --- Other Regions/Cities ---\n",
    "        'beba': ('Torino', 'Italia', 'Italia', 'Torino', 'Piemonte', 45.0703, 7.6869),\n",
    "        'alfa': ('Genova', 'Italia', 'Italia', 'Genova', 'Liguria', 44.4073, 8.9463),\n",
    "        'anna pepe': ('La Spezia', 'Italia', 'Italia', 'La Spezia', 'Liguria', 44.1027, 9.8252),\n",
    "        'caneda': ('Varese', 'Italia', 'Italia', 'Varese', 'Lombardia', 45.8197, 8.8256),\n",
    "        'fabri fibra': ('Senigallia', 'Italia', 'Italia', 'Ancona', 'Marche', 43.7126, 13.2201),\n",
    "        'nesli': ('Senigallia', 'Italia', 'Italia', 'Ancona', 'Marche', 43.7126, 13.2201),\n",
    "        'hindaco': ('Padova', 'Italia', 'Italia', 'Padova', 'Veneto', 45.4064, 11.8767),\n",
    "        'johnny marsiglia': ('Palermo', 'Italia', 'Italia', 'Palermo', 'Sicilia', 38.1157, 13.3615),\n",
    "        'priestess': ('Palermo', 'Italia', 'Italia', 'Palermo', 'Sicilia', 38.1157, 13.3615),\n",
    "        'skioffi': ('Taranto', 'Italia', 'Italia', 'Taranto', 'Puglia', 40.4667, 17.2403),\n",
    "        'sottotono': ('Varese', 'Italia', 'Italia', 'Varese', 'Lombardia', 45.8197, 8.8256),\n",
    "\n",
    "        # --- International ---\n",
    "        'baby k': ('Singapore', 'Italia', 'Singapore', np.nan, np.nan, 1.3521, 103.8198),\n",
    "        'shablo': ('Buenos Aires', 'Argentina', 'Argentina', np.nan, np.nan, -34.6037, -58.3816),\n",
    "        'yeиdry': ('Santo Domingo', 'Dominicana', 'Dominican Republic', np.nan, np.nan, 18.4861, -69.9312),\n",
    "\n",
    "    }\n",
    "\n",
    "    # 2. Add Latitude and Longitude columns if they don't exist\n",
    "    for col in ['latitude', 'longitude']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # 3. Iterate and Apply Imputation/Fill Missing Values\n",
    "    for index, row in df.iterrows():\n",
    "        artist = row['name']\n",
    "\n",
    "        if artist in IMPUTATION_MAP:\n",
    "            (birth_place, nationality, country, province, region, lat, lon) = IMPUTATION_MAP[artist]\n",
    "\n",
    "            # Use fillna() logic: only fill if the current cell is NaN\n",
    "\n",
    "            # Biographical Imputation\n",
    "            df.loc[index, 'birth_place'] = row['birth_place'] if pd.notna(row['birth_place']) else birth_place\n",
    "            df.loc[index, 'nationality'] = row['nationality'] if pd.notna(row['nationality']) else nationality\n",
    "            df.loc[index, 'country'] = row['country'] if pd.notna(row['country']) else country\n",
    "            df.loc[index, 'province'] = row['province'] if pd.notna(row['province']) else province\n",
    "            df.loc[index, 'region'] = row['region'] if pd.notna(row['region']) else region\n",
    "\n",
    "            # Geospatial Imputation (Always fill the Lat/Lng to ensure consistency with the Birth Place)\n",
    "            df.loc[index, 'latitude'] = lat\n",
    "            df.loc[index, 'longitude'] = lon\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = impute_all_artist_data(df)\n",
    "\n",
    "\n",
    "print ('checking the left null values after filling')\n",
    "# Define columns to check\n",
    "cols_to_check = [\n",
    "    'birth_place',\n",
    "    'nationality',\n",
    "    'province',\n",
    "    'region',\n",
    "    'country',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# Filter rows where any of these columns are null\n",
    "missing_rows = df[df[cols_to_check].isnull().any(axis=1)]\n",
    "\n",
    "# Select only artist name + the relevant columns\n",
    "columns_to_show = ['name'] + cols_to_check\n",
    "missing_subset = missing_rows[columns_to_show]\n",
    "\n",
    "# Keep only unique artist names (first occurrence)\n",
    "unique_missing_subset = missing_subset.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Show the result in D-Tale\n",
    "print(unique_missing_subset.shape)\n",
    "display(unique_missing_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d201f5e3ad62b6c",
   "metadata": {},
   "source": [
    "#### Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4aec37c8c611b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:29.840989Z",
     "start_time": "2025-11-14T14:25:29.831556Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Shape before fixing NaNs: {df.shape}\")\n",
    "original_row_count = len(df)\n",
    "\n",
    "# Action 1: Drop all rows where 'lyrics' is missing (in-place)\n",
    "df.dropna(subset=['lyrics'], inplace=True)\n",
    "\n",
    "rows_removed_lyrics = original_row_count - len(df)\n",
    "print(f\"\\nShape after dropping 'lyrics' NaNs: {df.shape}\")\n",
    "print(f\"Rows removed: {rows_removed_lyrics}\")\n",
    "\n",
    "if rows_removed_lyrics == 3:\n",
    "    print(\"SUCCESS: Correctly removed the 3 'NaN' lyric rows.\")\n",
    "else:\n",
    "    print(f\"WARNING: Expected to remove 3 'NaN' rows, but removed {rows_removed_lyrics}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb33ecb0ba3f64",
   "metadata": {},
   "source": [
    "### Fixing out of range and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc2d1715fc3fac",
   "metadata": {},
   "source": [
    "####  Correcting \"Priestess\" Entry\n",
    "\n",
    "Based on the inspection of the artists description above, we noticed an entry labeled “gruppo musicale canadese” (Canadian music group). Upon checking, this description is incorrectly assigned to the Italian rapper Priestess. Further research revealed a mix-up with a Canadian band that shares the same name. This confusion becomes evident when comparing the active_start year in the dataset, which matches that of the Canadian group rather than the Italian artist.\n",
    "\n",
    "We are going to correct its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4c6dd6c4cb9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:29.868059Z",
     "start_time": "2025-11-14T14:25:29.847994Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Before')\n",
    "display( df[df['description'].str.contains('gruppo musicale canadese', case=False, na=False)]\n",
    "         .drop_duplicates(subset=['name'])\n",
    "         .sort_values(by='name'))\n",
    "\n",
    "print('After')\n",
    "# Fix Priestess' incorrect description and active_start date\n",
    "df.loc[df['name'].str.lower() == 'priestess', ['description','active_start' ]] = [\n",
    "    'cantante e rapper italiana',\n",
    "    '2017-01-01'\n",
    "\n",
    "]\n",
    "\n",
    "# Verify the update\n",
    "print(df[df['name'].str.lower() == 'priestess'][['name', 'description', 'active_start']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbad92abad1310",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e740b5af3016e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:29.948246Z",
     "start_time": "2025-11-14T14:25:29.940245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix out-of-range popularity values\n",
    "df['popularity'] = df['popularity'].clip(lower=0, upper=100)\n",
    "#\n",
    "display(df.loc[df['modified_popularity'] == 1, ['popularity', 'modified_popularity', 'title']].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c030a065bc58f75",
   "metadata": {},
   "source": [
    "Now we don't need anymore modified_popularity, so we can just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7588dbfbdbcd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:29.983758Z",
     "start_time": "2025-11-14T14:25:29.976757Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['modified_popularity'], inplace=True)\n",
    "print(\"OK — column 'modified_popularity' has been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec63da3e255bf2",
   "metadata": {},
   "source": [
    "#### Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d36e36a87af90",
   "metadata": {},
   "source": [
    "##### Remove Junk Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c38eb7bca0ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:30.837817Z",
     "start_time": "2025-11-14T14:25:29.995270Z"
    }
   },
   "outputs": [],
   "source": [
    "# maximum tokens for junk lyrics: 36\n",
    "# minimum tokens for actual lyrics: 108\n",
    "pattern = 'Contributors|Contributor|Lyrics|COMING SOON|instrumental'\n",
    "\n",
    "token_threshold = 60  # Safe threshold based on your analysis\n",
    "\n",
    "# Find the indices of rows that meet BOTH conditions\n",
    "conditions_to_eliminate = (\n",
    "        (df['lyrics'].str.contains(pattern, case=False, na=False)) &\n",
    "        (df['n_tokens'] < token_threshold)\n",
    ")\n",
    "\n",
    "# Get the actual index labels of the rows to drop\n",
    "indices_to_drop = df[conditions_to_eliminate].index\n",
    "num_to_eliminate = len(indices_to_drop)\n",
    "\n",
    "print(f\"Found {num_to_eliminate} 'junk AND short' rows (< {token_threshold} tokens) to ELIMINATE.\")\n",
    "\n",
    "# 2. Set the 'lyrics' column to NaN where the conditions are true\n",
    "if num_to_eliminate > 0:\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "    print(f\"DataFrame shape AFTER elimination: {df.shape}\")\n",
    "    print(f\"Successfully removed {num_to_eliminate} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8daa70c583014",
   "metadata": {},
   "source": [
    "##### Cleaning \"Contributors\" Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a46d2596af5d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:31.015872Z",
     "start_time": "2025-11-14T14:25:30.903336Z"
    }
   },
   "outputs": [],
   "source": [
    "contributor_pattern = 'Contributors|Contributor'\n",
    "contributor_matches = df[df['lyrics'].str.contains(contributor_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"\\nRows containing Contributors words\")\n",
    "print(f\"Total number: {len(contributor_matches)}\")\n",
    "if not contributor_matches.empty:\n",
    "    display(contributor_matches[['id', 'name', 'title', 'lyrics', 'n_tokens']].head(23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4de6f08863549b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:31.136241Z",
     "start_time": "2025-11-14T14:25:31.020878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the found pattern from the beginning of the lyrics\n",
    "print(f\"DataFrame shape BEFORE cleaning: {df.shape}\")\n",
    "\n",
    "indices_to_clean = df[df['lyrics'].str.contains(contributor_pattern, case=False, na=False)].index\n",
    "\n",
    "print(f\"Number of rows to clean: {len(indices_to_clean)}\")\n",
    "\n",
    "if len(indices_to_clean) > 0:\n",
    "    # Definisci la regex per l'intestazione\n",
    "    header_regex = r\"^\\s*\\d+\\s+Contributor(s)?.*?\\s+Lyrics\\s*\"\n",
    "\n",
    "    original_lyrics = df.loc[indices_to_clean, 'lyrics']\n",
    "\n",
    "    cleaned_lyrics_series = original_lyrics.str.replace(\n",
    "        header_regex, '', regex=True, flags=re.IGNORECASE\n",
    "    ).str.strip()\n",
    "\n",
    "    empty_mask = (cleaned_lyrics_series == '') | (cleaned_lyrics_series.str.isspace()) | (cleaned_lyrics_series.isna())\n",
    "    indices_to_drop = cleaned_lyrics_series[empty_mask].index\n",
    "\n",
    "    indices_to_update = cleaned_lyrics_series[~empty_mask].index\n",
    "    updates_to_apply = cleaned_lyrics_series[indices_to_update]\n",
    "\n",
    "    print(f\"  > Identified {len(indices_to_drop)} rows to DROP (lyrics were only the header).\")\n",
    "    print(f\"  > Identified {len(indices_to_update)} rows to UPDATE (lyrics were contaminated).\")\n",
    "\n",
    "    # Delete the rows that are now empty\n",
    "    if not indices_to_drop.empty:\n",
    "        df.drop(indices_to_drop, inplace=True)\n",
    "        print(f\"  > Dropped {len(indices_to_drop)} 'junk' rows.\")\n",
    "\n",
    "    # Update the rows with cleaned lyrics\n",
    "    if not indices_to_update.empty:\n",
    "        df.loc[indices_to_update, 'lyrics'] = updates_to_apply\n",
    "        print(f\"  > Cleaned and updated {len(indices_to_update)} 'contaminated' rows.\")\n",
    "\n",
    "    print(f\"\\nDataFrame shape AFTER cleaning: {df.shape}\")\n",
    "\n",
    "print(\"\\nVerifying the cleaning (first 5 modified lyrics):\")\n",
    "for index in indices_to_clean[:20]:\n",
    "    if index in df.index:\n",
    "        print(\"==============================================\")\n",
    "        print(f\"INDEX: {index}\")\n",
    "        print(f\"CLEANED TEXT (preview):\\n'{str(df.loc[index, 'lyrics'])[:200]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1bc448f88a917",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Cleaning editorial sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d143cc4284ebf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:31.614526Z",
     "start_time": "2025-11-14T14:25:31.139540Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_editorial_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    pattern = (\n",
    "        r\"(Il brano vanta[^\\.]*\\.?)|\"      # frasi tipo \"Il brano vanta...\"\n",
    "        r\"(La produzione è curata[^\\.]*\\.?)\"  # frasi tipo \"La produzione è curata...\"\n",
    "        r\"(La produzione è opera di [^\\.]*\\.?)\"\n",
    "    )\n",
    "\n",
    "    cleaned = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    return cleaned.strip()\n",
    "\n",
    "df[\"lyrics\"] = df[\"lyrics\"].apply(remove_editorial_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43acffef807e618",
   "metadata": {},
   "source": [
    "##### Recalculate _auto features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586469473c5c588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:34.134488Z",
     "start_time": "2025-11-14T14:25:31.618531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the \"atomic\" counting functions\n",
    "print(\"Recalculating n_tokens_auto...\")\n",
    "# We use .loc to ensure we modify the original df\n",
    "df.loc[:, 'n_tokens_auto'] = df['lyrics'].apply(functions.count_tokens)\n",
    "\n",
    "print(\"Recalculating n_sentences_auto...\")\n",
    "df.loc[:, 'n_sentences_auto'] = df['lyrics'].apply(functions.count_sentences)\n",
    "\n",
    "print(\"Recalculating n_unique_words_auto...\")\n",
    "df.loc[:, 'n_unique_words_auto'] = df['lyrics'].apply(functions.count_unique_tokens)\n",
    "\n",
    "print(\"Recalculating total_chars_auto...\")\n",
    "df.loc[:, 'total_chars_auto'] = df['lyrics'].apply(functions.count_total_token_chars)\n",
    "\n",
    "# Recalculate Ratios (handling division by zero)\n",
    "print(\"Recalculating lexical_density_auto...\")\n",
    "df.loc[:, 'lexical_density_auto'] = np.where(\n",
    "    df['n_tokens_auto'] == 0, 0.0,\n",
    "    df['n_unique_words_auto'] / df['n_tokens_auto']\n",
    ")\n",
    "\n",
    "print(\"Recalculating tokens_per_sent_auto...\")\n",
    "df.loc[:, 'tokens_per_sent_auto'] = np.where(\n",
    "    df['n_sentences_auto'] == 0, 0.0,\n",
    "    df['n_tokens_auto'] / df['n_sentences_auto']\n",
    ")\n",
    "\n",
    "print(\"Recalculating char_per_tok_auto...\")\n",
    "df.loc[:, 'char_per_tok_auto'] = np.where(\n",
    "    df['n_tokens_auto'] == 0, 0.0,\n",
    "    df['total_chars_auto'] / df['n_tokens_auto']\n",
    ")\n",
    "\n",
    "print(\"\\n--- RECALCULATION COMPLETE ---\")\n",
    "\n",
    "# Final Verification -\n",
    "nan_check = df['n_tokens_auto'].isna().sum()\n",
    "print(f\"NaNs in 'n_tokens_auto' (should now be 0): {nan_check}\")\n",
    "if nan_check == 0:\n",
    "    print(\"SUCCESS: All 'broken' rows have been fixed.\")\n",
    "else:\n",
    "    print(\"WARNING: 'NaN' values still found. Please review the 'lyrics' column for errors.\")\n",
    "\n",
    "print(\"\\nAll derived features are now clean and synchronized with the 'lyrics' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc227790a59f4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:34.217716Z",
     "start_time": "2025-11-14T14:25:34.208717Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Shape before column cleanup: {df.shape}\")\n",
    "print(f\"Columns before cleanup: {df.columns.tolist()}\")\n",
    "\n",
    "# Define Columns to Drop\n",
    "\n",
    "# These are the original \"dirty\" features we are replacing\n",
    "original_dirty_features = [\n",
    "    'n_tokens',\n",
    "    'n_sentences',\n",
    "    'tokens_per_sent',\n",
    "    'char_per_tok',\n",
    "    'lexical_density',\n",
    "    'avg_token_per_clause'\n",
    "]\n",
    "\n",
    "# These are the intermediate calculation/helper columns we created\n",
    "helper_and_match_cols = [\n",
    "    'n_sentences_match',\n",
    "    'n_tokens_match',\n",
    "    'tokens_per_sent_match',\n",
    "    'char_per_tok_match',\n",
    "    'lexical_density_match',\n",
    "    'total_chars_auto',\n",
    "    'n_unique_words_auto'\n",
    "]\n",
    "\n",
    "cols_to_drop = original_dirty_features + helper_and_match_cols\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "\n",
    "print(f\"\\nDropping {len(existing_cols_to_drop)} old/helper columns...\")\n",
    "\n",
    "#  Drop the old and helper columns\n",
    "df.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"Old columns dropped.\")\n",
    "\n",
    "# Define Columns to Rename ---\n",
    "\n",
    "rename_map = {\n",
    "    'n_tokens_auto': 'n_tokens',\n",
    "    'n_sentences_auto': 'n_sentences',\n",
    "    'tokens_per_sent_auto': 'tokens_per_sent',\n",
    "    'char_per_tok_auto': 'char_per_tok',\n",
    "    'lexical_density_auto': 'lexical_density'\n",
    "}\n",
    "\n",
    "existing_rename_map = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "\n",
    "print(f\"\\nRenaming {len(existing_rename_map)} '_auto' columns to their final names...\")\n",
    "\n",
    "# Rename the '_auto' columns\n",
    "df.rename(columns=existing_rename_map, inplace=True)\n",
    "\n",
    "print(\"Columns successfully renamed.\")\n",
    "\n",
    "print(f\"Final DataFrame shape: {df.shape}\")\n",
    "print(f\"Final columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1aca061db9194",
   "metadata": {},
   "source": [
    "The avg_token_per_clause feature was removed during the data cleaning process for two primary reasons:\n",
    "\n",
    "Unreliable Source Data: The calculation of this feature depends on the original n_tokens column. Our feature inspection proved that the original n_tokens column is inconsistent and unreliable (with a >99% mismatch compared to our clean, recalculated n_tokens_auto). This makes any metric derived from it inherently untrustworthy.\n",
    "\n",
    "Unverifiable Calculation: Unlike simpler metrics like n_tokens_auto or n_sentences_auto (which we could recalculate using regex), accurately identifying grammatical \"clauses\" (proposizioni) requires complex syntactic NLP parsing. This makes the feature impossible for us to verify or reliably recalculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ed74063e186c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:34.225227Z",
     "start_time": "2025-11-14T14:25:34.220226Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(df.loc[df['n_tokens'] < 10, ['lyrics', 'n_tokens']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b6f9eb1aa7b00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We decide to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c741c2e2d4528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:34.230516Z",
     "start_time": "2025-11-14T14:25:34.227450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove = df[df['n_tokens'] < 10].shape[0]\n",
    "print(\"Deleted rows:\", to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d208dc897f92047",
   "metadata": {},
   "source": [
    "#### Languages\n",
    "\n",
    "Since the language of a track is inherently encoded in its lyrics, automatic language identification allows us to objectively validate the declared label and detect annotation inconsistencies, making it a reliable and justifiable approach for language quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36750a59ea2a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:12.550632Z",
     "start_time": "2025-11-14T14:25:34.232522Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_lang(text, idx):\n",
    "    try:\n",
    "        if not isinstance(text, str) or len(text.strip()) < 20:\n",
    "            print(f\"[{idx}] Skipped (too short or invalid)\")\n",
    "            return None\n",
    "        lang = detect(text)\n",
    "        if idx % 100 == 0:  # stampa ogni 100 per non intasare\n",
    "            print(f\"[{idx}] Detected: {lang}\")\n",
    "        return lang\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Applica con indice\n",
    "df['predicted_language'] = [\n",
    "    detect_lang(txt, i) for i, txt in enumerate(df['lyrics'])\n",
    "]\n",
    "# Confronto con lingua dichiarata\n",
    "df['language_match'] = df['language'] == df['predicted_language']\n",
    "\n",
    "# Statistiche\n",
    "total = len(df)\n",
    "matches = df['language_match'].sum()\n",
    "mismatches = total - matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a56b73a6b4a097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:19.530699Z",
     "start_time": "2025-11-14T14:26:19.518446Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- RESULT SUMMARY ---\")\n",
    "print(f\"Total tracks: {total}\")\n",
    "print(f\"Language matches: {matches} ({matches/total:.2%})\")\n",
    "print(f\"Mismatches: {mismatches} ({mismatches/total:.2%})\")\n",
    "\n",
    "print(\"\\n--- SAMPLE PREDICTIONS ---\")\n",
    "display(df[['lyrics', 'language', 'predicted_language', 'language_match']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ee0c6f84f4bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:21.180158Z",
     "start_time": "2025-11-14T14:26:21.173158Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.loc[df['predicted_language'].isna(), ['title', 'lyrics', 'language', 'predicted_language']].head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fc1112578ea9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:22.387633Z",
     "start_time": "2025-11-14T14:26:22.383655Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[[8854, 9181], \"predicted_language\"] = \"it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0dc77fb15241b",
   "metadata": {},
   "source": [
    "Now we can fix the coloums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f67245a577cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:23.213909Z",
     "start_time": "2025-11-14T14:26:23.207713Z"
    }
   },
   "outputs": [],
   "source": [
    "df['language'] = df['predicted_language']\n",
    "display(df[['language', 'predicted_language', 'language_match']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f7be06f40ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:24.469445Z",
     "start_time": "2025-11-14T14:26:24.457446Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['predicted_language'], inplace=True)\n",
    "df.drop(columns=['language_match'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f655cc9455b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:25.387201Z",
     "start_time": "2025-11-14T14:26:25.272126Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1) Conta lingue includendo i NaN\n",
    "lang_counts = df['language'].value_counts(dropna=False).reset_index()\n",
    "lang_counts.columns = ['language', 'count']\n",
    "\n",
    "# Mostra \"NaN\" come etichetta senza toccare df\n",
    "lang_counts['language'] = lang_counts['language'].astype(object).where(\n",
    "    ~lang_counts['language'].isna(), 'NaN'\n",
    ")\n",
    "\n",
    "# Ordina per count decrescente\n",
    "lang_counts = lang_counts.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# --- 2) Palette (simile all'Altair che avevi)\n",
    "base_palette = [\"#f3e5f5\", \"#e1bee7\", \"#ce93d8\", \"#ba68c8\", \"#9c27b0\"]\n",
    "# se le categorie sono più di 5, cicla i colori\n",
    "palette = list(islice(cycle(base_palette), len(lang_counts)))\n",
    "\n",
    "# --- 3) Plot (barh ordinato)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(\n",
    "    y=lang_counts['language'],\n",
    "    width=lang_counts['count'],\n",
    "    color=palette,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Inverti asse Y per avere la lingua più frequente in alto\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel('Number of Tracks')\n",
    "plt.ylabel('Predicted Language')\n",
    "plt.title('Number of Tracks per Predicted Language (Including Skipped = NaN)')\n",
    "\n",
    "# --- 4) Aggiungi etichette con i conteggi alla fine delle barre\n",
    "for bar, val in zip(bars, lang_counts['count']):\n",
    "    x = bar.get_width()\n",
    "    y = bar.get_y() + bar.get_height() / 2\n",
    "    plt.text(x + max(lang_counts['count']) * 0.01, y, str(val), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74268a0ddd49d2fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:26.521837Z",
     "start_time": "2025-11-14T14:26:26.511540Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.loc[df['language']== \"en\", ['title', 'lyrics', 'language',]].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6d23a13578cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:27.775480Z",
     "start_time": "2025-11-14T14:26:27.763202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[[41, 291, 376, 730, 866, 880, 885, 952, 963, 1006, 1035, 1049, 1061, 1065, 1194, 1264, 1285, 1483,1722, 1735, 1842, 1945, 2500, 2557,2572, 2602, 2904, 2963, 3007, 3017, 3034, 3045,3069,3096, 3266, 3404, 3503, 3924, 4301, 4367,4886, 5003, 5005, 5007, 5008, 5034, 5050, 5069, 5076, 5081, 5234, 6777,7474, 7720, 8967, 8981, 9214, 9229, 9727, 9920, 9985, 10432], \"predicted_language\"] = \"it\"\n",
    "\n",
    "display(df.loc[df['language']== \"en\", ['name', 'lyrics', 'language',]].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca57a658a1efed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:29.005963Z",
     "start_time": "2025-11-14T14:26:28.995885Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[[3601], \"predicted_language\"] = \"it\"\n",
    "\n",
    "display(df.loc[df['language']== \"es\", ['name', 'lyrics', 'language',]].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69489f1794f107b9",
   "metadata": {},
   "source": [
    "#### Correcting wrong active start\n",
    "Upon Inspection on the age of the artists when they started their career (code above).\n",
    "\n",
    "Among the unique artists, several had unusual ages at career start.  Nesli (age 10)  had incorrect active start dates, while Salmo (age 13) thasup (Age 14) nitro (age 14)  ghemon (age 14 )and Mudimbi (age 27) were correct.   These values comes from the original data.\n",
    "\n",
    "After filling birthdate and Active start date we realized that (bigmama) started at the age of one which is obviously wrong.\n",
    "\n",
    "We have age (7) it was for the singer priestess, but we already corrected its data above in the section (Correcting \"Priestess\" Entry).\n",
    "\n",
    " We will correct the errors by updating  Nesli’s to 1999,bigmama to 2016, leaving Salmo and Mudimbi unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36955662809d0f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:30.262963Z",
     "start_time": "2025-11-14T14:26:30.256052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate age at career start without adding a column\n",
    "ages = df['active_start'].dt.year - df['birth_date'].dt.year\n",
    "\n",
    "# Filter rows where age is 1,7  10,  13,or 27\n",
    "outliers = df[ages.isin([1,7 ,10, 13,14,27,])].copy()\n",
    "\n",
    "# Keep only unique artists based on name\n",
    "unique_outliers = outliers.drop_duplicates(subset=['name'])\n",
    "\n",
    "print(unique_outliers[['name', 'birth_date', 'active_start']].assign(age_at_start=ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d664a3c01430a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:31.612173Z",
     "start_time": "2025-11-14T14:26:31.494666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correcting wrong active_start dates\n",
    "df.loc[df['name'] == 'bigmama', 'active_start'] = pd.to_datetime('2016-01-01')\n",
    "df.loc[df['name'] == 'nesli', 'active_start'] = pd.to_datetime('1999-01-01')\n",
    "\n",
    "# Verify the changes\n",
    "outliers_corrected = df[df['name'].isin(['bigmama', 'nesli', 'salmo',])]\n",
    "print(outliers_corrected[['name', 'birth_date', 'active_start']])\n",
    "functions.plot_age_at_career_start(df,'Age of unique Artists When They Started Their Career After Filling Missing Values and Correcting Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4071a3e2c36952",
   "metadata": {},
   "source": [
    "#### Correcting album release dates for albums that have multiple release dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed65cc34d591e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:33.227681Z",
     "start_time": "2025-11-14T14:26:33.147303Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Compute the recent date per album ---\n",
    "earliest_dates = (\n",
    "    df.groupby('album', as_index=False)['album_release_date']\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# --- 3. Merge back into the main dataframe ---\n",
    "df = df.drop(columns=['album_release_date']).merge(\n",
    "    earliest_dates,\n",
    "    on='album',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 4. Confirm result ---\n",
    "print(\"Replaced albums with multiple release dates by their recent  date.\")\n",
    "print(df[['album', 'album_release_date']].drop_duplicates().shape)\n",
    "display(df[['album', 'album_release_date']].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c270aca68d792",
   "metadata": {},
   "source": [
    "#### Handling albums from 50s till 80s\n",
    "\n",
    "As we inspected before, we found that all the albums from the 1950s to 1980s were incorrectly assigned — their tracks and artists do not actually belong to those albums. Because this data is unreliable, we decided to fill the columns album_name, album_release_year, and album_type with None (null values) for these records.\n",
    "\n",
    "We do this to remove incorrect associations and avoid misleading results in future analysis. By replacing these wrong values with None, we clearly mark them as invalid or unknown, ensuring that only verified album–artist–track relationships remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2737709205db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:34.843885Z",
     "start_time": "2025-11-14T14:26:34.833897Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import project_fuctions\n",
    "importlib.reload(project_fuctions)\n",
    "functions.plot_unique_album_release_distribution(df)\n",
    "def fix_corrupted_release_dates(df):\n",
    "    \"\"\"\n",
    "    Fixes corrupted album release dates by imputing them with the verified modern release dates,\n",
    "    including month and day, and casts the column to datetime objects.\n",
    "    \"\"\"\n",
    "    df = df.copy() \n",
    "    \n",
    "    # 1. Define the Master Mapping Dictionary: {Album Name: Correct Date String (YYYY-MM-DD)}\n",
    "    CORRECT_DATE_MAP = {\n",
    "        'K1 Mixtape': '2014-12-29',\n",
    "        'S.O.S. EP': '2008-10-16',\n",
    "        'Entics Television': '2014-01-01',    # Using YYYY-01-01 for unknown M/D\n",
    "        'Cracovia': '2016-01-01',             # Using YYYY-01-01 for unknown M/D\n",
    "        'Council Estate Vol.1': '2012-01-01',   # Using YYYY-01-01 for unknown M/D\n",
    "        'Happy EP!': '2011-05-03',\n",
    "        'Equilibrio - EP': '2010-06-07',\n",
    "        'Jolly Mixtape': '2017-11-07',\n",
    "        'Quello Che Vi Consiglio Vol. 4': '2013-10-18',\n",
    "        'Rimo Da Quando': '2010-11-15',\n",
    "        'Nobiltà di Strada': '2007-02-16',\n",
    "        'Vivere aiuta a non morire': '2013-04-30',\n",
    "        'Quattro San Simoni e un funerale EP': '2015-04-28',\n",
    "        'VERA BADDIE': '2024-06-28',\n",
    "        'Haterproof': '2011-10-15',\n",
    "        'Tutto il Contrario Remixtape': '2011-01-01', # Using YYYY-01-01 for unknown M/D\n",
    "        'Radiografia - EP': '2006-01-01',    # Using YYYY-01-01 for unknown M/D\n",
    "    }\n",
    "    \n",
    "    # 2. Apply the Mapping and Overwrite Corrupted Dates\n",
    "    is_corrupted_row = df['album'].isin(CORRECT_DATE_MAP.keys())\n",
    "    df.loc[is_corrupted_row, 'album_release_date'] = df.loc[is_corrupted_row, 'album'].map(CORRECT_DATE_MAP)\n",
    "    \n",
    "    # # 3. Final Cleaning: Cast the column to datetime objects\n",
    "    df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce')\n",
    "    \n",
    "    print(\"### ✅ Full Release Date Imputation Complete\")\n",
    "    print(f\"Corrected {is_corrupted_row.sum()} records with precise dates.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "fix_corrupted_release_dates(df)\n",
    "functions.plot_unique_album_release_distribution(df,\n",
    "                                                 \n",
    "    title1=\"Percentage of Unique Albums by Release Decade After Cleaning\",\n",
    "    title2=\"Distribution of Unique Album Release Years After Cleaning\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ca547fcc81cc6",
   "metadata": {},
   "source": [
    "#### Cleaning Albums Released Before the Artist’s Birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248f3f0bb151a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:36.505935Z",
     "start_time": "2025-11-14T14:26:36.496935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a mask for albums released before the artist was born\n",
    "mask = (df['album_release_date'] < df['birth_date'])\n",
    "\n",
    "# Nullify incorrect album information\n",
    "df.loc[mask, ['album_name', 'album_release_date', 'disc_number','track_number']] = np.nan\n",
    "\n",
    "# Confirm how many records were affected\n",
    "print(f\"Albums released before artist's birth: {mask.sum()} records cleaned.\")\n",
    "df.loc[mask, ['name', 'album_name', 'album_release_date', 'birth_date', 'album_type','disc_number','track_number']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0474a54063cad",
   "metadata": {},
   "source": [
    "#### Renumbering Duplicates Track Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cee5ff544d069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:45.576781Z",
     "start_time": "2025-11-14T14:26:38.168378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up strings (optional but helpful)\n",
    "df['album'] = df['album'].str.strip()\n",
    "df['title'] = df['title'].str.strip()\n",
    "df['name'] = df['name'].str.strip()\n",
    "\n",
    "# Sort to maintain consistent order\n",
    "df = df.sort_values(['album', 'disc_number', 'track_number']).reset_index(drop=True)\n",
    "\n",
    "# Renumber duplicate track numbers within each album/disc group\n",
    "def renumber_duplicates(group):\n",
    "    # Count duplicates of track_number within this disc\n",
    "    counts = group['track_number'].value_counts()\n",
    "    duplicates = counts[counts > 1].index\n",
    "\n",
    "    if len(duplicates) > 0:\n",
    "        # Reassign track numbers so that each entry within a disc has unique numbers starting at 1\n",
    "        group['track_number'] = range(1, len(group) + 1)\n",
    "    return group\n",
    "\n",
    "df = df.groupby(['album', 'disc_number'], group_keys=False).apply(renumber_duplicates)\n",
    "\n",
    "\n",
    "duplicates = df[df.duplicated(subset=['album', 'disc_number', 'track_number'], keep=False)]\n",
    "print(f\"🎵 Found {len(duplicates)} duplicate track entries.\")\n",
    "dtale.show(duplicates[['album', 'disc_number', 'track_number', 'title', 'name']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43170ca31ea82b4",
   "metadata": {},
   "source": [
    "#### Correcting Coordinates\n",
    "We will correct coordinates to make them match the birth_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ca411a2bd2a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:45.753168Z",
     "start_time": "2025-11-14T14:26:45.742939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correct coordinates for each birth place\n",
    "coord_map = {\n",
    "    \"Almería\": (36.8340, -2.4637),\n",
    "    \"Buenos Aires\": (-34.6037, -58.3816),\n",
    "    \"Singapore\": (1.290270, 103.851959)\n",
    "}\n",
    "\n",
    "# Update each row based on exact city name\n",
    "for place, (lat, lon) in coord_map.items():\n",
    "    mask = df['birth_place'] == place\n",
    "    df.loc[mask, 'latitude'] = lat\n",
    "    df.loc[mask, 'longitude'] = lon\n",
    "\n",
    "# Keep only the columns we care about\n",
    "cols = ['birth_place', 'latitude', 'longitude','province','region']\n",
    "\n",
    "# Drop duplicates so each place appears once (keeping the first lat/lon found)\n",
    "unique_places = df[cols].drop_duplicates(subset=['birth_place'])\n",
    "\n",
    "# Sort alphabetically by birth_place (optional)\n",
    "unique_places = unique_places.sort_values(by='birth_place').reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "display(unique_places)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7363b84a3877ce",
   "metadata": {},
   "source": [
    "#### Fixing Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854e78167aabdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:47:28.412730Z",
     "start_time": "2025-11-14T14:47:28.404631Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nApplying new rule: 'explicit' = (swear_IT > 0) | (swear_EN > 0)\")\n",
    "new_explicit_mask = (df['swear_IT'] > 0) | (df['swear_EN'] > 0)\n",
    "\n",
    "# --- 3. Overwrite the 'explicit' Column ---\n",
    "# We replace the original 'explicit' values with our new, consistent rule.\n",
    "df['explicit'] = new_explicit_mask\n",
    "\n",
    "print(\"\\n'explicit' column successfully overwritten.\")\n",
    "print(\"\\nNew 'explicit' values (After standardization):\")\n",
    "print(df['explicit'].value_counts())\n",
    "\n",
    "# --- 4. Verification ---\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(\"Checking for rows where 'explicit = True' but swear count is 0 (should be 0):\")\n",
    "\n",
    "# Find any errors (where explicit is True, but both swear counts are 0)\n",
    "errors = df[(df['explicit'] == True) & (df['swear_IT'] == 0) & (df['swear_EN'] == 0)]\n",
    "num_errors = len(errors)\n",
    "\n",
    "print(f\"Errors found: {num_errors}\")\n",
    "\n",
    "if num_errors == 0:\n",
    "    print(\"SUCCESS: The 'explicit' column is now 100% consistent with swear counts.\")\n",
    "else:\n",
    "    print(\"ERROR: Discrepancies still found. Please review the logic.\")\n",
    "\n",
    "print(\"\\n(Note: The 1,202 'explicit-for-other-reasons' cases are now set to False.)\")\n",
    "print(\"(Note: The 3,125 'not-explicit-with-swears' cases are now set to True.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994ccc25f9b8c72",
   "metadata": {},
   "source": [
    "#### Fixing Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35257e38977e8a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:45.762013Z",
     "start_time": "2025-11-14T14:26:45.755181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define valid duration range (in milliseconds)\n",
    "MIN_DUR = 30_000      # 30 seconds\n",
    "MAX_DUR = 600_000    # 10 minutes\n",
    "\n",
    "# Replace unrealistic values with NaN\n",
    "df['duration_ms'] = df['duration_ms'].where(\n",
    "    (df['duration_ms'] >= MIN_DUR) & (df['duration_ms'] <= MAX_DUR),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Show songs now missing duration (i.e., invalid before)\n",
    "invalid_songs = df[df['duration_ms'].isna()][['title', 'duration_ms']]\n",
    "invalid_songs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a3781214aa41",
   "metadata": {},
   "source": [
    "### Fixing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4e010d2758743",
   "metadata": {},
   "source": [
    "### Save new cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed608b6a3a0085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:46.078504Z",
     "start_time": "2025-11-14T14:26:45.765017Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \"data\"\n",
    "output_file = \"merge_dataset_cleaned.csv\"\n",
    "output_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "print(f\"--- Saving Cleaned DataFrame ---\")\n",
    "print(f\"Saving {len(df)} rows and {len(df.columns)} columns to '{output_filename}'...\")\n",
    "\n",
    "try:\n",
    "    df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSUCCESS: DataFrame saved successfully to '{output_filename}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: An error occurred while saving the file.\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
